{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"7807d5ca35414c179050705be150bc13":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1939a3d68b4e469fa1828103d0396a37","IPY_MODEL_d2a360e7b9b54b7b94bad42231b80ace","IPY_MODEL_96a74670613f42a8a7c92790ea161e07"],"layout":"IPY_MODEL_f4bdb547ac264b9fb503793dd442b61f"}},"1939a3d68b4e469fa1828103d0396a37":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_182bf0c8bf5d4265ba2fe4617c7f432f","placeholder":"​","style":"IPY_MODEL_5e9a58848bc249c9a979ec2cdd641ae0","value":"tokenizer_config.json: 100%"}},"d2a360e7b9b54b7b94bad42231b80ace":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a8c9502e60b4bd8abffbbb09f386805","max":49,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b9b3d399d6c448e9a5bb745cee2399b5","value":49}},"96a74670613f42a8a7c92790ea161e07":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_08d20a4f3a8044feb9e8a5a8b244067a","placeholder":"​","style":"IPY_MODEL_d73ed46493f84e88a70d39e87866edee","value":" 49.0/49.0 [00:00&lt;00:00, 1.16kB/s]"}},"f4bdb547ac264b9fb503793dd442b61f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"182bf0c8bf5d4265ba2fe4617c7f432f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e9a58848bc249c9a979ec2cdd641ae0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3a8c9502e60b4bd8abffbbb09f386805":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9b3d399d6c448e9a5bb745cee2399b5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"08d20a4f3a8044feb9e8a5a8b244067a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d73ed46493f84e88a70d39e87866edee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bc94f3848a9f4dbfa2e4121656740fd1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_49731a870c4347b58832a54133b13221","IPY_MODEL_198244bcdb084c3eaf6538ade38b13de","IPY_MODEL_1fde796d99e048399223702fd7f929ef"],"layout":"IPY_MODEL_d817a0093d694d5690d69c850fff06c7"}},"49731a870c4347b58832a54133b13221":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f956fcccb56d4b57996380baffef772a","placeholder":"​","style":"IPY_MODEL_9d382953953d4facbafe133e95280489","value":"vocab.txt: 100%"}},"198244bcdb084c3eaf6538ade38b13de":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0007a12b45214175af403d0c44266aff","max":213450,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7877323eca52434d9c525fa289602354","value":213450}},"1fde796d99e048399223702fd7f929ef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_806613d7d45741dab0b0fd17dee1efd9","placeholder":"​","style":"IPY_MODEL_fa80c7cab7f64d96a6a7f4d840b188e5","value":" 213k/213k [00:00&lt;00:00, 2.57MB/s]"}},"d817a0093d694d5690d69c850fff06c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f956fcccb56d4b57996380baffef772a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d382953953d4facbafe133e95280489":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0007a12b45214175af403d0c44266aff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7877323eca52434d9c525fa289602354":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"806613d7d45741dab0b0fd17dee1efd9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa80c7cab7f64d96a6a7f4d840b188e5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"57ac976b4a29459fb18f3a039b5951d9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5a0d1444138245ba9feddd5b2c9e0c99","IPY_MODEL_dbcc08dc3a8049cdb77164c3da07ff66","IPY_MODEL_12a0776e8b63496d8246cdc7f777b158"],"layout":"IPY_MODEL_37671c3615404898975f6ebcb5e53404"}},"5a0d1444138245ba9feddd5b2c9e0c99":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c113b50a80c423fb95a7713b79e319a","placeholder":"​","style":"IPY_MODEL_25211a2611284fa79313b74b90c0dd13","value":"tokenizer.json: 100%"}},"dbcc08dc3a8049cdb77164c3da07ff66":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_02aee807b72e42a193817ed9d6541370","max":435797,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f3629ed467bc4e079271df4ba7db6bbe","value":435797}},"12a0776e8b63496d8246cdc7f777b158":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b05a3df27b9a4812aa8992595802514d","placeholder":"​","style":"IPY_MODEL_ac93d1d126524094965cd292abfd30c3","value":" 436k/436k [00:00&lt;00:00, 2.72MB/s]"}},"37671c3615404898975f6ebcb5e53404":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c113b50a80c423fb95a7713b79e319a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25211a2611284fa79313b74b90c0dd13":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"02aee807b72e42a193817ed9d6541370":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3629ed467bc4e079271df4ba7db6bbe":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b05a3df27b9a4812aa8992595802514d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac93d1d126524094965cd292abfd30c3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8acb83b3962e40adabed570b5e6e0cc0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e1d3fef17b1d46cd8d09b4d6df33f043","IPY_MODEL_fb52226142e9444ea385847bf606b579","IPY_MODEL_66f73d4e5c1c451ab21d75bc79f672dc"],"layout":"IPY_MODEL_12b62f5d1cc74840bfeb496c0c07e03a"}},"e1d3fef17b1d46cd8d09b4d6df33f043":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9bccc5c247bd4872a016f2c09bf23f33","placeholder":"​","style":"IPY_MODEL_42733d3246a344e0819882112fdb4b9b","value":"config.json: 100%"}},"fb52226142e9444ea385847bf606b579":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_87fd03cf11734008b7c1e483deb746b4","max":465,"min":0,"orientation":"horizontal","style":"IPY_MODEL_171094d764d2482fafb8636e1cdc3ade","value":465}},"66f73d4e5c1c451ab21d75bc79f672dc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc74fa80ac5d49f789db06ee5404eb37","placeholder":"​","style":"IPY_MODEL_5920c341f8084724970b688af9618b7f","value":" 465/465 [00:00&lt;00:00, 34.9kB/s]"}},"12b62f5d1cc74840bfeb496c0c07e03a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9bccc5c247bd4872a016f2c09bf23f33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42733d3246a344e0819882112fdb4b9b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"87fd03cf11734008b7c1e483deb746b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"171094d764d2482fafb8636e1cdc3ade":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dc74fa80ac5d49f789db06ee5404eb37":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5920c341f8084724970b688af9618b7f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"99383b201b5c4d26a7e8c61a0d46fb62":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ea0180ee2d07452db614214a0776a12d","IPY_MODEL_58de319fe6f54c6abcbbdbf5cc1f2e5c","IPY_MODEL_30eab1226c0c456b877bbc246abbcef2"],"layout":"IPY_MODEL_4d5c4a5720a44bf2bd4c2dce54c05d58"}},"ea0180ee2d07452db614214a0776a12d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8873ec2206c041899ea75b411056098d","placeholder":"​","style":"IPY_MODEL_8637c09332e748d281abc9f3d89c7fd1","value":"Map: 100%"}},"58de319fe6f54c6abcbbdbf5cc1f2e5c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_264ec78c0e6b41f2bb6ddb23464593eb","max":18616,"min":0,"orientation":"horizontal","style":"IPY_MODEL_688a5f8741f144d3b3fb9dae487eba9e","value":18616}},"30eab1226c0c456b877bbc246abbcef2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e8be0d94fef4592b8b9d2e4f2394d96","placeholder":"​","style":"IPY_MODEL_cad38b578cf44203b9383c86ab7c9861","value":" 18616/18616 [00:11&lt;00:00, 1725.20 examples/s]"}},"4d5c4a5720a44bf2bd4c2dce54c05d58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8873ec2206c041899ea75b411056098d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8637c09332e748d281abc9f3d89c7fd1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"264ec78c0e6b41f2bb6ddb23464593eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"688a5f8741f144d3b3fb9dae487eba9e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4e8be0d94fef4592b8b9d2e4f2394d96":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cad38b578cf44203b9383c86ab7c9861":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3d4341b2d4ca4ab59679f77d9bae2141":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_70ca81fb1d4a430fb20f5ef13129b202","IPY_MODEL_6ad8d35a9dda420483788f80b338fc4c","IPY_MODEL_7e1afaf82e144cc3abcb3859b7e98d5f"],"layout":"IPY_MODEL_95d8acc9a8ae477abc30654500dd0666"}},"70ca81fb1d4a430fb20f5ef13129b202":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ba258f4defd464bb058fbfd88360681","placeholder":"​","style":"IPY_MODEL_8a2886b963a54d0cae192abef4d480d9","value":"Map: 100%"}},"6ad8d35a9dda420483788f80b338fc4c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ecf514f3265843a085e940db60f186ab","max":1035,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0018376b1b364f3ea8db7ed2139589ff","value":1035}},"7e1afaf82e144cc3abcb3859b7e98d5f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_259368276ddf4f7eb8135595ee243898","placeholder":"​","style":"IPY_MODEL_eb3e6970fb904e1dac949950415d1282","value":" 1035/1035 [00:00&lt;00:00, 2146.35 examples/s]"}},"95d8acc9a8ae477abc30654500dd0666":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ba258f4defd464bb058fbfd88360681":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a2886b963a54d0cae192abef4d480d9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ecf514f3265843a085e940db60f186ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0018376b1b364f3ea8db7ed2139589ff":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"259368276ddf4f7eb8135595ee243898":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb3e6970fb904e1dac949950415d1282":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5596132eb19a497fb7f085bf13db8d41":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aa9cc9ef5b8944d18b0b2b007bcf60f5","IPY_MODEL_79a182e2c94f48c69e1c6fb0712b8ad7","IPY_MODEL_739635977b794d18ada58bea892aefd5"],"layout":"IPY_MODEL_800c6a20859f46d58c69d38eefc53da9"}},"aa9cc9ef5b8944d18b0b2b007bcf60f5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_afc21072452e49aa9c7b15c9568a55bf","placeholder":"​","style":"IPY_MODEL_dece2963be034c778907a25d3cc9b34d","value":"Map: 100%"}},"79a182e2c94f48c69e1c6fb0712b8ad7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b3f419c5b61f434d9963b278373223cb","max":1034,"min":0,"orientation":"horizontal","style":"IPY_MODEL_25139197502647c994fefb49f24bf8e8","value":1034}},"739635977b794d18ada58bea892aefd5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef485aa52b77456eaa8d7b70b5464b5b","placeholder":"​","style":"IPY_MODEL_eb215ad370cc40099ef625db75ff9390","value":" 1034/1034 [00:00&lt;00:00, 2235.84 examples/s]"}},"800c6a20859f46d58c69d38eefc53da9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"afc21072452e49aa9c7b15c9568a55bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dece2963be034c778907a25d3cc9b34d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b3f419c5b61f434d9963b278373223cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25139197502647c994fefb49f24bf8e8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ef485aa52b77456eaa8d7b70b5464b5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb215ad370cc40099ef625db75ff9390":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"038e2a0ee2684186bd574bbe65b59aad":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_13cddb35709a4bd5bb14c573785922dc","IPY_MODEL_e0be08adb7024b9ea94e079cb94a1374","IPY_MODEL_a91fa3e0229d4addad85b56ad5cd23f6"],"layout":"IPY_MODEL_d6d88601abae4aa3863f4baa915ff266"}},"13cddb35709a4bd5bb14c573785922dc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e8d0f78c1c244b9a6c70157f8a67d37","placeholder":"​","style":"IPY_MODEL_51f27f0f44724da19531ba2503e5a477","value":"model.safetensors: 100%"}},"e0be08adb7024b9ea94e079cb94a1374":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac465ba90fac4d39afd4b6c4069685ed","max":263260784,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ed666ed8c26f43f3b53ce9a5a123744b","value":263260784}},"a91fa3e0229d4addad85b56ad5cd23f6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_68b2fc452493475b991b302802597fbe","placeholder":"​","style":"IPY_MODEL_3730cb7d2bb8459a87a940f65f47a34d","value":" 263M/263M [00:02&lt;00:00, 182MB/s]"}},"d6d88601abae4aa3863f4baa915ff266":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e8d0f78c1c244b9a6c70157f8a67d37":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51f27f0f44724da19531ba2503e5a477":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac465ba90fac4d39afd4b6c4069685ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed666ed8c26f43f3b53ce9a5a123744b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"68b2fc452493475b991b302802597fbe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3730cb7d2bb8459a87a940f65f47a34d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e48a20504824d71bb401ae60a484308":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a0449c608cf94799b4bccbb1d28361a8","IPY_MODEL_8748ac01a949445ba211ca100f0aec48","IPY_MODEL_0c334f7ee755488ba236f8847d83029f"],"layout":"IPY_MODEL_bb37e27e395744c992389cce38d6ec21"}},"a0449c608cf94799b4bccbb1d28361a8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_08369c20a3cc441099d7b2303bf8fc88","placeholder":"​","style":"IPY_MODEL_41cd36f51f3f4a1a89e5c3f192022d44","value":"config.json: 100%"}},"8748ac01a949445ba211ca100f0aec48":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e11656e3e29a4045b18cb5b022e7d2b9","max":1716,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0cd24dadd9f14aa8a48fd4165d43f0c9","value":1716}},"0c334f7ee755488ba236f8847d83029f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8fc7a537008d48ff96a335f742e8187b","placeholder":"​","style":"IPY_MODEL_4473dc09c5584d0f988d8aeb1d745617","value":" 1.72k/1.72k [00:00&lt;00:00, 47.7kB/s]"}},"bb37e27e395744c992389cce38d6ec21":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08369c20a3cc441099d7b2303bf8fc88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41cd36f51f3f4a1a89e5c3f192022d44":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e11656e3e29a4045b18cb5b022e7d2b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0cd24dadd9f14aa8a48fd4165d43f0c9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8fc7a537008d48ff96a335f742e8187b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4473dc09c5584d0f988d8aeb1d745617":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8af0fc12eda34f28bb37f76d29310e81":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3cdde870fed04a9eb718cf743e18c3eb","IPY_MODEL_5b21ba1d6c18420f8487dcf4313b4f60","IPY_MODEL_2af7069fb5564411a63048cbb13f3dad"],"layout":"IPY_MODEL_d6057f563cc34a8886b32db2efbac85a"}},"3cdde870fed04a9eb718cf743e18c3eb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d92d8a7b6e2e496cb90f4897355af61a","placeholder":"​","style":"IPY_MODEL_1abb6ab5267343b0b191e50f36d7bf9f","value":"vocab.json: 100%"}},"5b21ba1d6c18420f8487dcf4313b4f60":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c0edfb52d664cbe81bf3e984416b234","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fd589c0bf9334f09a9c2eedf3dbfdf3f","value":898823}},"2af7069fb5564411a63048cbb13f3dad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_49357b44a0b342e6a74d2748064c9a65","placeholder":"​","style":"IPY_MODEL_fc24533183c9472bb3cc77d3f4e25e29","value":" 899k/899k [00:00&lt;00:00, 4.25MB/s]"}},"d6057f563cc34a8886b32db2efbac85a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d92d8a7b6e2e496cb90f4897355af61a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1abb6ab5267343b0b191e50f36d7bf9f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0c0edfb52d664cbe81bf3e984416b234":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd589c0bf9334f09a9c2eedf3dbfdf3f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"49357b44a0b342e6a74d2748064c9a65":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc24533183c9472bb3cc77d3f4e25e29":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b3b28275e403439bb9ece2d79d0e59a2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e314de7a913d4603ab52cc8ef5cf5278","IPY_MODEL_d10196c8fa1b41bca72ed575a2d3d95e","IPY_MODEL_592cfe053194438b807eb75690b8fe01"],"layout":"IPY_MODEL_2038937726ab433396d55f426f30872d"}},"e314de7a913d4603ab52cc8ef5cf5278":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7949ebec40b4470f8c0423aedde490f3","placeholder":"​","style":"IPY_MODEL_90b7774df5b94e57849e838d06466ce9","value":"merges.txt: 100%"}},"d10196c8fa1b41bca72ed575a2d3d95e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a603c3bbf15477799a9afbc3d938a39","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a989625f2485453195afca459f0320ee","value":456318}},"592cfe053194438b807eb75690b8fe01":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6b32bb39375429b9fda32f3febf18be","placeholder":"​","style":"IPY_MODEL_730a9bea417c48bea5aceb0d6f440d6f","value":" 456k/456k [00:00&lt;00:00, 1.09MB/s]"}},"2038937726ab433396d55f426f30872d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7949ebec40b4470f8c0423aedde490f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90b7774df5b94e57849e838d06466ce9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9a603c3bbf15477799a9afbc3d938a39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a989625f2485453195afca459f0320ee":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d6b32bb39375429b9fda32f3febf18be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"730a9bea417c48bea5aceb0d6f440d6f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5d7a5a494cee46d595ce66fcfc41f201":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4d1d705c8b0441559955e8de64ea6ac1","IPY_MODEL_93235b299df34936b7e7503403f39889","IPY_MODEL_e8762840f5da438c8bb74fda123bb4c9"],"layout":"IPY_MODEL_be9525441ead4200bec634035d45d2b9"}},"4d1d705c8b0441559955e8de64ea6ac1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e569dbb77574220af3a5192a0a30f86","placeholder":"​","style":"IPY_MODEL_534bb33a969647b2ad1bdb806084b16f","value":"tokenizer.json: 100%"}},"93235b299df34936b7e7503403f39889":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c3a2e42b2bc4ff7b21ce5a43af836c3","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e4b8933fb777490cabb0e849c099172e","value":1355863}},"e8762840f5da438c8bb74fda123bb4c9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e0f3f7423b144bc8b7c265fba071065","placeholder":"​","style":"IPY_MODEL_4287932e0f85461abc965d3e59e54c17","value":" 1.36M/1.36M [00:00&lt;00:00, 1.61MB/s]"}},"be9525441ead4200bec634035d45d2b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e569dbb77574220af3a5192a0a30f86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"534bb33a969647b2ad1bdb806084b16f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4c3a2e42b2bc4ff7b21ce5a43af836c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4b8933fb777490cabb0e849c099172e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6e0f3f7423b144bc8b7c265fba071065":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4287932e0f85461abc965d3e59e54c17":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"71b187c8ecf04c7a9141dd6881c7ddbd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9677f076f7854bfcb704f90bcb07b76b","IPY_MODEL_367a9e57f5634d7c8fe0596925fdf556","IPY_MODEL_3bc0359bfc6b4a738dc2345a1630dff7"],"layout":"IPY_MODEL_309adf280bce499e84cfc2a794f7850f"}},"9677f076f7854bfcb704f90bcb07b76b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0a50859f3534eb48b892d889226c54c","placeholder":"​","style":"IPY_MODEL_0b8b5a941aca4ee483a3d1b8b470dc19","value":"tokenizer_config.json: 100%"}},"367a9e57f5634d7c8fe0596925fdf556":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_de994dd648ce47a49036f8517816fe75","max":49,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c5bfe956d5964cd29a30ae5ad4ad9cec","value":49}},"3bc0359bfc6b4a738dc2345a1630dff7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4685715a48324bf99e6846c8c352bca4","placeholder":"​","style":"IPY_MODEL_ff44514ead0b499fbee7f01e0936627f","value":" 49.0/49.0 [00:00&lt;00:00, 1.72kB/s]"}},"309adf280bce499e84cfc2a794f7850f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0a50859f3534eb48b892d889226c54c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b8b5a941aca4ee483a3d1b8b470dc19":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"de994dd648ce47a49036f8517816fe75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5bfe956d5964cd29a30ae5ad4ad9cec":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4685715a48324bf99e6846c8c352bca4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff44514ead0b499fbee7f01e0936627f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e9a1bcc735664825baeadcd9dba4b5f5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cafe46d9897f4936b6e138cf2223e82a","IPY_MODEL_fa75f3dd97e8446abf54aeda1c54bf93","IPY_MODEL_660eb586790b4dc18055c6444aae1777"],"layout":"IPY_MODEL_86286308a9284515a369c0242a6841b7"}},"cafe46d9897f4936b6e138cf2223e82a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_593cfa6e711f40d094bab94f93d21c92","placeholder":"​","style":"IPY_MODEL_07c5d7446f244ba7b47cf726fc7643ae","value":"config.json: 100%"}},"fa75f3dd97e8446abf54aeda1c54bf93":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bed43938106742d7b131ad8322275bc9","max":465,"min":0,"orientation":"horizontal","style":"IPY_MODEL_43ee672da0ef4f8f9346851d881452c7","value":465}},"660eb586790b4dc18055c6444aae1777":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe57145df42d417881ff4fce81f9fc61","placeholder":"​","style":"IPY_MODEL_fc8c90002f9a4691a27480719a32624b","value":" 465/465 [00:00&lt;00:00, 35.7kB/s]"}},"86286308a9284515a369c0242a6841b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"593cfa6e711f40d094bab94f93d21c92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07c5d7446f244ba7b47cf726fc7643ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bed43938106742d7b131ad8322275bc9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43ee672da0ef4f8f9346851d881452c7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fe57145df42d417881ff4fce81f9fc61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc8c90002f9a4691a27480719a32624b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"163acf8169a3486b8f3043d8dd2e9ccc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ea181a22fbf848db8b3471b372c65de1","IPY_MODEL_d849c714d9534224b6f502896dfe7ada","IPY_MODEL_7b4b8033079b43b8b19314951418632e"],"layout":"IPY_MODEL_00709b59c86f495689d2d5648ea86150"}},"ea181a22fbf848db8b3471b372c65de1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc63a64bbf164f81a91e34f1d75a280e","placeholder":"​","style":"IPY_MODEL_3aac4a241bd8436a9383010d50ecf2d4","value":"vocab.txt: 100%"}},"d849c714d9534224b6f502896dfe7ada":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e87a87827e354c70bdf016717989de5c","max":213450,"min":0,"orientation":"horizontal","style":"IPY_MODEL_58f4be19f90c46fdbc643381e50e00a2","value":213450}},"7b4b8033079b43b8b19314951418632e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c2af64e08f540139b93307b31207882","placeholder":"​","style":"IPY_MODEL_9520182cea464e9ba22b9cc5e28b93af","value":" 213k/213k [00:00&lt;00:00, 1.01MB/s]"}},"00709b59c86f495689d2d5648ea86150":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc63a64bbf164f81a91e34f1d75a280e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3aac4a241bd8436a9383010d50ecf2d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e87a87827e354c70bdf016717989de5c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58f4be19f90c46fdbc643381e50e00a2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5c2af64e08f540139b93307b31207882":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9520182cea464e9ba22b9cc5e28b93af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dd65d71ce7704e17bfa3691bc14ec512":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c71e42b1cec24b8fb7a93c2a833e0220","IPY_MODEL_a71612db976e4dd6843690763bac1176","IPY_MODEL_b340284c37594bcb808f3e431d613005"],"layout":"IPY_MODEL_9e47c0f20a9647ae86719960b25f7e5b"}},"c71e42b1cec24b8fb7a93c2a833e0220":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd8ac3421ac346b49b5b4eeccb13f09a","placeholder":"​","style":"IPY_MODEL_9bba265b05d04a9c912e42fda69dfec8","value":"tokenizer.json: 100%"}},"a71612db976e4dd6843690763bac1176":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fede6f1be06240908caa666b321ffa4c","max":435797,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6e4d55188dbe40e2970c25df38dad6df","value":435797}},"b340284c37594bcb808f3e431d613005":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f1572aed4434217a1db3a7f44d3ac09","placeholder":"​","style":"IPY_MODEL_0d88075a34bb4cb2aa5ddf4bc526bd23","value":" 436k/436k [00:00&lt;00:00, 1.03MB/s]"}},"9e47c0f20a9647ae86719960b25f7e5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd8ac3421ac346b49b5b4eeccb13f09a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9bba265b05d04a9c912e42fda69dfec8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fede6f1be06240908caa666b321ffa4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e4d55188dbe40e2970c25df38dad6df":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7f1572aed4434217a1db3a7f44d3ac09":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d88075a34bb4cb2aa5ddf4bc526bd23":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c2cd03bdfa264134b2ce8246eb920ab0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cd915d040f9344e885191d1e576d2265","IPY_MODEL_4228ffe47b054d4c8d7298e3e6d3d079","IPY_MODEL_d2ed7580d2c4482ea9a5af236a388d59"],"layout":"IPY_MODEL_1ee6b0ee9b2e402eb53ece117c26a36f"}},"cd915d040f9344e885191d1e576d2265":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_09af7b181b9f4ff79da396d9e55e3337","placeholder":"​","style":"IPY_MODEL_65e8acd91de4407eb0270658e09efa50","value":"model.safetensors: 100%"}},"4228ffe47b054d4c8d7298e3e6d3d079":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_92bc2438238542ffb8d950379c7f371a","max":557709915,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0d4078db79564f0cac001ca57443ac86","value":557709915}},"d2ed7580d2c4482ea9a5af236a388d59":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_63bf5ccce0aa4046b959d6b18136d2b5","placeholder":"​","style":"IPY_MODEL_131a27222e7a401dbc6308d1859ab24a","value":" 558M/558M [00:04&lt;00:00, 185MB/s]"}},"1ee6b0ee9b2e402eb53ece117c26a36f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09af7b181b9f4ff79da396d9e55e3337":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65e8acd91de4407eb0270658e09efa50":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"92bc2438238542ffb8d950379c7f371a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d4078db79564f0cac001ca57443ac86":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"63bf5ccce0aa4046b959d6b18136d2b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"131a27222e7a401dbc6308d1859ab24a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0a04c4bb30704ef9b685e912ce4f8226":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9bd9d38c07e044cc87a39b0a285bcbcf","IPY_MODEL_46049c93ac644a9eb23f91545900fca1","IPY_MODEL_d3423c90fc034c9fa9e406e2ab116943"],"layout":"IPY_MODEL_7e8c49cb90ad44028e43183a357a5be1"}},"9bd9d38c07e044cc87a39b0a285bcbcf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3879680cc934a29942f16f4b4f8b920","placeholder":"​","style":"IPY_MODEL_7e6bd96c516e424f98f9c5c58ed0f111","value":"config.json: 100%"}},"46049c93ac644a9eb23f91545900fca1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_27fad13a8f444916b5cbbfd51ee44953","max":465,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0f83dda03c384c7f941dd58dc248c371","value":465}},"d3423c90fc034c9fa9e406e2ab116943":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7829771857124c3f96b901c83b08ac90","placeholder":"​","style":"IPY_MODEL_0d22d60e098b49328a4d195bf4a1c3a2","value":" 465/465 [00:00&lt;00:00, 37.2kB/s]"}},"7e8c49cb90ad44028e43183a357a5be1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3879680cc934a29942f16f4b4f8b920":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e6bd96c516e424f98f9c5c58ed0f111":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"27fad13a8f444916b5cbbfd51ee44953":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f83dda03c384c7f941dd58dc248c371":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7829771857124c3f96b901c83b08ac90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d22d60e098b49328a4d195bf4a1c3a2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6b61a64f327a4ca1b1f782265b460244":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_89244d430be8477c8c4874eb5e39e20a","IPY_MODEL_ca20bc8956494066b15fa10c90d16b5f","IPY_MODEL_d219438106f44fd9a527ea84aeecc9c4"],"layout":"IPY_MODEL_f1315ffe031f49f08a7b7708d9ae1e9b"}},"89244d430be8477c8c4874eb5e39e20a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_33dedd9568b44f97aacb26246caaddd1","placeholder":"​","style":"IPY_MODEL_c6509c3c2bee4d1aaef51ac17b663bcf","value":"model.safetensors: 100%"}},"ca20bc8956494066b15fa10c90d16b5f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc4174680b4541dcbd68959ff5e55cf9","max":263260784,"min":0,"orientation":"horizontal","style":"IPY_MODEL_45dd2f2203814577b20b011d604c01c0","value":263260784}},"d219438106f44fd9a527ea84aeecc9c4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b6a2f782f2f44f6a4c7cae47a5edd99","placeholder":"​","style":"IPY_MODEL_744d15d727e74989b0e71db12bc820fb","value":" 263M/263M [00:11&lt;00:00, 24.5MB/s]"}},"f1315ffe031f49f08a7b7708d9ae1e9b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33dedd9568b44f97aacb26246caaddd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6509c3c2bee4d1aaef51ac17b663bcf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc4174680b4541dcbd68959ff5e55cf9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45dd2f2203814577b20b011d604c01c0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1b6a2f782f2f44f6a4c7cae47a5edd99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"744d15d727e74989b0e71db12bc820fb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"583745c5e64c4a08bc6ed33ff673d61c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5fa16c749d53450d82fa37bb7c8f707b","IPY_MODEL_356dfac33ab54915b76352cc43321a7c","IPY_MODEL_4c883d2059074641b76a25f865e0874d"],"layout":"IPY_MODEL_84743ac22379473abd1a20098f9326eb"}},"5fa16c749d53450d82fa37bb7c8f707b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f5c945a7dd74a37ae80852b89aa4c64","placeholder":"​","style":"IPY_MODEL_8b987d9448584807be7ac11a308eb685","value":"Downloading builder script: 100%"}},"356dfac33ab54915b76352cc43321a7c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b3e4dc3e6d14d14906f64fbf77576b2","max":8146,"min":0,"orientation":"horizontal","style":"IPY_MODEL_469c174e2c5949319f8bb91ceac0ce25","value":8146}},"4c883d2059074641b76a25f865e0874d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fbeb052839624138bbd5ec9558829586","placeholder":"​","style":"IPY_MODEL_67afdc47ce3341e4a9aca864071f6e04","value":" 8.15k/8.15k [00:00&lt;00:00, 442kB/s]"}},"84743ac22379473abd1a20098f9326eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f5c945a7dd74a37ae80852b89aa4c64":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b987d9448584807be7ac11a308eb685":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4b3e4dc3e6d14d14906f64fbf77576b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"469c174e2c5949319f8bb91ceac0ce25":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fbeb052839624138bbd5ec9558829586":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67afdc47ce3341e4a9aca864071f6e04":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"20f536b4421745cfb99e4d91db052a1b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c3776d21fe1e4d548136798a2b9a99db","IPY_MODEL_ddb12c314d5d4ba08fa505ffc1139319","IPY_MODEL_3d8c51ca316d4616bd73b10ee4594dc8"],"layout":"IPY_MODEL_03606a585aca417789f15cd8eb22a83d"}},"c3776d21fe1e4d548136798a2b9a99db":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c858af8b16d341a3a02cb4453c44441e","placeholder":"​","style":"IPY_MODEL_97c0333536ae4bdfae5ba03970da66fc","value":"Downloading builder script: 100%"}},"ddb12c314d5d4ba08fa505ffc1139319":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8721561ba3a94b308f6f37b2d5c0a9bd","max":6270,"min":0,"orientation":"horizontal","style":"IPY_MODEL_78e17b2285304afd9ef8f5610c16db57","value":6270}},"3d8c51ca316d4616bd73b10ee4594dc8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff01969a80cd4026ac86359ac64f2203","placeholder":"​","style":"IPY_MODEL_c1d4f6f5066141edbe65664598e69d38","value":" 6.27k/6.27k [00:00&lt;00:00, 490kB/s]"}},"03606a585aca417789f15cd8eb22a83d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c858af8b16d341a3a02cb4453c44441e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97c0333536ae4bdfae5ba03970da66fc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8721561ba3a94b308f6f37b2d5c0a9bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78e17b2285304afd9ef8f5610c16db57":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ff01969a80cd4026ac86359ac64f2203":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1d4f6f5066141edbe65664598e69d38":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["# @title Start installing and importing the required libraries (session restart may be needed)\n","import torch\n","print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n","print(f\"CUDA version: {torch.version.cuda}\")\n","print(f\"PyTorch version: {torch.__version__}\")\n","\n","# Ensure the Google Colab runtime is using GPU\n","!pip install fsspec\n","\n","!pip install torch\n","!pip install scikit-learn\n","!pip install tokenizers\n","!pip install transformers\n","!pip install bert-score\n","!pip install rouge-score\n","!pip install sacrebleu\n","!pip install evaluate\n","!pip install tabulate\n","!pip install comet_ml\n","!pip install pyspellchecker\n","!pip install datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7wQBMVimxMEo","executionInfo":{"status":"ok","timestamp":1738530948660,"user_tz":-60,"elapsed":188507,"user":{"displayName":"Michele Sgrillo","userId":"00105951137606608722"}},"outputId":"04ceaf1a-cee5-48a7-9e83-ce2281fdd8bd","collapsed":true,"cellView":"form"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Is CUDA available: True\n","CUDA version: 12.4\n","PyTorch version: 2.5.1+cu124\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (2024.10.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers) (0.27.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.17.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.12.14)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n","Collecting bert-score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.5.1+cu124)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.47.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert-score) (1.26.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.32.3)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.67.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score) (24.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.17.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2024.10.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.3.1.170)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.27.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.55.7)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.2.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2024.12.14)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n","Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: bert-score\n","Successfully installed bert-score-0.3.13\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.26.4)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n","Building wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=a2324aa05bf7e2658492a703f9a6c9c6013dd0bf20abb6448eba9764482a4c5d\n","  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n","Successfully built rouge-score\n","Installing collected packages: rouge-score\n","Successfully installed rouge-score-0.1.2\n","Collecting sacrebleu\n","  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting portalocker (from sacrebleu)\n","  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\n","Collecting colorama (from sacrebleu)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.0)\n","Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n","Installing collected packages: portalocker, colorama, sacrebleu\n","Successfully installed colorama-0.4.6 portalocker-3.1.1 sacrebleu-2.5.1\n","Collecting evaluate\n","  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n","Collecting datasets>=2.0.0 (from evaluate)\n","  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n","Collecting dill (from evaluate)\n","  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n","Collecting xxhash (from evaluate)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess (from evaluate)\n","  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.10.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.27.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n","Collecting dill (from evaluate)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Collecting multiprocess (from evaluate)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n","  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.11)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2024.12.14)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n","Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, evaluate\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2024.10.0\n","    Uninstalling fsspec-2024.10.0:\n","      Successfully uninstalled fsspec-2024.10.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (0.9.0)\n","Collecting comet_ml\n","  Downloading comet_ml-3.48.1-py3-none-any.whl.metadata (3.8 kB)\n","Collecting everett<3.2.0,>=1.0.1 (from everett[ini]<3.2.0,>=1.0.1->comet_ml)\n","  Downloading everett-3.1.0-py2.py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (4.23.0)\n","Requirement already satisfied: psutil>=5.6.3 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (5.9.5)\n","Collecting python-box<7.0.0 (from comet_ml)\n","  Downloading python_box-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.8 kB)\n","Requirement already satisfied: requests-toolbelt>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (1.0.0)\n","Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (2.32.3)\n","Collecting semantic-version>=2.8.0 (from comet_ml)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n","Requirement already satisfied: sentry-sdk>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (2.20.0)\n","Collecting simplejson (from comet_ml)\n","  Downloading simplejson-3.19.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n","Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (2.3.0)\n","Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (1.17.2)\n","Collecting wurlitzer>=1.0.2 (from comet_ml)\n","  Downloading wurlitzer-3.1.1-py3-none-any.whl.metadata (2.5 kB)\n","Collecting dulwich!=0.20.33,>=0.20.6 (from comet_ml)\n","  Downloading dulwich-0.22.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n","Requirement already satisfied: rich>=13.3.2 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (13.9.4)\n","Collecting configobj (from everett[ini]<3.2.0,>=1.0.1->comet_ml)\n","  Downloading configobj-5.0.9-py2.py3-none-any.whl.metadata (3.2 kB)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (25.1.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.22.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.18.4->comet_ml) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.18.4->comet_ml) (3.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.18.4->comet_ml) (2024.12.14)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.3.2->comet_ml) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.3.2->comet_ml) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet_ml) (0.1.2)\n","Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing>=0.28.4->jsonschema!=3.1.0,>=2.6.0->comet_ml) (4.12.2)\n","Downloading comet_ml-3.48.1-py3-none-any.whl (710 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m710.5/710.5 kB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dulwich-0.22.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (979 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m979.7/979.7 kB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading everett-3.1.0-py2.py3-none-any.whl (35 kB)\n","Downloading python_box-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Downloading wurlitzer-3.1.1-py3-none-any.whl (8.6 kB)\n","Downloading simplejson-3.19.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading configobj-5.0.9-py2.py3-none-any.whl (35 kB)\n","Installing collected packages: everett, wurlitzer, simplejson, semantic-version, python-box, dulwich, configobj, comet_ml\n","  Attempting uninstall: python-box\n","    Found existing installation: python-box 7.3.2\n","    Uninstalling python-box-7.3.2:\n","      Successfully uninstalled python-box-7.3.2\n","Successfully installed comet_ml-3.48.1 configobj-5.0.9 dulwich-0.22.7 everett-3.1.0 python-box-6.1.0 semantic-version-2.10.0 simplejson-3.19.3 wurlitzer-3.1.1\n","Collecting pyspellchecker\n","  Downloading pyspellchecker-0.8.2-py3-none-any.whl.metadata (9.4 kB)\n","Downloading pyspellchecker-0.8.2-py3-none-any.whl (7.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyspellchecker\n","Successfully installed pyspellchecker-0.8.2\n","Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"]}]},{"cell_type":"code","source":["# @title Reorganized imports\n","import argparse\n","import csv\n","import logging\n","import os\n","import pickle\n","import random\n","import sys\n","import time\n","from typing import List, Optional, Tuple, Union, Dict\n","import comet_ml\n","import re\n","from spellchecker import SpellChecker\n","\n","# Third-party libraries\n","import evaluate\n","import numpy as np\n","import pandas as pd\n","import rouge_score\n","import sacrebleu\n","import sklearn\n","import tokenizers\n","import transformers\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","from torch import nn, Tensor\n","from torch.optim import AdamW\n","from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n","from torch.utils.data import DataLoader, Dataset\n","from tqdm import tqdm\n","from transformers import (\n","    AutoModelForSeq2SeqLM,\n","    AutoModelForSequenceClassification,\n","    DistilBertTokenizer,\n","    DistilBertForSequenceClassification,\n","    AutoTokenizer,\n","    DebertaForSequenceClassification,\n","    DebertaTokenizer,\n","    get_scheduler,\n","    Trainer,\n","    TrainingArguments\n",")\n","\n","logging.basicConfig(level=logging.DEBUG)\n","\n","from sklearn.metrics import classification_report"],"metadata":{"id":"o5fe87bOzV3w","cellView":"form","executionInfo":{"status":"ok","timestamp":1738530984180,"user_tz":-60,"elapsed":35524,"user":{"displayName":"Michele Sgrillo","userId":"00105951137606608722"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# @title Mount Google Drive on Colab for persistent storage\n","from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EOfdVk46xYP4","executionInfo":{"status":"ok","timestamp":1738531012603,"user_tz":-60,"elapsed":28430,"user":{"displayName":"Michele Sgrillo","userId":"00105951137606608722"}},"outputId":"87d96796-499f-4628-a562-2d3f2afdb51b","cellView":"form"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## Preprocessing for GAN and Classifier model"],"metadata":{"id":"xiLmrIu9ZRRG"}},{"cell_type":"code","source":["# @title Preprocessing on Trump and Shakespeare Train Eval and Test\n","\n","# Initialize spell checker\n","spell = SpellChecker()\n","\n","# Function to clean and correct text line by line\n","def correct_text_line(line):\n","    # Fix common OCR and formatting errors\n","    line = re.sub(r'(\\s[oO]Jce\\s|\\soJce\\s)', ' once ', line, flags=re.IGNORECASE)  # Fix OCR \"oJce\" -> \"once\"\n","    line = re.sub(r'([a-zA-Z])(\\d+)', r'\\1', line)  # Remove digits mixed with letters\n","    line = re.sub(r'\\s+', ' ', line).strip()  # Normalize spaces\n","\n","    # Tokenize while keeping punctuation\n","    tokens = re.findall(r\"[\\w']+|[.,!?\\'-]\", line)\n","\n","    corrected_tokens = []\n","    for i, token in enumerate(tokens):\n","        if token.isalpha():  # Only process alphabetic words\n","            corrected_word = spell.correction(token)\n","            corrected_word = corrected_word if corrected_word is not None else token\n","\n","            # Handle capitalization:\n","            if i == 0 or (i > 0 and tokens[i - 1] in \".!?\"):  # Capitalize first word of a sentence\n","                corrected_word = corrected_word.capitalize()\n","            elif token.lower() == \"i\":  # Ensure 'i' is always capitalized\n","                corrected_word = \"I\"\n","\n","            corrected_tokens.append(corrected_word)\n","        else:\n","            corrected_tokens.append(token)  # Keep punctuation as-is\n","\n","    # Join words carefully, ensuring spacing is correct\n","    return ''.join(\n","        corrected_tokens[i] if corrected_tokens[i] in \".,!?'-\" else ' ' + corrected_tokens[i]\n","        for i in range(len(corrected_tokens))\n","    ).strip()\n","\n","# Define base paths\n","base_input_path = \"/content/drive/MyDrive/ProjectNLP/20250112_Autori/Processed/\"\n","base_output_path = \"/content/drive/MyDrive/ProjectNLP/20250112_Autori/Processed/\"\n","\n","# Define file categories\n","categories = [\"train\", \"eval\", \"test\"]\n","authors = [\"shakespeare\", \"trump\"]\n","\n","# Process each file type (train, eval, test) for both authors\n","for category in categories:\n","    for author in authors:\n","        input_file_path = f\"{base_input_path}{category}_{author}.txt\"\n","        output_file_path = f\"{base_output_path}{category}_{author}_spellchecked.txt\"\n","\n","        with open(input_file_path, \"r\", encoding=\"utf-8\") as infile, open(output_file_path, \"w\", encoding=\"utf-8\") as outfile:\n","            for line in infile:\n","                corrected_line = correct_text_line(line)  # Apply spell-checking\n","                outfile.write(corrected_line + \"\\n\")\n","\n","        print(f\"Processing complete. Corrected file saved at: {output_file_path}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2xQTOL8QBuiV","executionInfo":{"status":"ok","timestamp":1738265460940,"user_tz":-60,"elapsed":428605,"user":{"displayName":"Francesco La Rota","userId":"12370830885715187383"}},"outputId":"beda82f6-f2a7-45ff-f8fd-06a79de5a86c","cellView":"form"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing complete. Corrected file saved at: /content/drive/MyDrive/ProjectNLP/20250112_Autori/Processed/train_shakespeare_spellchecked.txt\n","Processing complete. Corrected file saved at: /content/drive/MyDrive/ProjectNLP/20250112_Autori/Processed/train_trump_spellchecked.txt\n","Processing complete. Corrected file saved at: /content/drive/MyDrive/ProjectNLP/20250112_Autori/Processed/eval_shakespeare_spellchecked.txt\n","Processing complete. Corrected file saved at: /content/drive/MyDrive/ProjectNLP/20250112_Autori/Processed/eval_trump_spellchecked.txt\n","Processing complete. Corrected file saved at: /content/drive/MyDrive/ProjectNLP/20250112_Autori/Processed/test_shakespeare_spellchecked.txt\n","Processing complete. Corrected file saved at: /content/drive/MyDrive/ProjectNLP/20250112_Autori/Processed/test_trump_spellchecked.txt\n"]}]},{"cell_type":"code","source":["#@title copy and rename files\n","\n","import shutil\n","import os\n","\n","# Define source and destination directories\n","source_dir = \"/content/drive/MyDrive/ProjectNLP/20250112_Autori/Processed/\"\n","destination_dir = \"/content/drive/MyDrive/ProjectNLP/20250112_Autori/UltimoClassifier/Data/\"\n","\n","# Ensure the destination directory exists\n","os.makedirs(destination_dir, exist_ok=True)\n","\n","# Define file categories and labels\n","categories = {\n","    \"train\": \"train\",\n","    \"eval\": \"dev\",\n","    \"test\": \"test\"\n","}\n","authors = {\n","    \"shakespeare\": \"1\",\n","    \"trump\": \"0\"\n","}\n","\n","# Copy and rename files\n","for category, new_category in categories.items():\n","    for author, label in authors.items():\n","        src_file = f\"{source_dir}{category}_{author}_spellchecked.txt\"\n","        dest_file = f\"{destination_dir}{new_category}.{label}.txt\"  # Rename format\n","\n","        shutil.copy(src_file, dest_file)\n","        print(f\"Copied and renamed {src_file} to {dest_file}\")\n","\n","print(\"✅ All files copied and renamed successfully!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zge6ZUdUtgHM","executionInfo":{"status":"ok","timestamp":1738265857671,"user_tz":-60,"elapsed":445,"user":{"displayName":"Francesco La Rota","userId":"12370830885715187383"}},"outputId":"d66c9b23-bc81-4524-fc57-7f9203327ca2","cellView":"form"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Copied and renamed /content/drive/MyDrive/ProjectNLP/20250112_Autori/Processed/train_shakespeare_spellchecked.txt to /content/drive/MyDrive/ProjectNLP/20250112_Autori/UltimoClassifier/Data/train.1.txt\n","Copied and renamed /content/drive/MyDrive/ProjectNLP/20250112_Autori/Processed/train_trump_spellchecked.txt to /content/drive/MyDrive/ProjectNLP/20250112_Autori/UltimoClassifier/Data/train.0.txt\n","Copied and renamed /content/drive/MyDrive/ProjectNLP/20250112_Autori/Processed/eval_shakespeare_spellchecked.txt to /content/drive/MyDrive/ProjectNLP/20250112_Autori/UltimoClassifier/Data/dev.1.txt\n","Copied and renamed /content/drive/MyDrive/ProjectNLP/20250112_Autori/Processed/eval_trump_spellchecked.txt to /content/drive/MyDrive/ProjectNLP/20250112_Autori/UltimoClassifier/Data/dev.0.txt\n","Copied and renamed /content/drive/MyDrive/ProjectNLP/20250112_Autori/Processed/test_shakespeare_spellchecked.txt to /content/drive/MyDrive/ProjectNLP/20250112_Autori/UltimoClassifier/Data/test.1.txt\n","Copied and renamed /content/drive/MyDrive/ProjectNLP/20250112_Autori/Processed/test_trump_spellchecked.txt to /content/drive/MyDrive/ProjectNLP/20250112_Autori/UltimoClassifier/Data/test.0.txt\n","✅ All files copied and renamed successfully!\n"]}]},{"cell_type":"markdown","source":["# Binary Experiments"],"metadata":{"id":"tgr557E6-IG_"}},{"cell_type":"markdown","source":["## Training the Classifier"],"metadata":{"id":"4Sf_I0QSTM8r"}},{"cell_type":"code","source":["!CUDA_VISIBLE_DEVICES=0 python \"/content/drive/MyDrive/ProjectNLP/02.Training Parallelo Shakespeare/GAN_Originale/my_utils/train_classifier.py\" \\\n","  --dataset_path \"/content/drive/MyDrive/ProjectNLP/20250112_Autori/UltimoClassifier/Data/\" \\\n","  --max_samples_train 5000 \\\n","  --max_samples_eval 1000 \\\n","  --lowercase \\\n","  --max_sequence_length 32 \\\n","  --batch_size 32 \\\n","  --use_cuda_if_available \\\n","  --learning_rate 2e-5 \\\n","  --epochs 10 \\\n","  --lr_scheduler_type \"linear\" \\\n","  --model_tag \"distilbert/distilbert-base-cased\" \\\n","  --save_base_folder \"/content/drive/MyDrive/ProjectNLP/20250112_Autori/UltimoClassifier/\" \\\n","  --save_steps 1 \\\n","  --eval_strategy \"epochs\" \\\n","  --eval_steps 1\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mYWPXI5aa3wG","executionInfo":{"status":"ok","timestamp":1738266688811,"user_tz":-60,"elapsed":270029,"user":{"displayName":"Francesco La Rota","userId":"12370830885715187383"}},"outputId":"2a972623-8b7a-44be-b1b4-3ee75a2f25c5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-01-30 19:47:05.826695: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1738266425.850038   73413 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1738266425.856550   73413 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","Arguments summary: \n"," \n","\tmax_samples_train:\t\t5000\n","\tmax_samples_eval:\t\t1000\n","\tdataset_path:\t\t/content/drive/MyDrive/ProjectNLP/20250112_Autori/UltimoClassifier/Data/\n","\tlowercase:\t\tTrue\n","\tmax_sequence_length:\t\t32\n","\tbatch_size:\t\t32\n","\tuse_cuda_if_available:\t\tTrue\n","\tlearning_rate:\t\t2e-05\n","\tepochs:\t\t10\n","\tlr_scheduler_type:\t\tlinear\n","\tmodel_tag:\t\tdistilbert/distilbert-base-cased\n","\tsave_base_folder:\t\t/content/drive/MyDrive/ProjectNLP/20250112_Autori/UltimoClassifier/\n","\tsave_steps:\t\t1\n","\teval_strategy:\t\tepochs\n","\teval_steps:\t\t1\n","\tcomet_logging:\t\tFalse\n","\tcomet_key:\t\tNone\n","\tcomet_workspace:\t\tNone\n","\tcomet_project_name:\t\tNone\n","Training set: 5000\n","Evaluation set: 718\n","Test set: 718\n","Training set (batches): 157\n","Evaluation set (batches): 23\n","Test set (batches): 23\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","comet_ml is installed but the Comet API Key is not configured. Please set the `COMET_API_KEY` environment variable to enable Comet logging. Check out the documentation for other ways of configuring it: https://www.comet.com/docs/v2/guides/experiment-management/configure-sdk/#set-the-api-key\n","Total number of training steps: 1570\n","Start training...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.4\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250130_194948-5q7pb2m7\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/content/drive/MyDrive/ProjectNLP/20250112_Autori/UltimoClassifier/classifiers/Data/distilbert/distilbert-base-cased_10/checkpoints/\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/polibho/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/polibho/huggingface/runs/5q7pb2m7\u001b[0m\n","{'loss': 0.6866, 'grad_norm': 1.0773041248321533, 'learning_rate': 2e-05, 'epoch': 0.06}\n","{'loss': 0.6238, 'grad_norm': 2.311925172805786, 'learning_rate': 1.9871794871794873e-05, 'epoch': 0.13}\n","{'loss': 0.4494, 'grad_norm': 4.428264617919922, 'learning_rate': 1.9743589743589745e-05, 'epoch': 0.19}\n","{'loss': 0.3758, 'grad_norm': 4.115257263183594, 'learning_rate': 1.9615384615384617e-05, 'epoch': 0.25}\n","{'loss': 0.2873, 'grad_norm': 3.8217275142669678, 'learning_rate': 1.9487179487179488e-05, 'epoch': 0.32}\n","{'loss': 0.2958, 'grad_norm': 4.103095054626465, 'learning_rate': 1.935897435897436e-05, 'epoch': 0.38}\n","{'loss': 0.3021, 'grad_norm': 4.414221286773682, 'learning_rate': 1.923076923076923e-05, 'epoch': 0.45}\n","{'loss': 0.2327, 'grad_norm': 5.50068473815918, 'learning_rate': 1.9102564102564106e-05, 'epoch': 0.51}\n","{'loss': 0.3224, 'grad_norm': 10.066835403442383, 'learning_rate': 1.8974358974358975e-05, 'epoch': 0.57}\n","{'loss': 0.3211, 'grad_norm': 3.2865686416625977, 'learning_rate': 1.8846153846153846e-05, 'epoch': 0.64}\n","{'loss': 0.2422, 'grad_norm': 5.596362113952637, 'learning_rate': 1.8717948717948718e-05, 'epoch': 0.7}\n","{'loss': 0.2931, 'grad_norm': 3.186145067214966, 'learning_rate': 1.8589743589743593e-05, 'epoch': 0.76}\n","{'loss': 0.2706, 'grad_norm': 5.749700546264648, 'learning_rate': 1.8461538461538465e-05, 'epoch': 0.83}\n","{'loss': 0.2406, 'grad_norm': 2.4397811889648438, 'learning_rate': 1.8333333333333333e-05, 'epoch': 0.89}\n","{'loss': 0.2968, 'grad_norm': 6.86102294921875, 'learning_rate': 1.8205128205128208e-05, 'epoch': 0.96}\n"," 10% 157/1570 [00:16<02:10, 10.85it/s]\n","  0% 0/23 [00:00<?, ?it/s]\u001b[A\n"," 22% 5/23 [00:00<00:00, 45.83it/s]\u001b[A\n"," 43% 10/23 [00:00<00:00, 40.68it/s]\u001b[A\n"," 65% 15/23 [00:00<00:00, 39.15it/s]\u001b[A\n","                                      \n","\u001b[A{'eval_loss': 0.2064315229654312, 'eval_acc': 0.9164345403899722, 'eval_prec': 0.9183165971959075, 'eval_rec': 0.9138472352389878, 'eval_f1_macro': 0.9154876527225217, 'eval_runtime': 0.6228, 'eval_samples_per_second': 1152.842, 'eval_steps_per_second': 36.929, 'epoch': 1.0}\n"," 10% 157/1570 [00:16<02:10, 10.85it/s]\n","100% 23/23 [00:00<00:00, 38.62it/s]\u001b[A\n","{'loss': 0.2713, 'grad_norm': 7.944751262664795, 'learning_rate': 1.807692307692308e-05, 'epoch': 1.02}\n","{'loss': 0.1954, 'grad_norm': 3.7240488529205322, 'learning_rate': 1.794871794871795e-05, 'epoch': 1.08}\n","{'loss': 0.1805, 'grad_norm': 4.585639953613281, 'learning_rate': 1.7820512820512823e-05, 'epoch': 1.15}\n","{'loss': 0.1715, 'grad_norm': 3.8897762298583984, 'learning_rate': 1.7692307692307694e-05, 'epoch': 1.21}\n","{'loss': 0.1834, 'grad_norm': 2.661273717880249, 'learning_rate': 1.7564102564102566e-05, 'epoch': 1.27}\n","{'loss': 0.2468, 'grad_norm': 4.874606132507324, 'learning_rate': 1.7435897435897438e-05, 'epoch': 1.34}\n","{'loss': 0.1746, 'grad_norm': 2.8246686458587646, 'learning_rate': 1.730769230769231e-05, 'epoch': 1.4}\n","{'loss': 0.2073, 'grad_norm': 4.3806939125061035, 'learning_rate': 1.717948717948718e-05, 'epoch': 1.46}\n","{'loss': 0.1752, 'grad_norm': 3.339327096939087, 'learning_rate': 1.7051282051282053e-05, 'epoch': 1.53}\n","{'loss': 0.1914, 'grad_norm': 2.5461575984954834, 'learning_rate': 1.6923076923076924e-05, 'epoch': 1.59}\n","{'loss': 0.1699, 'grad_norm': 2.8812241554260254, 'learning_rate': 1.6794871794871796e-05, 'epoch': 1.66}\n","{'loss': 0.1852, 'grad_norm': 12.9656343460083, 'learning_rate': 1.6666666666666667e-05, 'epoch': 1.72}\n","{'loss': 0.1711, 'grad_norm': 4.641510486602783, 'learning_rate': 1.653846153846154e-05, 'epoch': 1.78}\n","{'loss': 0.1762, 'grad_norm': 7.825736045837402, 'learning_rate': 1.641025641025641e-05, 'epoch': 1.85}\n","{'loss': 0.1957, 'grad_norm': 7.380385398864746, 'learning_rate': 1.6282051282051282e-05, 'epoch': 1.91}\n","{'loss': 0.1592, 'grad_norm': 3.083730697631836, 'learning_rate': 1.6153846153846154e-05, 'epoch': 1.97}\n"," 20% 314/1570 [00:41<01:54, 10.98it/s]\n","  0% 0/23 [00:00<?, ?it/s]\u001b[A\n"," 22% 5/23 [00:00<00:00, 43.70it/s]\u001b[A\n"," 43% 10/23 [00:00<00:00, 39.45it/s]\u001b[A\n"," 61% 14/23 [00:00<00:00, 38.57it/s]\u001b[A\n"," 78% 18/23 [00:00<00:00, 38.29it/s]\u001b[A\n","                                      \n","\u001b[A{'eval_loss': 0.20255640149116516, 'eval_acc': 0.9206128133704735, 'eval_prec': 0.9227703295227412, 'eval_rec': 0.9179397063417682, 'eval_f1_macro': 0.9196891649250876, 'eval_runtime': 0.6306, 'eval_samples_per_second': 1138.627, 'eval_steps_per_second': 36.474, 'epoch': 2.0}\n"," 20% 314/1570 [00:42<01:54, 10.98it/s]\n","100% 23/23 [00:00<00:00, 37.99it/s]\u001b[A\n","{'loss': 0.1688, 'grad_norm': 4.269021034240723, 'learning_rate': 1.602564102564103e-05, 'epoch': 2.04}\n","{'loss': 0.1308, 'grad_norm': 2.2783689498901367, 'learning_rate': 1.5897435897435897e-05, 'epoch': 2.1}\n","{'loss': 0.139, 'grad_norm': 1.3427904844284058, 'learning_rate': 1.576923076923077e-05, 'epoch': 2.17}\n","{'loss': 0.1158, 'grad_norm': 1.5698779821395874, 'learning_rate': 1.5641025641025644e-05, 'epoch': 2.23}\n","{'loss': 0.1281, 'grad_norm': 5.337438106536865, 'learning_rate': 1.5512820512820516e-05, 'epoch': 2.29}\n","{'loss': 0.1142, 'grad_norm': 8.010878562927246, 'learning_rate': 1.5384615384615387e-05, 'epoch': 2.36}\n","{'loss': 0.1787, 'grad_norm': 11.048998832702637, 'learning_rate': 1.5256410256410257e-05, 'epoch': 2.42}\n","{'loss': 0.1038, 'grad_norm': 1.3622153997421265, 'learning_rate': 1.5128205128205129e-05, 'epoch': 2.48}\n","{'loss': 0.0878, 'grad_norm': 3.9221742153167725, 'learning_rate': 1.5000000000000002e-05, 'epoch': 2.55}\n","{'loss': 0.122, 'grad_norm': 6.129832744598389, 'learning_rate': 1.4871794871794874e-05, 'epoch': 2.61}\n","{'loss': 0.133, 'grad_norm': 4.4004292488098145, 'learning_rate': 1.4743589743589745e-05, 'epoch': 2.68}\n","{'loss': 0.1478, 'grad_norm': 4.81823205947876, 'learning_rate': 1.4615384615384615e-05, 'epoch': 2.74}\n","{'loss': 0.1067, 'grad_norm': 6.689244747161865, 'learning_rate': 1.4487179487179489e-05, 'epoch': 2.8}\n","{'loss': 0.1521, 'grad_norm': 5.373281002044678, 'learning_rate': 1.435897435897436e-05, 'epoch': 2.87}\n","{'loss': 0.1048, 'grad_norm': 5.936906814575195, 'learning_rate': 1.4230769230769232e-05, 'epoch': 2.93}\n","{'loss': 0.144, 'grad_norm': 3.80079984664917, 'learning_rate': 1.4102564102564105e-05, 'epoch': 2.99}\n"," 30% 471/1570 [01:02<01:38, 11.13it/s]\n","  0% 0/23 [00:00<?, ?it/s]\u001b[A\n"," 22% 5/23 [00:00<00:00, 43.50it/s]\u001b[A\n"," 43% 10/23 [00:00<00:00, 38.37it/s]\u001b[A\n"," 61% 14/23 [00:00<00:00, 37.59it/s]\u001b[A\n"," 78% 18/23 [00:00<00:00, 37.12it/s]\u001b[A\n","                                      \n","\u001b[A{'eval_loss': 0.22585290670394897, 'eval_acc': 0.9122562674094707, 'eval_prec': 0.9123525492808715, 'eval_rec': 0.9108872227428928, 'eval_f1_macro': 0.9115312175458978, 'eval_runtime': 0.6473, 'eval_samples_per_second': 1109.158, 'eval_steps_per_second': 35.53, 'epoch': 3.0}\n"," 30% 471/1570 [01:02<01:38, 11.13it/s]\n","100% 23/23 [00:00<00:00, 36.93it/s]\u001b[A\n","{'loss': 0.0781, 'grad_norm': 5.324106693267822, 'learning_rate': 1.3974358974358975e-05, 'epoch': 3.06}\n","{'loss': 0.0479, 'grad_norm': 2.416821241378784, 'learning_rate': 1.3846153846153847e-05, 'epoch': 3.12}\n","{'loss': 0.0802, 'grad_norm': 6.738986492156982, 'learning_rate': 1.3717948717948718e-05, 'epoch': 3.18}\n","{'loss': 0.0855, 'grad_norm': 1.5424216985702515, 'learning_rate': 1.3589743589743592e-05, 'epoch': 3.25}\n","{'loss': 0.0805, 'grad_norm': 19.083480834960938, 'learning_rate': 1.3461538461538463e-05, 'epoch': 3.31}\n","{'loss': 0.0998, 'grad_norm': 4.991396903991699, 'learning_rate': 1.3333333333333333e-05, 'epoch': 3.38}\n","{'loss': 0.1245, 'grad_norm': 3.8406643867492676, 'learning_rate': 1.3205128205128207e-05, 'epoch': 3.44}\n","{'loss': 0.0507, 'grad_norm': 0.26451051235198975, 'learning_rate': 1.3076923076923078e-05, 'epoch': 3.5}\n","{'loss': 0.0886, 'grad_norm': 8.263205528259277, 'learning_rate': 1.294871794871795e-05, 'epoch': 3.57}\n","{'loss': 0.1117, 'grad_norm': 0.9519112706184387, 'learning_rate': 1.2820512820512823e-05, 'epoch': 3.63}\n","{'loss': 0.1079, 'grad_norm': 4.4320244789123535, 'learning_rate': 1.2692307692307693e-05, 'epoch': 3.69}\n","{'loss': 0.0669, 'grad_norm': 0.5868911743164062, 'learning_rate': 1.2564102564102565e-05, 'epoch': 3.76}\n","{'loss': 0.1159, 'grad_norm': 5.75502872467041, 'learning_rate': 1.2435897435897436e-05, 'epoch': 3.82}\n","{'loss': 0.0465, 'grad_norm': 4.811863422393799, 'learning_rate': 1.230769230769231e-05, 'epoch': 3.89}\n","{'loss': 0.0432, 'grad_norm': 0.24750196933746338, 'learning_rate': 1.217948717948718e-05, 'epoch': 3.95}\n"," 40% 627/1570 [01:26<01:42,  9.18it/s]\n","  0% 0/23 [00:00<?, ?it/s]\u001b[A\n"," 22% 5/23 [00:00<00:00, 45.74it/s]\u001b[A\n"," 43% 10/23 [00:00<00:00, 38.61it/s]\u001b[A\n"," 61% 14/23 [00:00<00:00, 37.55it/s]\u001b[A\n"," 78% 18/23 [00:00<00:00, 36.76it/s]\u001b[A\n","                                      \n","\u001b[A{'eval_loss': 0.33578866720199585, 'eval_acc': 0.9178272980501393, 'eval_prec': 0.9191736578833354, 'eval_rec': 0.9155888784754764, 'eval_f1_macro': 0.9169690020482364, 'eval_runtime': 0.648, 'eval_samples_per_second': 1107.984, 'eval_steps_per_second': 35.493, 'epoch': 4.0}\n"," 40% 628/1570 [01:26<01:42,  9.18it/s]\n","100% 23/23 [00:00<00:00, 36.60it/s]\u001b[A\n","{'train_runtime': 248.6703, 'train_samples_per_second': 201.069, 'train_steps_per_second': 6.314, 'train_loss': 0.18620415629854628, 'epoch': 4.0}\n"," 40% 628/1570 [01:32<02:18,  6.82it/s]\n","End training...\n","Start test...\n","100% 23/23 [00:00<00:00, 35.56it/s]\n","End test...\n","              precision    recall  f1-score   support\n","\n","           0       0.91      0.93      0.92       388\n","           1       0.91      0.89      0.90       330\n","\n","    accuracy                           0.91       718\n","   macro avg       0.91      0.91      0.91       718\n","weighted avg       0.91      0.91      0.91       718\n","\n","\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33m/content/drive/MyDrive/ProjectNLP/20250112_Autori/UltimoClassifier/classifiers/Data/distilbert/distilbert-base-cased_10/checkpoints/\u001b[0m at: \u001b[34mhttps://wandb.ai/polibho/huggingface/runs/5q7pb2m7\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250130_194948-5q7pb2m7/logs\u001b[0m\n"]}]},{"cell_type":"code","source":["# @title Manual inference for classifier for visual inspection\n","\n","import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","# Define model and tokenizer paths\n","model_path = \"/content/drive/MyDrive/ProjectNLP/20250112_Autori/UltimoClassifier/classifiers/Data/distilbert/distilbert-base-cased_10/checkpoints/checkpoint-314/\"  # Update if different\n","tokenizer_path = \"/content/drive/MyDrive/ProjectNLP/20250112_Autori/UltimoClassifier/classifiers/Data/distilbert/distilbert-base-cased_10/checkpoints/checkpoint-314/\"  # Tokenizer is usually saved with the model\n","\n","# Load the trained model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\n","tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n","\n","# Function to classify a sentence\n","def classify_sentence(sentence):\n","    # Tokenize input\n","    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=32).to(device)\n","\n","    # Get model prediction\n","    with torch.no_grad():\n","        logits = model(**inputs).logits\n","\n","    # Convert logits to probabilities\n","    probabilities = torch.nn.functional.softmax(logits, dim=1)\n","    predicted_class = torch.argmax(probabilities, dim=1).item()\n","\n","    # Labels\n","    labels = {0: \"Trump\", 1: \"Shakespeare\"}\n","\n","    return labels[predicted_class], probabilities.cpu().numpy()\n","\n","# Test the classifier\n","sentence = \"When we're gonna declare war to Ukraine\"\n","prediction, _ = classify_sentence(sentence)\n","print(f\"Sentence: {sentence}\")\n","print(f\"Predicted Class: {prediction}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mPT48pifvtRw","executionInfo":{"status":"ok","timestamp":1738267386558,"user_tz":-60,"elapsed":908,"user":{"displayName":"Francesco La Rota","userId":"12370830885715187383"}},"outputId":"1e875b30-11c9-470a-9086-b6a66d9d3052","cellView":"form"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentence: When we're gonna declare war to Ukraine\n","Predicted Class: Trump\n"]}]},{"cell_type":"markdown","source":["## Train the GAN"],"metadata":{"id":"2LfT96_hTIlk"}},{"cell_type":"markdown","source":["### Basic GAN Train"],"metadata":{"id":"6I346E6BEcUE"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vCTG2vjGxG3O","executionInfo":{"status":"ok","timestamp":1738301489014,"user_tz":-60,"elapsed":32757194,"user":{"displayName":"Michele Sgrillo","userId":"00105951137606608722"}},"outputId":"01e533a4-7aaa-4e3d-c389-3a1a6805bf30"},"outputs":[{"output_type":"stream","name":"stdout","text":["2025-01-30 20:25:46.213464: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1738268746.234139    2081 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1738268746.240605    2081 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","Arguments summary: \n"," \n","\tstyle_a:\t\ttrump\n","\tstyle_b:\t\tshakespeare\n","\tlang:\t\ten\n","\tmax_samples_train:\t\tNone\n","\tmax_samples_eval:\t\tNone\n","\tnonparal_same_size:\t\tFalse\n","\tpath_mono_A:\t\t/content/drive/MyDrive/ProjectNLP/20250112_Autori/Processed/train_trump_spellchecked.txt\n","\tpath_mono_B:\t\t/content/drive/MyDrive/ProjectNLP/20250112_Autori/Processed/train_shakespeare_spellchecked.txt\n","\tpath_mono_A_eval:\t\t/content/drive/MyDrive/ProjectNLP/20250112_Autori/Processed/eval_trump_spellchecked.txt\n","\tpath_mono_B_eval:\t\t/content/drive/MyDrive/ProjectNLP/20250112_Autori/Processed/eval_shakespeare_spellchecked.txt\n","\tpath_paral_A_eval:\t\tNone\n","\tpath_paral_B_eval:\t\tNone\n","\tpath_paral_eval_ref:\t\tNone\n","\tn_references:\t\tNone\n","\tlowercase_ref:\t\tFalse\n","\tbertscore:\t\tTrue\n","\tmax_sequence_length:\t\t32\n","\tbatch_size:\t\t16\n","\tshuffle:\t\tTrue\n","\tnum_workers:\t\t4\n","\tpin_memory:\t\tTrue\n","\tuse_cuda_if_available:\t\tTrue\n","\tlearning_rate:\t\t0.0001\n","\tepochs:\t\t20\n","\tlr_scheduler_type:\t\tlinear\n","\twarmup:\t\tFalse\n","\tlambdas:\t\t10|1|1|1|1\n","\tgenerator_model_tag:\t\tfacebook/bart-base\n","\tdiscriminator_model_tag:\t\tdistilbert/distilbert-base-cased\n","\tpretrained_classifier_model:\t\t/content/drive/MyDrive/ProjectNLP/20250112_Autori/UltimoClassifier/classifiers/Data/distilbert/distilbert-base-cased_10/checkpoints/checkpoint-314/\n","\tpretrained_classifier_eval:\t\t/content/drive/MyDrive/ProjectNLP/20250112_Autori/UltimoClassifier/classifiers/Data/distilbert/distilbert-base-cased_10/checkpoints/checkpoint-314/\n","\tsave_base_folder:\t\t/content/drive/MyDrive/ProjectNLP/02.Training Parallelo Shakespeare/Ultimi_Checkpoint/\n","\tfrom_pretrained:\t\tNone\n","\tsave_steps:\t\t1\n","\teval_strategy:\t\tepochs\n","\teval_steps:\t\t1\n","\tadditional_eval:\t\t0\n","\tcontrol_file:\t\tNone\n","\tlambda_file:\t\tNone\n","\tcomet_logging:\t\tFalse\n","\tcomet_key:\t\tNone\n","\tcomet_workspace:\t\tNone\n","\tcomet_project_name:\t\tNone\n","\tcomet_exp:\t\tNone\n","Mono A  : 6964\n","Mono B  : 5926\n","Mono A eval: 388\n","Mono B eval: 330\n","\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","Mono A eval (batches): 25\n","Mono B eval (batches): 21\n","config.json: 100% 1.72k/1.72k [00:00<00:00, 8.49MB/s]\n","model.safetensors: 100% 558M/558M [00:02<00:00, 232MB/s]\n","vocab.json: 100% 899k/899k [00:00<00:00, 22.3MB/s]\n","merges.txt: 100% 456k/456k [00:00<00:00, 34.8MB/s]\n","tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 5.36MB/s]\n","Generator pretrained models not loaded - Initial weights will be used\n","config.json: 100% 465/465 [00:00<00:00, 2.78MB/s]\n","model.safetensors: 100% 263M/263M [00:01<00:00, 191MB/s]\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","tokenizer_config.json: 100% 49.0/49.0 [00:00<00:00, 369kB/s]\n","vocab.txt: 100% 213k/213k [00:00<00:00, 60.3MB/s]\n","tokenizer.json: 100% 436k/436k [00:00<00:00, 34.7MB/s]\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Discriminator pretrained models not loaded - Initial weights will be used\n","Classifier pretrained model loaded correctly\n","Device: cuda\n","Total number of training steps: 7420\n","  0% 0/7420 [00:00<?, ?it/s]\n","Downloading builder script: 100% 8.15k/8.15k [00:00<00:00, 17.1MB/s]\n","\n","Downloading builder script: 100% 6.27k/6.27k [00:00<00:00, 23.0MB/s]\n","\n","Downloading builder script: 100% 7.95k/7.95k [00:00<00:00, 20.4MB/s]\n","Start training...\n","\n","Training epoch: 0\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n","  0% 5/7420 [00:28<10:30:53,  5.10s/it]Dummy classification metrics computation end\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n","  5% 371/7420 [25:10<6:33:41,  3.35s/it]Start validation...\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 88.9269964573777\n","self-BLEU B->A: 94.44799712811385\n","self-BLEU avg: 91.68749679274578\n","self-ROUGE-1 A->B: 0.9583129416846092\n","self-ROUGE-1 B->A: 0.989040212854534\n","self-ROUGE-2 A->B: 0.9446087639611275\n","self-ROUGE-2 B->A: 0.981665256522563\n","self-ROUGE-L A->B: 0.9581983941473812\n","self-ROUGE-L B->A: 0.989040212854534\n","style accuracy: 0.10167130919220056\n","acc-BLEU: 50.927313855982916\n","g-acc-BLEU: 30.531930557162863\n","h-acc-BLEU: 18.304495246525818\n","End validation...\n","/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","\n","Training epoch: 1\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 10% 742/7420 [51:54<6:27:47,  3.48s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 87.72067973971228\n","self-BLEU B->A: 89.87636153302714\n","self-BLEU avg: 88.79852063636972\n","self-ROUGE-1 A->B: 0.9523090170957853\n","self-ROUGE-1 B->A: 0.9770807765339244\n","self-ROUGE-2 A->B: 0.9382352050044244\n","self-ROUGE-2 B->A: 0.9662737760733014\n","self-ROUGE-L A->B: 0.9521918662054385\n","self-ROUGE-L B->A: 0.9770807765339244\n","style accuracy: 0.11281337047353761\n","acc-BLEU: 50.03992884186174\n","g-acc-BLEU: 31.650687837790905\n","h-acc-BLEU: 20.01933363577396\n","End validation...\n","\n","Training epoch: 2\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 15% 1113/7420 [1:18:32<6:24:01,  3.65s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 88.0233883016111\n","self-BLEU B->A: 88.57432008100783\n","self-BLEU avg: 88.29885419130946\n","self-ROUGE-1 A->B: 0.9549220762605719\n","self-ROUGE-1 B->A: 0.9717675252673864\n","self-ROUGE-2 A->B: 0.9396219367689436\n","self-ROUGE-2 B->A: 0.9604415304794969\n","self-ROUGE-L A->B: 0.9549220762605719\n","self-ROUGE-L B->A: 0.9717675252673864\n","style accuracy: 0.11838440111420613\n","acc-BLEU: 50.068647151365035\n","g-acc-BLEU: 32.33141966030688\n","h-acc-BLEU: 20.87774977526039\n","End validation...\n","\n","Training epoch: 3\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 20% 1484/7420 [1:45:21<5:58:34,  3.62s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 85.16657054282605\n","self-BLEU B->A: 85.00255541414373\n","self-BLEU avg: 85.08456297848488\n","self-ROUGE-1 A->B: 0.9433402696091324\n","self-ROUGE-1 B->A: 0.9629470124291062\n","self-ROUGE-2 A->B: 0.9233910759584513\n","self-ROUGE-2 B->A: 0.9478507467739451\n","self-ROUGE-L A->B: 0.9432529028434502\n","self-ROUGE-L B->A: 0.9629470124291062\n","style accuracy: 0.13788300835654596\n","acc-BLEU: 49.43643190706974\n","g-acc-BLEU: 34.25159194574099\n","h-acc-BLEU: 23.730910458048704\n","End validation...\n","\n","Training epoch: 4\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 25% 1855/7420 [2:12:08<5:36:01,  3.62s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 84.42536997551238\n","self-BLEU B->A: 85.08514839859907\n","self-BLEU avg: 84.75525918705573\n","self-ROUGE-1 A->B: 0.9363881918640357\n","self-ROUGE-1 B->A: 0.9526015049395282\n","self-ROUGE-2 A->B: 0.9100742693260632\n","self-ROUGE-2 B->A: 0.9295207076024815\n","self-ROUGE-L A->B: 0.9363881918640357\n","self-ROUGE-L B->A: 0.9526015049395282\n","style accuracy: 0.14484679665738162\n","acc-BLEU: 49.61996942639694\n","g-acc-BLEU: 35.037876352757365\n","h-acc-BLEU: 24.74110284895647\n","End validation...\n","\n","Training epoch: 5\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 30% 2226/7420 [2:38:48<5:13:37,  3.62s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 84.1230240054101\n","self-BLEU B->A: 83.39696298771602\n","self-BLEU avg: 83.75999349656306\n","self-ROUGE-1 A->B: 0.9338022043676188\n","self-ROUGE-1 B->A: 0.9490087965101013\n","self-ROUGE-2 A->B: 0.9069399395781997\n","self-ROUGE-2 B->A: 0.9097043742342711\n","self-ROUGE-L A->B: 0.9338022043676188\n","self-ROUGE-L B->A: 0.9490087965101013\n","style accuracy: 0.149025069637883\n","acc-BLEU: 49.33125023017568\n","g-acc-BLEU: 35.33035361228916\n","h-acc-BLEU: 25.3031064060546\n","End validation...\n","\n","Training epoch: 6\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 35% 2597/7420 [3:05:35<4:29:17,  3.35s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 82.39161529563806\n","self-BLEU B->A: 79.21777482381489\n","self-BLEU avg: 80.80469505972647\n","self-ROUGE-1 A->B: 0.9246382142936032\n","self-ROUGE-1 B->A: 0.9220117174717688\n","self-ROUGE-2 A->B: 0.8885247506570312\n","self-ROUGE-2 B->A: 0.8732498045310632\n","self-ROUGE-L A->B: 0.9246382142936032\n","self-ROUGE-L B->A: 0.9220117174717688\n","style accuracy: 0.17688022284122562\n","acc-BLEU: 49.24635867192451\n","g-acc-BLEU: 37.80575679546926\n","h-acc-BLEU: 29.022962730877452\n","End validation...\n","\n","Training epoch: 7\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 40% 2968/7420 [3:32:18<4:18:02,  3.48s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 79.86359163172233\n","self-BLEU B->A: 78.20782502228663\n","self-BLEU avg: 79.03570832700447\n","self-ROUGE-1 A->B: 0.9113746307101066\n","self-ROUGE-1 B->A: 0.9174622101870534\n","self-ROUGE-2 A->B: 0.8667182721535832\n","self-ROUGE-2 B->A: 0.8648399446138201\n","self-ROUGE-L A->B: 0.9113746307101066\n","self-ROUGE-L B->A: 0.9174622101870534\n","style accuracy: 0.20055710306406685\n","acc-BLEU: 49.545709316705576\n","g-acc-BLEU: 39.81353124338578\n","h-acc-BLEU: 31.99302777035993\n","End validation...\n","\n","Training epoch: 8\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 45% 3339/7420 [3:59:02<4:19:48,  3.82s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 76.29040693754648\n","self-BLEU B->A: 73.20071722717486\n","self-BLEU avg: 74.74556208236066\n","self-ROUGE-1 A->B: 0.889645957266548\n","self-ROUGE-1 B->A: 0.880944266527386\n","self-ROUGE-2 A->B: 0.8340878650698236\n","self-ROUGE-2 B->A: 0.8198785437919027\n","self-ROUGE-L A->B: 0.889645957266548\n","self-ROUGE-L B->A: 0.880944266527386\n","style accuracy: 0.24512534818941503\n","acc-BLEU: 49.629048450651084\n","g-acc-BLEU: 42.80424269982147\n","h-acc-BLEU: 36.91795897453286\n","End validation...\n","\n","Training epoch: 9\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 50% 3710/7420 [4:25:58<3:38:24,  3.53s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 71.59389761864816\n","self-BLEU B->A: 72.28590604446603\n","self-BLEU avg: 71.9399018315571\n","self-ROUGE-1 A->B: 0.8663430127464995\n","self-ROUGE-1 B->A: 0.8797072344494271\n","self-ROUGE-2 A->B: 0.7948149416736041\n","self-ROUGE-2 B->A: 0.8078431788086032\n","self-ROUGE-L A->B: 0.8661868115593705\n","self-ROUGE-L B->A: 0.8797072344494271\n","style accuracy: 0.26880222841225626\n","acc-BLEU: 49.41006233639136\n","g-acc-BLEU: 43.97454482320596\n","h-acc-BLEU: 39.136979016021456\n","End validation...\n","\n","Training epoch: 10\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 55% 4081/7420 [4:53:00<3:10:12,  3.42s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 69.16756450321694\n","self-BLEU B->A: 69.10646181348311\n","self-BLEU avg: 69.13701315835002\n","self-ROUGE-1 A->B: 0.8528615043667502\n","self-ROUGE-1 B->A: 0.8585108219883223\n","self-ROUGE-2 A->B: 0.776431498969533\n","self-ROUGE-2 B->A: 0.7752003427179609\n","self-ROUGE-L A->B: 0.852509891390715\n","self-ROUGE-L B->A: 0.8580904693940565\n","style accuracy: 0.2841225626740947\n","acc-BLEU: 48.774634712879745\n","g-acc-BLEU: 44.32085892013264\n","h-acc-BLEU: 40.2737719481618\n","End validation...\n","\n","Training epoch: 11\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 60% 4452/7420 [5:20:26<2:42:29,  3.28s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 64.79114401914157\n","self-BLEU B->A: 61.95515699898393\n","self-BLEU avg: 63.373150509062754\n","self-ROUGE-1 A->B: 0.831286805897314\n","self-ROUGE-1 B->A: 0.8125204376762722\n","self-ROUGE-2 A->B: 0.7357201656551331\n","self-ROUGE-2 B->A: 0.7084372484939815\n","self-ROUGE-L A->B: 0.8310451821859738\n","self-ROUGE-L B->A: 0.8115345924272643\n","style accuracy: 0.3579387186629526\n","acc-BLEU: 49.58351118767901\n","g-acc-BLEU: 47.62741258020255\n","h-acc-BLEU: 45.748482749125316\n","End validation...\n","\n","Training epoch: 12\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 65% 4823/7420 [5:47:42<2:41:32,  3.73s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 62.63741846686914\n","self-BLEU B->A: 59.777345564909446\n","self-BLEU avg: 61.207382015889294\n","self-ROUGE-1 A->B: 0.8126594148056554\n","self-ROUGE-1 B->A: 0.8015531232032104\n","self-ROUGE-2 A->B: 0.7119993064425841\n","self-ROUGE-2 B->A: 0.6943112141814685\n","self-ROUGE-L A->B: 0.8124216345255666\n","self-ROUGE-L B->A: 0.8005068831045452\n","style accuracy: 0.362116991643454\n","acc-BLEU: 48.70954059011735\n","g-acc-BLEU: 47.07890508706153\n","h-acc-BLEU: 45.50285744010503\n","End validation...\n","\n","Training epoch: 13\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 70% 5194/7420 [6:14:56<2:11:11,  3.54s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 59.03872222263671\n","self-BLEU B->A: 56.81792250882063\n","self-BLEU avg: 57.92832236572867\n","self-ROUGE-1 A->B: 0.7862470974867714\n","self-ROUGE-1 B->A: 0.7758850518628473\n","self-ROUGE-2 A->B: 0.6871923284477028\n","self-ROUGE-2 B->A: 0.6583602047564079\n","self-ROUGE-L A->B: 0.785837528785814\n","self-ROUGE-L B->A: 0.7750166705947465\n","style accuracy: 0.4011142061281337\n","acc-BLEU: 49.01987148927102\n","g-acc-BLEU: 48.20360260194653\n","h-acc-BLEU: 47.40092557391725\n","End validation...\n","\n","Training epoch: 14\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 75% 5565/7420 [6:42:07<1:49:28,  3.54s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 56.41091367787619\n","self-BLEU B->A: 53.14984750362311\n","self-BLEU avg: 54.780380590749644\n","self-ROUGE-1 A->B: 0.7622541357449466\n","self-ROUGE-1 B->A: 0.7555584025924504\n","self-ROUGE-2 A->B: 0.6536810392764669\n","self-ROUGE-2 B->A: 0.6209791256954798\n","self-ROUGE-L A->B: 0.7618433190424122\n","self-ROUGE-L B->A: 0.7544880342306626\n","style accuracy: 0.4331476323119777\n","acc-BLEU: 49.04757191097371\n","g-acc-BLEU: 48.71138691315638\n","h-acc-BLEU: 48.377505722838634\n","End validation...\n","\n","Training epoch: 15\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 80% 5936/7420 [7:09:05<1:28:01,  3.56s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 55.880371519173664\n","self-BLEU B->A: 50.15533551771833\n","self-BLEU avg: 53.017853518445996\n","self-ROUGE-1 A->B: 0.7581652870405035\n","self-ROUGE-1 B->A: 0.7306735765422567\n","self-ROUGE-2 A->B: 0.6475283031672041\n","self-ROUGE-2 B->A: 0.5921784159178296\n","self-ROUGE-L A->B: 0.7576013366572223\n","self-ROUGE-L B->A: 0.7289949113793337\n","style accuracy: 0.4373259052924791\n","acc-BLEU: 48.37522202384695\n","g-acc-BLEU: 48.15192705034601\n","h-acc-BLEU: 47.929662287731446\n","End validation...\n","\n","Training epoch: 16\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 85% 6307/7420 [7:36:05<1:00:53,  3.28s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 53.555905376292685\n","self-BLEU B->A: 48.19212385290032\n","self-BLEU avg: 50.874014614596504\n","self-ROUGE-1 A->B: 0.7472703788431329\n","self-ROUGE-1 B->A: 0.7124474997214196\n","self-ROUGE-2 A->B: 0.6264708320252753\n","self-ROUGE-2 B->A: 0.5728009493684983\n","self-ROUGE-L A->B: 0.7462638094512221\n","self-ROUGE-L B->A: 0.7105134724112167\n","style accuracy: 0.46935933147632314\n","acc-BLEU: 48.90497388111441\n","g-acc-BLEU: 48.86531846721528\n","h-acc-BLEU: 48.825694709380585\n","End validation...\n","\n","Training epoch: 17\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 90% 6678/7420 [8:04:51<48:08,  3.89s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 51.17300323617738\n","self-BLEU B->A: 47.677504140763155\n","self-BLEU avg: 49.425253688470264\n","self-ROUGE-1 A->B: 0.7267484004095075\n","self-ROUGE-1 B->A: 0.708836307208289\n","self-ROUGE-2 A->B: 0.6042221815572815\n","self-ROUGE-2 B->A: 0.565400461926365\n","self-ROUGE-L A->B: 0.7255109718902257\n","self-ROUGE-L B->A: 0.7065706126292629\n","style accuracy: 0.4735376044568245\n","acc-BLEU: 48.38950706707636\n","g-acc-BLEU: 48.37842104834452\n","h-acc-BLEU: 48.36733706964468\n","End validation...\n","\n","Training epoch: 18\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 95% 7049/7420 [8:34:15<27:21,  4.42s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 51.58186416433818\n","self-BLEU B->A: 45.876408923435996\n","self-BLEU avg: 48.72913654388709\n","self-ROUGE-1 A->B: 0.7320535659459553\n","self-ROUGE-1 B->A: 0.6941672897214858\n","self-ROUGE-2 A->B: 0.6064332136516207\n","self-ROUGE-2 B->A: 0.5521765409462085\n","self-ROUGE-L A->B: 0.7301028760582164\n","self-ROUGE-L B->A: 0.692153945932389\n","style accuracy: 0.49442896935933145\n","acc-BLEU: 49.086016739910114\n","g-acc-BLEU: 49.0847193729008\n","h-acc-BLEU: 49.08342154020795\n","End validation...\n","\n","Training epoch: 19\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n","100% 7420/7420 [9:03:16<00:00,  3.55s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 50.94021153275275\n","self-BLEU B->A: 46.191627084079414\n","self-BLEU avg: 48.565919308416085\n","self-ROUGE-1 A->B: 0.7280278671068591\n","self-ROUGE-1 B->A: 0.6947011172592343\n","self-ROUGE-2 A->B: 0.6034454011494094\n","self-ROUGE-2 B->A: 0.5532772282297223\n","self-ROUGE-L A->B: 0.7254384835815102\n","self-ROUGE-L B->A: 0.6933925271482124\n","style accuracy: 0.49025069637883006\n","acc-BLEU: 48.795494473149546\n","g-acc-BLEU: 48.794954412550744\n","h-acc-BLEU: 48.79441385794032\n","End validation...\n","End training...\n","100% 7420/7420 [9:05:08<00:00,  4.41s/it]\n"]}],"source":["!CUDA_VISIBLE_DEVICES=0 python \"/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/GAN_Binaria_shakespeare/train.py\"  \\\n","         --style_a=trump \\\n","         --style_b=shakespeare \\\n","         --lang=en \\\n","         --path_mono_A=\"/content/drive/MyDrive/ProjectNLP/20250112_Autori/Processed/train_trump_spellchecked.txt\" \\\n","         --path_mono_B=\"/content/drive/MyDrive/ProjectNLP/20250112_Autori/Processed/train_shakespeare_spellchecked.txt\" \\\n","         --path_mono_A_eval=\"/content/drive/MyDrive/ProjectNLP/20250112_Autori/Processed/eval_trump_spellchecked.txt\" \\\n","\t     --path_mono_B_eval=\"/content/drive/MyDrive/ProjectNLP/20250112_Autori/Processed/eval_shakespeare_spellchecked.txt\" \\\n","         --pretrained_classifier_model=\"/content/drive/MyDrive/ProjectNLP/20250112_Autori/UltimoClassifier/classifiers/Data/distilbert/distilbert-base-cased_10/checkpoints/checkpoint-314/\" \\\n","         --pretrained_classifier_eval=\"/content/drive/MyDrive/ProjectNLP/20250112_Autori/UltimoClassifier/classifiers/Data/distilbert/distilbert-base-cased_10/checkpoints/checkpoint-314/\" \\\n","         --shuffle \\\n","         --generator_model_tag=\"facebook/bart-base\" \\\n","         --discriminator_model_tag=\"distilbert/distilbert-base-cased\" \\\n","         --lambdas=\"10|1|1|1|1\" \\\n","         --epochs=20 \\\n","         --learning_rate=1e-4 \\\n","         --max_sequence_length=32 \\\n","         --batch_size=16 \\\n","         --save_base_folder=\"/content/drive/MyDrive/ProjectNLP/02.Training Parallelo Shakespeare/Ultimi_Checkpoint/\" \\\n","         --save_steps=1 \\\n","         --eval_strategy=epochs \\\n","         --eval_steps=1 \\\n","         --pin_memory \\\n","         --use_cuda_if_available\n"]},{"cell_type":"markdown","source":["### Enhanced tokenizer"],"metadata":{"id":"qGg_5NWUEjY7"}},{"cell_type":"code","source":["!CUDA_VISIBLE_DEVICES=0 python \"/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/GAN_Tokenizer_Shakespeare/train.py\"  \\\n","         --style_a=trump \\\n","         --style_b=shakespeare \\\n","         --lang=en \\\n","         --path_mono_A=\"/content/drive/MyDrive/ProjectNLP/03.Ultima Estensione Shakespeare/Data_spellchecked/train_trump_spellchecked.txt\" \\\n","         --path_mono_B=\"/content/drive/MyDrive/ProjectNLP/03.Ultima Estensione Shakespeare/Data_spellchecked/train_shakespeare_spellchecked.txt\" \\\n","         --path_mono_A_eval=\"/content/drive/MyDrive/ProjectNLP/03.Ultima Estensione Shakespeare/Data_spellchecked/eval_trump_spellchecked.txt\" \\\n","\t     --path_mono_B_eval=\"/content/drive/MyDrive/ProjectNLP/03.Ultima Estensione Shakespeare/Data_spellchecked/eval_shakespeare_spellchecked.txt\" \\\n","         --pretrained_classifier_model=\"/content/drive/MyDrive/ProjectNLP/03.Ultima Estensione Shakespeare/UltimoClassifier/classifiers/Data/distilbert/distilbert-base-cased_10/checkpoints/checkpoint-314/\" \\\n","         --pretrained_classifier_eval=\"/content/drive/MyDrive/ProjectNLP/03.Ultima Estensione Shakespeare/UltimoClassifier/classifiers/Data/distilbert/distilbert-base-cased_10/checkpoints/checkpoint-314/\" \\\n","         --shuffle \\\n","         --generator_model_tag=\"facebook/bart-base\" \\\n","         --discriminator_model_tag=\"distilbert/distilbert-base-cased\" \\\n","         --lambdas=\"10|1|1|1|1\" \\\n","         --epochs=20 \\\n","         --learning_rate=1e-4 \\\n","         --max_sequence_length=32 \\\n","         --batch_size=16 \\\n","         --save_base_folder=\"/content/drive/MyDrive/ProjectNLP/03.Ultima Estensione Shakespeare/Checkpoint_GAN_Enhanced_Tokenizer/\" \\\n","         --save_steps=1 \\\n","         --eval_strategy=epochs \\\n","         --eval_steps=1 \\\n","         --pin_memory \\\n","         --use_cuda_if_available\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hf5RTs8wEiV7","executionInfo":{"status":"ok","timestamp":1738388447451,"user_tz":-60,"elapsed":35435452,"user":{"displayName":"Michele Sgrillo","userId":"00105951137606608722"}},"outputId":"f8ee5802-3462-44f3-8a11-e8ebd345c380","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-01-31 19:50:31.009490: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1738353031.264266    3693 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1738353031.329594    3693 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-01-31 19:50:31.868715: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","Arguments summary: \n"," \n","\tstyle_a:\t\ttrump\n","\tstyle_b:\t\tshakespeare\n","\tlang:\t\ten\n","\tmax_samples_train:\t\tNone\n","\tmax_samples_eval:\t\tNone\n","\tnonparal_same_size:\t\tFalse\n","\tpath_mono_A:\t\t/content/drive/MyDrive/ProjectNLP/03.Ultima Estensione Shakespeare/Data_spellchecked/train_trump_spellchecked.txt\n","\tpath_mono_B:\t\t/content/drive/MyDrive/ProjectNLP/03.Ultima Estensione Shakespeare/Data_spellchecked/train_shakespeare_spellchecked.txt\n","\tpath_mono_A_eval:\t\t/content/drive/MyDrive/ProjectNLP/03.Ultima Estensione Shakespeare/Data_spellchecked/eval_trump_spellchecked.txt\n","\tpath_mono_B_eval:\t\t/content/drive/MyDrive/ProjectNLP/03.Ultima Estensione Shakespeare/Data_spellchecked/eval_shakespeare_spellchecked.txt\n","\tpath_paral_A_eval:\t\tNone\n","\tpath_paral_B_eval:\t\tNone\n","\tpath_paral_eval_ref:\t\tNone\n","\tn_references:\t\tNone\n","\tlowercase_ref:\t\tFalse\n","\tbertscore:\t\tTrue\n","\tmax_sequence_length:\t\t32\n","\tbatch_size:\t\t16\n","\tshuffle:\t\tTrue\n","\tnum_workers:\t\t4\n","\tpin_memory:\t\tTrue\n","\tuse_cuda_if_available:\t\tTrue\n","\tlearning_rate:\t\t0.0001\n","\tepochs:\t\t20\n","\tlr_scheduler_type:\t\tlinear\n","\twarmup:\t\tFalse\n","\tlambdas:\t\t10|1|1|1|1\n","\tgenerator_model_tag:\t\tfacebook/bart-base\n","\tdiscriminator_model_tag:\t\tdistilbert/distilbert-base-cased\n","\tpretrained_classifier_model:\t\t/content/drive/MyDrive/ProjectNLP/03.Ultima Estensione Shakespeare/UltimoClassifier/classifiers/Data/distilbert/distilbert-base-cased_10/checkpoints/checkpoint-314/\n","\tpretrained_classifier_eval:\t\t/content/drive/MyDrive/ProjectNLP/03.Ultima Estensione Shakespeare/UltimoClassifier/classifiers/Data/distilbert/distilbert-base-cased_10/checkpoints/checkpoint-314/\n","\tsave_base_folder:\t\t/content/drive/MyDrive/ProjectNLP/03.Ultima Estensione Shakespeare/Checkpoint_GAN_Enhanced_Tokenizer\n","\tfrom_pretrained:\t\tNone\n","\tsave_steps:\t\t1\n","\teval_strategy:\t\tepochs\n","\teval_steps:\t\t1\n","\tadditional_eval:\t\t0\n","\tcontrol_file:\t\tNone\n","\tlambda_file:\t\tNone\n","\tcomet_logging:\t\tFalse\n","\tcomet_key:\t\tNone\n","\tcomet_workspace:\t\tNone\n","\tcomet_project_name:\t\tNone\n","\tcomet_exp:\t\tNone\n","Mono A  : 6964\n","Mono B  : 5926\n","Mono A eval: 388\n","Mono B eval: 330\n","\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","Mono A eval (batches): 25\n","Mono B eval (batches): 21\n","config.json: 100% 1.72k/1.72k [00:00<00:00, 10.9MB/s]\n","vocab.json: 100% 899k/899k [00:00<00:00, 4.64MB/s]\n","merges.txt: 100% 456k/456k [00:00<00:00, 6.71MB/s]\n","tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 10.5MB/s]\n","tokenizer_config.json: 100% 49.0/49.0 [00:00<00:00, 366kB/s]\n","config.json: 100% 465/465 [00:00<00:00, 3.69MB/s]\n","vocab.txt: 100% 213k/213k [00:00<00:00, 54.1MB/s]\n","tokenizer.json: 100% 436k/436k [00:00<00:00, 51.9MB/s]\n","Trump-specific tokens: 3819, Shakespeare-specific tokens: 3467\n","3467 new tokens added to G_AB & D_AB (Trump → Shakespeare)\n","3819 new tokens added to G_BA & D_BA (Shakespeare → Trump)\n","model.safetensors: 100% 558M/558M [00:02<00:00, 225MB/s]\n","Generator pretrained models not loaded - Initial weights will be used\n","config.json: 100% 465/465 [00:00<00:00, 2.91MB/s]\n","model.safetensors: 100% 263M/263M [00:02<00:00, 130MB/s]\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Discriminator pretrained models not loaded - Initial weights will be used\n","The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","Classifier pretrained model loaded correctly\n","Device: cuda\n","Total number of training steps: 7420\n","  0% 0/7420 [00:00<?, ?it/s]\n","Downloading builder script: 100% 8.15k/8.15k [00:00<00:00, 24.8MB/s]\n","\n","Downloading builder script: 100% 6.27k/6.27k [00:00<00:00, 22.9MB/s]\n","\n","Downloading builder script: 100% 7.95k/7.95k [00:00<00:00, 26.6MB/s]\n","Start training...\n","\n","Training epoch: 0\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n","  0% 5/7420 [00:28<10:18:25,  5.00s/it]Dummy classification metrics computation end\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n","  5% 371/7420 [25:20<6:45:01,  3.45s/it]Start validation...\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 72.99525429002018\n","self-BLEU B->A: 71.55920659884474\n","self-BLEU avg: 72.27723044443246\n","self-ROUGE-1 A->B: 0.8786097489701494\n","self-ROUGE-1 B->A: 0.879927635247071\n","self-ROUGE-2 A->B: 0.8040751640994718\n","self-ROUGE-2 B->A: 0.7828752268311171\n","self-ROUGE-L A->B: 0.8786097489701494\n","self-ROUGE-L B->A: 0.879927635247071\n","style accuracy: 0.21727019498607242\n","acc-BLEU: 47.00212497151985\n","g-acc-BLEU: 39.627879014293875\n","h-acc-BLEU: 33.41059110450339\n","End validation...\n","/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","\n","Training epoch: 1\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 10% 742/7420 [53:11<6:11:08,  3.33s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 72.7314120618266\n","self-BLEU B->A: 65.66766840798627\n","self-BLEU avg: 69.19954023490644\n","self-ROUGE-1 A->B: 0.877181726718421\n","self-ROUGE-1 B->A: 0.8485463087243817\n","self-ROUGE-2 A->B: 0.800513479087584\n","self-ROUGE-2 B->A: 0.7452734824055824\n","self-ROUGE-L A->B: 0.8770696693450458\n","self-ROUGE-L B->A: 0.8482937834718566\n","style accuracy: 0.2590529247910863\n","acc-BLEU: 47.55241635700753\n","g-acc-BLEU: 42.339512623613146\n","h-acc-BLEU: 37.69806978677102\n","End validation...\n","\n","Training epoch: 2\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 15% 1113/7420 [1:21:06<6:01:48,  3.44s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 72.75634707990292\n","self-BLEU B->A: 63.38491957302057\n","self-BLEU avg: 68.07063332646175\n","self-ROUGE-1 A->B: 0.8763657987037425\n","self-ROUGE-1 B->A: 0.835317283808184\n","self-ROUGE-2 A->B: 0.8020783219739467\n","self-ROUGE-2 B->A: 0.7227115725598807\n","self-ROUGE-L A->B: 0.8763657987037425\n","self-ROUGE-L B->A: 0.835317283808184\n","style accuracy: 0.28690807799442897\n","acc-BLEU: 48.38072056295232\n","g-acc-BLEU: 44.19277607885554\n","h-acc-BLEU: 40.36734911442613\n","End validation...\n","\n","Training epoch: 3\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 20% 1484/7420 [1:49:07<6:03:51,  3.68s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 70.09462571871622\n","self-BLEU B->A: 61.32631664182584\n","self-BLEU avg: 65.71047118027103\n","self-ROUGE-1 A->B: 0.860905065565968\n","self-ROUGE-1 B->A: 0.8247104627681071\n","self-ROUGE-2 A->B: 0.774595885994897\n","self-ROUGE-2 B->A: 0.7058196844634486\n","self-ROUGE-L A->B: 0.8607930081925927\n","self-ROUGE-L B->A: 0.82459611171036\n","style accuracy: 0.3384401114206128\n","acc-BLEU: 49.77724116116616\n","g-acc-BLEU: 47.15830699648991\n","h-acc-BLEU: 44.67716258593283\n","End validation...\n","\n","Training epoch: 4\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 25% 1855/7420 [2:17:17<5:52:03,  3.80s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 63.57320789528953\n","self-BLEU B->A: 59.81511885571066\n","self-BLEU avg: 61.694163375500096\n","self-ROUGE-1 A->B: 0.817826658114103\n","self-ROUGE-1 B->A: 0.8102718187900502\n","self-ROUGE-2 A->B: 0.7120706240333723\n","self-ROUGE-2 B->A: 0.686795515637818\n","self-ROUGE-L A->B: 0.8174895229594149\n","self-ROUGE-L B->A: 0.8099687884870198\n","style accuracy: 0.3607242339832869\n","acc-BLEU: 48.88329338691439\n","g-acc-BLEU: 47.174760015146894\n","h-acc-BLEU: 45.5259415954054\n","End validation...\n","\n","Training epoch: 5\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 30% 2226/7420 [2:45:30<5:08:09,  3.56s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 58.935410164251074\n","self-BLEU B->A: 55.59525362824202\n","self-BLEU avg: 57.26533189624655\n","self-ROUGE-1 A->B: 0.7940969352284327\n","self-ROUGE-1 B->A: 0.7890491953380664\n","self-ROUGE-2 A->B: 0.6746518110083938\n","self-ROUGE-2 B->A: 0.6517881381017354\n","self-ROUGE-L A->B: 0.7935370169329592\n","self-ROUGE-L B->A: 0.7890491953380664\n","style accuracy: 0.40947075208913647\n","acc-BLEU: 49.1062035525801\n","g-acc-BLEU: 48.42362906700621\n","h-acc-BLEU: 47.75054185634631\n","End validation...\n","\n","Training epoch: 6\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 35% 2597/7420 [3:13:27<4:49:02,  3.60s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 54.98610817587058\n","self-BLEU B->A: 52.73428788413191\n","self-BLEU avg: 53.86019803000124\n","self-ROUGE-1 A->B: 0.7683703481111199\n","self-ROUGE-1 B->A: 0.766563705006681\n","self-ROUGE-2 A->B: 0.6343553236946542\n","self-ROUGE-2 B->A: 0.6165213913481844\n","self-ROUGE-L A->B: 0.7681172185087636\n","self-ROUGE-L B->A: 0.7661289223979854\n","style accuracy: 0.4623955431754875\n","acc-BLEU: 50.04987617377499\n","g-acc-BLEU: 49.90462455887405\n","h-acc-BLEU: 49.75979398700748\n","End validation...\n","\n","Training epoch: 7\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 40% 2968/7420 [3:41:56<4:17:32,  3.47s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 53.342583003216745\n","self-BLEU B->A: 49.57291949657474\n","self-BLEU avg: 51.45775124989574\n","self-ROUGE-1 A->B: 0.7531207710347285\n","self-ROUGE-1 B->A: 0.7526992004983999\n","self-ROUGE-2 A->B: 0.6147792298009798\n","self-ROUGE-2 B->A: 0.5959389390980279\n","self-ROUGE-L A->B: 0.7529950481280148\n","self-ROUGE-L B->A: 0.7519958252828296\n","style accuracy: 0.4818941504178273\n","acc-BLEU: 49.82358314583924\n","g-acc-BLEU: 49.79677632234882\n","h-acc-BLEU: 49.76998342240124\n","End validation...\n","\n","Training epoch: 8\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 45% 3339/7420 [4:10:12<4:15:50,  3.76s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 48.78676524280537\n","self-BLEU B->A: 47.34881036032755\n","self-BLEU avg: 48.06778780156646\n","self-ROUGE-1 A->B: 0.7281037130659378\n","self-ROUGE-1 B->A: 0.7291591906914908\n","self-ROUGE-2 A->B: 0.5779881067167361\n","self-ROUGE-2 B->A: 0.5675806026624479\n","self-ROUGE-L A->B: 0.7276857226187702\n","self-ROUGE-L B->A: 0.7272766220519916\n","style accuracy: 0.5557103064066853\n","acc-BLEU: 51.819409221117496\n","g-acc-BLEU: 51.68342586119851\n","h-acc-BLEU: 51.54779884845817\n","End validation...\n","\n","Training epoch: 9\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 50% 3710/7420 [4:41:09<3:36:30,  3.50s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 46.509989809346905\n","self-BLEU B->A: 44.65827675374379\n","self-BLEU avg: 45.58413328154535\n","self-ROUGE-1 A->B: 0.7111610758648912\n","self-ROUGE-1 B->A: 0.7072645650935432\n","self-ROUGE-2 A->B: 0.554538040169301\n","self-ROUGE-2 B->A: 0.5417307110451045\n","self-ROUGE-L A->B: 0.7106776594260683\n","self-ROUGE-L B->A: 0.7056106545577501\n","style accuracy: 0.5501392757660167\n","acc-BLEU: 50.29903042907351\n","g-acc-BLEU: 50.077561911429896\n","h-acc-BLEU: 49.85706803236987\n","End validation...\n","\n","Training epoch: 10\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 55% 4081/7420 [5:13:13<4:43:49,  5.10s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 47.38347539253092\n","self-BLEU B->A: 44.21231738162667\n","self-BLEU avg: 45.797896387078794\n","self-ROUGE-1 A->B: 0.7156498238849786\n","self-ROUGE-1 B->A: 0.6985659100767323\n","self-ROUGE-2 A->B: 0.5653392141050205\n","self-ROUGE-2 B->A: 0.5358619946047594\n","self-ROUGE-L A->B: 0.7151951110720183\n","self-ROUGE-L B->A: 0.697318179017256\n","style accuracy: 0.5473537604456824\n","acc-BLEU: 50.26663621582352\n","g-acc-BLEU: 50.06760510346916\n","h-acc-BLEU: 49.869361560206045\n","End validation...\n","\n","Training epoch: 11\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 60% 4452/7420 [5:44:41<2:52:04,  3.48s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 44.26513495976953\n","self-BLEU B->A: 43.63512558326187\n","self-BLEU avg: 43.950130271515704\n","self-ROUGE-1 A->B: 0.6920797331545407\n","self-ROUGE-1 B->A: 0.6883033304052983\n","self-ROUGE-2 A->B: 0.5381343663176215\n","self-ROUGE-2 B->A: 0.5242085741010972\n","self-ROUGE-L A->B: 0.6910243237735615\n","self-ROUGE-L B->A: 0.6874265271296885\n","style accuracy: 0.5933147632311978\n","acc-BLEU: 51.640803297317746\n","g-acc-BLEU: 51.06492057765746\n","h-acc-BLEU: 50.49545944011634\n","End validation...\n","\n","Training epoch: 12\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 65% 4823/7420 [6:16:30<2:51:59,  3.97s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 40.25206207975363\n","self-BLEU B->A: 38.55318902215155\n","self-BLEU avg: 39.40262555095259\n","self-ROUGE-1 A->B: 0.6574555821251867\n","self-ROUGE-1 B->A: 0.6480852327091495\n","self-ROUGE-2 A->B: 0.48984029709747634\n","self-ROUGE-2 B->A: 0.46944026834394414\n","self-ROUGE-L A->B: 0.655800705042918\n","self-ROUGE-L B->A: 0.6459688777085065\n","style accuracy: 0.6545961002785515\n","acc-BLEU: 52.43111778940387\n","g-acc-BLEU: 50.786617357714995\n","h-acc-BLEU: 49.193696163452245\n","End validation...\n","\n","Training epoch: 13\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 70% 5194/7420 [6:45:32<2:22:49,  3.85s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 41.20605896685216\n","self-BLEU B->A: 39.19200343828441\n","self-BLEU avg: 40.199031202568285\n","self-ROUGE-1 A->B: 0.6701434110257988\n","self-ROUGE-1 B->A: 0.6548440509789475\n","self-ROUGE-2 A->B: 0.5077290739158571\n","self-ROUGE-2 B->A: 0.48234339801937604\n","self-ROUGE-L A->B: 0.6678081617945028\n","self-ROUGE-L B->A: 0.6531392713946177\n","style accuracy: 0.6253481894150418\n","acc-BLEU: 51.36692507203623\n","g-acc-BLEU: 50.138200385299875\n","h-acc-BLEU: 48.93886698262899\n","End validation...\n","\n","Training epoch: 14\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 75% 5565/7420 [7:13:54<1:43:37,  3.35s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 40.05793156394791\n","self-BLEU B->A: 35.05144273431184\n","self-BLEU avg: 37.554687149129876\n","self-ROUGE-1 A->B: 0.6510117632490369\n","self-ROUGE-1 B->A: 0.6008667793866476\n","self-ROUGE-2 A->B: 0.4912113555666098\n","self-ROUGE-2 B->A: 0.42623586043914485\n","self-ROUGE-L A->B: 0.6489425790059595\n","self-ROUGE-L B->A: 0.5985811184690916\n","style accuracy: 0.6518105849582173\n","acc-BLEU: 51.3678728224758\n","g-acc-BLEU: 49.47579468649007\n","h-acc-BLEU: 47.65340866834506\n","End validation...\n","\n","Training epoch: 15\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 80% 5936/7420 [7:43:17<1:37:07,  3.93s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 39.32255923984954\n","self-BLEU B->A: 35.85092420992839\n","self-BLEU avg: 37.58674172488897\n","self-ROUGE-1 A->B: 0.644904772718188\n","self-ROUGE-1 B->A: 0.6155823030917573\n","self-ROUGE-2 A->B: 0.4814520568269267\n","self-ROUGE-2 B->A: 0.436327125708471\n","self-ROUGE-L A->B: 0.6434893529525846\n","self-ROUGE-L B->A: 0.6132255475740425\n","style accuracy: 0.6657381615598886\n","acc-BLEU: 52.080278940438916\n","g-acc-BLEU: 50.02292308027785\n","h-acc-BLEU: 48.04683961723201\n","End validation...\n","\n","Training epoch: 16\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 85% 6307/7420 [8:14:38<2:03:21,  6.65s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 35.59559070752478\n","self-BLEU B->A: 32.85270647124869\n","self-BLEU avg: 34.22414858938673\n","self-ROUGE-1 A->B: 0.6094065372694702\n","self-ROUGE-1 B->A: 0.5794647252087778\n","self-ROUGE-2 A->B: 0.44391463862254266\n","self-ROUGE-2 B->A: 0.40181533354137916\n","self-ROUGE-L A->B: 0.6069052557950885\n","self-ROUGE-L B->A: 0.577243810211113\n","style accuracy: 0.7186629526462396\n","acc-BLEU: 53.04522192700534\n","g-acc-BLEU: 49.5939791477275\n","h-acc-BLEU: 46.36728163577397\n","End validation...\n","\n","Training epoch: 17\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 90% 6678/7420 [8:45:48<44:57,  3.64s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 34.588788468678224\n","self-BLEU B->A: 33.10048955788725\n","self-BLEU avg: 33.84463901328274\n","self-ROUGE-1 A->B: 0.5950931651976523\n","self-ROUGE-1 B->A: 0.5783358996280628\n","self-ROUGE-2 A->B: 0.43088733418894964\n","self-ROUGE-2 B->A: 0.4037572989453894\n","self-ROUGE-L A->B: 0.5928499385359346\n","self-ROUGE-L B->A: 0.5761554180502683\n","style accuracy: 0.7158774373259053\n","acc-BLEU: 52.716191372936635\n","g-acc-BLEU: 49.22256946162929\n","h-acc-BLEU: 45.96047738510424\n","End validation...\n","\n","Training epoch: 18\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"," 95% 7049/7420 [9:19:30<23:14,  3.76s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 34.0862123534175\n","self-BLEU B->A: 33.05493769125663\n","self-BLEU avg: 33.57057502233707\n","self-ROUGE-1 A->B: 0.5885606509276737\n","self-ROUGE-1 B->A: 0.5824194973918513\n","self-ROUGE-2 A->B: 0.4203485723568495\n","self-ROUGE-2 B->A: 0.400768831199636\n","self-ROUGE-L A->B: 0.5851738415881135\n","self-ROUGE-L B->A: 0.57972659809595\n","style accuracy: 0.7256267409470752\n","acc-BLEU: 53.06662455852229\n","g-acc-BLEU: 49.35555383660255\n","h-acc-BLEU: 45.904006366173945\n","End validation...\n","\n","Training epoch: 19\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n","100% 7420/7420 [9:47:55<00:00,  3.81s/it]Start validation...\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","self-BLEU A->B: 34.39170323815415\n","self-BLEU B->A: 31.890762811982462\n","self-BLEU avg: 33.1412330250683\n","self-ROUGE-1 A->B: 0.5899142981537195\n","self-ROUGE-1 B->A: 0.5680386247442021\n","self-ROUGE-2 A->B: 0.4265822037398452\n","self-ROUGE-2 B->A: 0.3882069835371806\n","self-ROUGE-L A->B: 0.5877335490348979\n","self-ROUGE-L B->A: 0.5656272389886946\n","style accuracy: 0.7214484679665738\n","acc-BLEU: 52.64303991086284\n","g-acc-BLEU: 48.897537558100765\n","h-acc-BLEU: 45.41852371339291\n","End validation...\n","End training...\n","100% 7420/7420 [9:49:36<00:00,  4.77s/it]\n"]}]},{"cell_type":"markdown","source":["# Ternary Experiment"],"metadata":{"id":"x0ZtQwi3-LAS"}},{"cell_type":"code","source":["# @title Preprocessing on Lyrics Train Eval and Test\n","\n","# Initialize spell checker\n","spell = SpellChecker()\n","\n","# Function to clean and correct text line by line\n","def correct_text_line(line):\n","    # Fix common OCR and formatting errors\n","    line = re.sub(r'(\\s[oO]Jce\\s|\\soJce\\s)', ' once ', line, flags=re.IGNORECASE)  # Fix OCR \"oJce\" -> \"once\"\n","    line = re.sub(r'([a-zA-Z])(\\d+)', r'\\1', line)  # Remove digits mixed with letters\n","    line = re.sub(r'\\s+', ' ', line).strip()  # Normalize spaces\n","\n","    # Tokenize while keeping punctuation\n","    tokens = re.findall(r\"[\\w']+|[.,!?\\'-]\", line)\n","\n","    corrected_tokens = []\n","    for i, token in enumerate(tokens):\n","        if token.isalpha():  # Only process alphabetic words\n","            corrected_word = spell.correction(token)\n","            corrected_word = corrected_word if corrected_word is not None else token\n","\n","            # Handle capitalization:\n","            if i == 0 or (i > 0 and tokens[i - 1] in \".!?\"):  # Capitalize first word of a sentence\n","                corrected_word = corrected_word.capitalize()\n","            elif token.lower() == \"i\":  # Ensure 'i' is always capitalized\n","                corrected_word = \"I\"\n","\n","            corrected_tokens.append(corrected_word)\n","        else:\n","            corrected_tokens.append(token)  # Keep punctuation as-is\n","\n","    # Join words carefully, ensuring spacing is correct\n","    return ''.join(\n","        corrected_tokens[i] if corrected_tokens[i] in \".,!?'-\" else ' ' + corrected_tokens[i]\n","        for i in range(len(corrected_tokens))\n","    ).strip()\n","\n","# Define base paths\n","base_input_path = \"/content/drive/MyDrive/ProjectNLP/20250112_Autori/Processed/\"\n","base_output_path = \"/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/Data_spellchecked/\"\n","\n","# Define file categories\n","categories = [\"train\", \"eval\", \"test\"]\n","authors = [\"lyrics\"]\n","\n","# Process each file type (train, eval, test) for both authors\n","for category in categories:\n","    for author in authors:\n","        input_file_path = f\"{base_input_path}{category}_{author}.txt\"\n","        output_file_path = f\"{base_output_path}{category}_{author}_spellchecked.txt\"\n","\n","        with open(input_file_path, \"r\", encoding=\"utf-8\") as infile, open(output_file_path, \"w\", encoding=\"utf-8\") as outfile:\n","            for line in infile:\n","                corrected_line = correct_text_line(line)  # Apply spell-checking\n","                outfile.write(corrected_line + \"\\n\")\n","\n","        print(f\"Processing complete. Corrected file saved at: {output_file_path}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_L7HKFqQ_veE","executionInfo":{"status":"ok","timestamp":1738422206123,"user_tz":-60,"elapsed":175828,"user":{"displayName":"Francesco La Rota","userId":"12370830885715187383"}},"outputId":"ea4e609e-c1a1-4315-a4f5-010ba9c614ba","collapsed":true,"cellView":"form"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing complete. Corrected file saved at: /content/drive/MyDrive/ProjectNLP/03.Ultima Estensione Shakespeare/Data_spellchecked/train_lyrics_spellchecked.txt\n","Processing complete. Corrected file saved at: /content/drive/MyDrive/ProjectNLP/03.Ultima Estensione Shakespeare/Data_spellchecked/eval_lyrics_spellchecked.txt\n","Processing complete. Corrected file saved at: /content/drive/MyDrive/ProjectNLP/03.Ultima Estensione Shakespeare/Data_spellchecked/test_lyrics_spellchecked.txt\n"]}]},{"cell_type":"code","source":["# @title Create aggregated test, train and eval files\n","splits = [\"train\", \"eval\", \"test\"]\n","classes = [\"trump\", \"lyrics\", \"shakespeare\"]\n","\n","# Define source and destination folders\n","source_folder = \"/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/Data_spellchecked\"\n","dest_folder = \"/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/Data_Ternary_Classifier\"\n","\n","# Create the destination folder if it doesn't exist\n","os.makedirs(dest_folder, exist_ok=True)\n","\n","# Process each split\n","for split in splits:\n","    all_rows = []  # To store all the rows for the current split\n","    for cls in classes:\n","        # Construct the file name based on split and class\n","        filename = f\"{split}_{cls}_spellchecked.txt\"\n","        file_path = os.path.join(source_folder, filename)\n","\n","        # Check if the file exists\n","        if not os.path.exists(file_path):\n","            print(f\"Warning: {file_path} not found. Skipping.\")\n","            continue\n","\n","        # Read the file (assuming one example per line)\n","        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","            lines = f.read().splitlines()\n","            # For each non-empty line, create a row with text and mapped label\n","            for line in lines:\n","                if line.strip():  # Ignore empty lines\n","                    all_rows.append({\n","                        \"text\": line.strip(),\n","                        \"label\": cls\n","                    })\n","\n","    # Create a DataFrame for the current split and save to CSV\n","    df = pd.DataFrame(all_rows)\n","    csv_path = os.path.join(dest_folder, f\"{split}.csv\")\n","    df=df.sample(frac=1)\n","    df.to_csv(csv_path, index=False)\n","    print(f\"Saved {split}.csv with {len(df)} records at {csv_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UyKLKRkBB4s8","executionInfo":{"status":"ok","timestamp":1738424395240,"user_tz":-60,"elapsed":525,"user":{"displayName":"Francesco La Rota","userId":"12370830885715187383"}},"outputId":"6dacb92d-c47b-424e-858e-87ebb308ae6a","cellView":"form"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved train.csv with 18616 records at /content/drive/MyDrive/ProjectNLP/03.Ultima Estensione Shakespeare/Data_Ternary_Classifier/train.csv\n","Saved eval.csv with 1035 records at /content/drive/MyDrive/ProjectNLP/03.Ultima Estensione Shakespeare/Data_Ternary_Classifier/eval.csv\n","Saved test.csv with 1034 records at /content/drive/MyDrive/ProjectNLP/03.Ultima Estensione Shakespeare/Data_Ternary_Classifier/test.csv\n"]}]},{"cell_type":"markdown","source":["## Train the Classifier"],"metadata":{"id":"zr_yMIlT-N_W"}},{"cell_type":"code","source":["from datasets import Dataset  # Import Hugging Face Dataset\n","# File paths\n","file_paths = {\n","    \"train\": \"/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/TernaryClassifier/Data_Ternary_Classifier/train.csv\",\n","    \"eval\":  \"/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/TernaryClassifier/Data_Ternary_Classifier/eval.csv\",\n","    \"test\":  \"/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/TernaryClassifier/Data_Ternary_Classifier/test.csv\"\n","}\n","\n","# Define label mapping\n","label_mapping = {\"trump\": 0, \"lyrics\": 1, \"shakespeare\": 2}\n","\n","# Dictionary to hold both pandas DataFrames and PyTorch Datasets\n","dfs = {}\n","datasets = {}\n","\n","# Loop over each split, load the CSV, map the labels, and convert to a Hugging Face Dataset\n","for split, path in file_paths.items():\n","    df = pd.read_csv(path)\n","    df[\"label\"] = df[\"label\"].map(label_mapping)\n","    dfs[split] = df\n","\n","    # Convert to Hugging Face Dataset\n","    hf_dataset = Dataset.from_pandas(df)\n","\n","    # ✅ Convert Hugging Face Dataset to PyTorch format\n","    datasets[split] = hf_dataset.with_format(\"torch\")\n","\n","# Optionally, extract datasets for convenience\n","train_dataset = datasets[\"train\"]\n","eval_dataset = datasets[\"eval\"]\n","test_dataset = datasets[\"test\"]\n","\n","print(\"Esempi di TRAIN:\")\n","print(dfs[\"train\"].head())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_UPyyk6U-Npn","executionInfo":{"status":"ok","timestamp":1738435402295,"user_tz":-60,"elapsed":258,"user":{"displayName":"Francesco La Rota","userId":"12370830885715187383"}},"outputId":"42eaaed5-ab83-4aef-aa06-2d2b4e5f5f86"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Esempi di TRAIN:\n","                                                text  label\n","0                          There's bones in the sink      1\n","1  He's trying to buy his way back into the Democ...      0\n","2  Under Republican leadership, the economy is bo...      0\n","3                       I owe my life to my enemies.      2\n","4          'To your dwarfs here, yes, he is a dwarf.      0\n"]}]},{"cell_type":"code","source":["# @title Initialize Classifier\n","model_name = \"distilbert-base-cased\"\n","tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n","\n","# Funzione di tokenizzazione\n","def tokenize_function(examples):\n","    truncation = 'longest_first'\n","    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=truncation, max_length=32)\n","\n","# Tokenizzazione dei dataset\n","train_dataset = train_dataset.map(tokenize_function, batched=True)\n","eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n","test_dataset = test_dataset.map(tokenize_function, batched=True)\n","\n","# Rimuovere colonne non necessarie\n","train_dataset = train_dataset.remove_columns([\"text\"])\n","eval_dataset = eval_dataset.remove_columns([\"text\"])\n","test_dataset = test_dataset.remove_columns([\"text\"])\n","\n","# Impostare il tipo per PyTorch\n","train_dataset.set_format(\"torch\")\n","eval_dataset.set_format(\"torch\")\n","test_dataset.set_format(\"torch\")\n","\n","# Inizializzazione del modello DeBERTa\n","model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=3)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":432,"referenced_widgets":["7807d5ca35414c179050705be150bc13","1939a3d68b4e469fa1828103d0396a37","d2a360e7b9b54b7b94bad42231b80ace","96a74670613f42a8a7c92790ea161e07","f4bdb547ac264b9fb503793dd442b61f","182bf0c8bf5d4265ba2fe4617c7f432f","5e9a58848bc249c9a979ec2cdd641ae0","3a8c9502e60b4bd8abffbbb09f386805","b9b3d399d6c448e9a5bb745cee2399b5","08d20a4f3a8044feb9e8a5a8b244067a","d73ed46493f84e88a70d39e87866edee","bc94f3848a9f4dbfa2e4121656740fd1","49731a870c4347b58832a54133b13221","198244bcdb084c3eaf6538ade38b13de","1fde796d99e048399223702fd7f929ef","d817a0093d694d5690d69c850fff06c7","f956fcccb56d4b57996380baffef772a","9d382953953d4facbafe133e95280489","0007a12b45214175af403d0c44266aff","7877323eca52434d9c525fa289602354","806613d7d45741dab0b0fd17dee1efd9","fa80c7cab7f64d96a6a7f4d840b188e5","57ac976b4a29459fb18f3a039b5951d9","5a0d1444138245ba9feddd5b2c9e0c99","dbcc08dc3a8049cdb77164c3da07ff66","12a0776e8b63496d8246cdc7f777b158","37671c3615404898975f6ebcb5e53404","2c113b50a80c423fb95a7713b79e319a","25211a2611284fa79313b74b90c0dd13","02aee807b72e42a193817ed9d6541370","f3629ed467bc4e079271df4ba7db6bbe","b05a3df27b9a4812aa8992595802514d","ac93d1d126524094965cd292abfd30c3","8acb83b3962e40adabed570b5e6e0cc0","e1d3fef17b1d46cd8d09b4d6df33f043","fb52226142e9444ea385847bf606b579","66f73d4e5c1c451ab21d75bc79f672dc","12b62f5d1cc74840bfeb496c0c07e03a","9bccc5c247bd4872a016f2c09bf23f33","42733d3246a344e0819882112fdb4b9b","87fd03cf11734008b7c1e483deb746b4","171094d764d2482fafb8636e1cdc3ade","dc74fa80ac5d49f789db06ee5404eb37","5920c341f8084724970b688af9618b7f","99383b201b5c4d26a7e8c61a0d46fb62","ea0180ee2d07452db614214a0776a12d","58de319fe6f54c6abcbbdbf5cc1f2e5c","30eab1226c0c456b877bbc246abbcef2","4d5c4a5720a44bf2bd4c2dce54c05d58","8873ec2206c041899ea75b411056098d","8637c09332e748d281abc9f3d89c7fd1","264ec78c0e6b41f2bb6ddb23464593eb","688a5f8741f144d3b3fb9dae487eba9e","4e8be0d94fef4592b8b9d2e4f2394d96","cad38b578cf44203b9383c86ab7c9861","3d4341b2d4ca4ab59679f77d9bae2141","70ca81fb1d4a430fb20f5ef13129b202","6ad8d35a9dda420483788f80b338fc4c","7e1afaf82e144cc3abcb3859b7e98d5f","95d8acc9a8ae477abc30654500dd0666","3ba258f4defd464bb058fbfd88360681","8a2886b963a54d0cae192abef4d480d9","ecf514f3265843a085e940db60f186ab","0018376b1b364f3ea8db7ed2139589ff","259368276ddf4f7eb8135595ee243898","eb3e6970fb904e1dac949950415d1282","5596132eb19a497fb7f085bf13db8d41","aa9cc9ef5b8944d18b0b2b007bcf60f5","79a182e2c94f48c69e1c6fb0712b8ad7","739635977b794d18ada58bea892aefd5","800c6a20859f46d58c69d38eefc53da9","afc21072452e49aa9c7b15c9568a55bf","dece2963be034c778907a25d3cc9b34d","b3f419c5b61f434d9963b278373223cb","25139197502647c994fefb49f24bf8e8","ef485aa52b77456eaa8d7b70b5464b5b","eb215ad370cc40099ef625db75ff9390","038e2a0ee2684186bd574bbe65b59aad","13cddb35709a4bd5bb14c573785922dc","e0be08adb7024b9ea94e079cb94a1374","a91fa3e0229d4addad85b56ad5cd23f6","d6d88601abae4aa3863f4baa915ff266","6e8d0f78c1c244b9a6c70157f8a67d37","51f27f0f44724da19531ba2503e5a477","ac465ba90fac4d39afd4b6c4069685ed","ed666ed8c26f43f3b53ce9a5a123744b","68b2fc452493475b991b302802597fbe","3730cb7d2bb8459a87a940f65f47a34d"]},"id":"SBhdYAE1K36H","executionInfo":{"status":"ok","timestamp":1738432026720,"user_tz":-60,"elapsed":20591,"user":{"displayName":"Francesco La Rota","userId":"12370830885715187383"}},"outputId":"54e8659f-0384-440a-a070-d8c5ce512a65","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7807d5ca35414c179050705be150bc13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc94f3848a9f4dbfa2e4121656740fd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57ac976b4a29459fb18f3a039b5951d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/465 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8acb83b3962e40adabed570b5e6e0cc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/18616 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99383b201b5c4d26a7e8c61a0d46fb62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/1035 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d4341b2d4ca4ab59679f77d9bae2141"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/1034 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5596132eb19a497fb7f085bf13db8d41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/263M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"038e2a0ee2684186bd574bbe65b59aad"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["# @title Ternary Classifier Training and Validation\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = logits.argmax(axis=-1)\n","    acc = accuracy_score(labels, predictions)\n","    return {\"accuracy\": acc}\n","\n","# Configurazione degli argomenti per il Trainer\n","training_args = TrainingArguments(\n","    output_dir=\"/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/TernaryClassifier/ProvaFra\",  # Directory per i risultati\n","    evaluation_strategy=\"epoch\",    # Valutazione dopo ogni epoca\n","    learning_rate=5e-5,             # Tasso di apprendimento\n","    per_device_train_batch_size=8,  # Batch size per il training\n","    per_device_eval_batch_size=8,   # Batch size per la valutazione\n","    num_train_epochs=10,            # Numero di epoche\n","    weight_decay=0.01,              # Decadimento del peso\n","    logging_dir=\"./logs\",           # Directory per i log\n","    logging_steps=10,               # Passi di log\n","    save_strategy=\"epoch\",          # Salvare dopo ogni epoca\n","    report_to=[]                    # no reporting to wandb, comet_ml, etc.\n",")\n","\n","# Inizializzazione del Trainer con compute_metrics\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics  # funzione di calcolo delle metriche\n",")\n","\n","# Addestramento del modello\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":530},"id":"G-0MJb_RL43W","outputId":"bbdec7c6-effc-4506-ebd9-2bcb993c0d00","collapsed":true,"executionInfo":{"status":"ok","timestamp":1738433342304,"user_tz":-60,"elapsed":1289942,"user":{"displayName":"Francesco La Rota","userId":"12370830885715187383"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","<ipython-input-7-602d980ccee9>:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='23270' max='23270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [23270/23270 21:26, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.537300</td>\n","      <td>0.347185</td>\n","      <td>0.867633</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.235600</td>\n","      <td>0.370057</td>\n","      <td>0.882126</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.328800</td>\n","      <td>0.442159</td>\n","      <td>0.877295</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.287600</td>\n","      <td>0.481198</td>\n","      <td>0.875362</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.074100</td>\n","      <td>0.525486</td>\n","      <td>0.891787</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.127700</td>\n","      <td>0.524305</td>\n","      <td>0.874396</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.124600</td>\n","      <td>0.655220</td>\n","      <td>0.879227</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.162700</td>\n","      <td>0.677973</td>\n","      <td>0.878261</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.114000</td>\n","      <td>0.639255</td>\n","      <td>0.885990</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.151300</td>\n","      <td>0.675730</td>\n","      <td>0.881159</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=23270, training_loss=0.20870743999773975, metrics={'train_runtime': 1288.0075, 'train_samples_per_second': 144.533, 'train_steps_per_second': 18.067, 'total_flos': 1541285669514240.0, 'train_loss': 0.20870743999773975, 'epoch': 10.0})"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# @title Test the Best Model\n","checkpoint_path = \"/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/TernaryClassifier/DistilBertCheckpoint/checkpoint-11635\"\n","\n","# Load the model from the checkpoint\n","model = DistilBertForSequenceClassification.from_pretrained(checkpoint_path)\n","model.to(trainer.args.device)\n","# Update the trainer's model with the loaded checkpoint\n","trainer.model = model\n","\n","# Evaluate the model on the test dataset\n","predictions = trainer.predict(test_dataset)\n","\n","# Extract predicted labels\n","preds = torch.argmax(torch.tensor(predictions.predictions), axis=1)\n","\n","# Get the true labels from the test dataset\n","true_labels = test_dataset[\"label\"]\n","\n","# Generate and print the classification report\n","print(\"Classification Report:\")\n","print(classification_report(true_labels, preds, target_names=list(label_mapping.keys())))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":208},"id":"fsMszDafqxjj","executionInfo":{"status":"ok","timestamp":1738433942305,"user_tz":-60,"elapsed":5335,"user":{"displayName":"Francesco La Rota","userId":"12370830885715187383"}},"outputId":"fa8a9ea7-473b-40a2-b256-10e7062b574c"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Classification Report:\n","              precision    recall  f1-score   support\n","\n","       trump       0.93      0.86      0.90       387\n","      lyrics       0.77      0.92      0.84       318\n"," shakespeare       0.92      0.82      0.87       329\n","\n","    accuracy                           0.87      1034\n","   macro avg       0.87      0.87      0.87      1034\n","weighted avg       0.88      0.87      0.87      1034\n","\n"]}]},{"cell_type":"markdown","source":["## Train Ternary GAN with enhanced Tokenizer"],"metadata":{"id":"Mn7QBFsxY7ry"}},{"cell_type":"markdown","source":["### Class Definition"],"metadata":{"id":"Z2NLsgDdZkDL"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"ycJm88zSc0eL","executionInfo":{"status":"ok","timestamp":1738531012603,"user_tz":-60,"elapsed":6,"user":{"displayName":"Michele Sgrillo","userId":"00105951137606608722"}}},"outputs":[],"source":["# @title MonostyleDataset Class\n","# Configure logging\n","logging.basicConfig(level=logging.DEBUG)\n","\n","class MonostyleDataset(Dataset):\n","    \"\"\"\n","    Mono-style dataset:\n","    Loads textual data from CSV files, line-based files, or a provided list of sentences.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        dataset_format: str,\n","        dataset_path: str = None,\n","        sentences_list: List[str] = None,\n","        text_column_name: str = None,\n","        separator: str = None,\n","        style: str = None,\n","        max_dataset_samples: int = None,\n","        SEED: int = 42\n","    ):\n","        super().__init__()\n","\n","        self.allowed_dataset_formats = [\"list\", \"csv\", \"line_file\"]\n","        if dataset_format not in self.allowed_dataset_formats:\n","            raise Exception(\n","                f\"MonostyleDataset: '{dataset_format}' is not supported. \"\n","                f\"Allowed formats: {self.allowed_dataset_formats}.\"\n","            )\n","\n","        self.dataset_format = dataset_format\n","        self.dataset_path = dataset_path\n","        self.sentences_list = sentences_list\n","        self.text_column_name = text_column_name\n","        self.separator = separator\n","        self.style = style\n","        self.max_dataset_samples = max_dataset_samples\n","\n","        # Load data based on the format\n","        self.load_data(SEED)\n","\n","    def _load_data_csv(self):\n","        try:\n","            df = pd.read_csv(self.dataset_path, sep=self.separator, header=None, encoding='utf-8')\n","            df.dropna(inplace=True)\n","            if self.text_column_name is not None:\n","                self.data = df[self.text_column_name].tolist()\n","            else:\n","                self.data = df.iloc[:, 0].tolist()\n","            logging.debug(\n","                f\"MonostyleDataset, _load_data_csv: parsed {len(self.data)} examples from '{self.dataset_path}'.\"\n","            )\n","        except UnicodeDecodeError as e:\n","            logging.error(\n","                f\"MonostyleDataset, _load_data_csv: UnicodeDecodeError while reading '{self.dataset_path}': {e}\"\n","            )\n","            raise\n","        except FileNotFoundError:\n","            logging.error(\n","                f\"MonostyleDataset, _load_data_csv: File not found: '{self.dataset_path}'.\"\n","            )\n","            raise\n","        except Exception as e:\n","            logging.error(\n","                f\"MonostyleDataset, _load_data_csv: Error loading CSV dataset: {e}\"\n","            )\n","            raise\n","\n","    def _load_data_line_file(self):\n","        try:\n","            with open(self.dataset_path, 'r', encoding='utf-8') as f:\n","                self.data = f.read().split(self.separator)\n","            logging.debug(\n","                f\"MonostyleDataset, _load_data_line_file: parsed {len(self.data)} examples from '{self.dataset_path}'.\"\n","            )\n","        except UnicodeDecodeError as e:\n","            logging.error(\n","                f\"MonostyleDataset, _load_data_line_file: UnicodeDecodeError while reading '{self.dataset_path}': {e}\"\n","            )\n","            raise\n","        except FileNotFoundError:\n","            logging.error(\n","                f\"MonostyleDataset, _load_data_line_file: File not found: '{self.dataset_path}'.\"\n","            )\n","            raise\n","        except Exception as e:\n","            logging.error(\n","                f\"MonostyleDataset, _load_data_line_file: Error loading line_file dataset: {e}\"\n","            )\n","            raise\n","\n","    def load_data(self, SEED=42):\n","        if self.dataset_format == \"csv\":\n","            self._load_data_csv()\n","        elif self.dataset_format == \"line_file\":\n","            self._load_data_line_file()\n","        elif self.dataset_format == \"list\":\n","            if self.sentences_list is None:\n","                raise Exception(\n","                    \"MonostyleDataset: 'list' format specified but 'sentences_list' is None.\"\n","                )\n","            self.data = self.sentences_list\n","            logging.debug(\n","                f\"MonostyleDataset, load_data: data already loaded, {len(self.data)} examples.\"\n","            )\n","        else:\n","            raise Exception(\n","                f\"MonostyleDataset, load_data: '{self.dataset_format}' format is not supported.\"\n","            )\n","\n","        # Limit the number of samples if needed\n","        if self.max_dataset_samples is not None and self.max_dataset_samples < len(self.data):\n","            random.seed(SEED)\n","            ix = random.sample(range(len(self.data)), self.max_dataset_samples)\n","            self.data = [self.data[i] for i in ix]\n","            logging.debug(f\"MonostyleDataset, load_data: reduced data to {len(self.data)} samples.\")\n","\n","        # Shuffle the data\n","        random.shuffle(self.data)\n","        logging.debug(\"MonostyleDataset, load_data: data has been shuffled.\")\n","\n","    def reduce_data(self, n_samples):\n","        if n_samples < len(self.data):\n","            self.data = self.data[:n_samples]\n","            logging.debug(f\"MonostyleDataset, reduce_data: reduced data to {n_samples} samples.\")\n","        else:\n","            logging.debug(\n","                f\"MonostyleDataset, reduce_data: requested {n_samples}, but dataset has {len(self.data)}.\"\n","            )\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return self.data[idx]\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"jmQaZLhAdKWg","executionInfo":{"status":"ok","timestamp":1738531012603,"user_tz":-60,"elapsed":5,"user":{"displayName":"Michele Sgrillo","userId":"00105951137606608722"}}},"outputs":[],"source":["# @title GeneratorModel Class\n","class GeneratorModel(nn.Module):\n","    def __init__(\n","        self,\n","        model_name_or_path: str,\n","        new_style_tokens: List[str] = None,\n","        pretrained_path: str = None,\n","        max_seq_length: int = 64,\n","        truncation: str = \"longest_first\",\n","        padding: str = \"max_length\",\n","        tokenizer = None\n","    ):\n","        super(GeneratorModel, self).__init__()\n","\n","        self.model_name_or_path = model_name_or_path\n","        self.max_seq_length = max_seq_length\n","        self.truncation = truncation\n","        self.padding = padding\n","\n","        # If no style tokens are provided, use default ones\n","        if new_style_tokens is None:\n","            new_style_tokens = [\n","                '[pos->neu]', '[pos->neg]',\n","                '[neu->pos]', '[neg->pos]',\n","                '[neu->neg]', '[neg->neu]'\n","            ]\n","\n","        if pretrained_path is None:\n","            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n","            if tokenizer:\n","                self.tokenizer = tokenizer\n","            else:\n","                self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n","        else:\n","            self.model = AutoModelForSeq2SeqLM.from_pretrained(pretrained_path)\n","            if tokenizer:\n","                self.tokenizer = tokenizer\n","            else:\n","                self.tokenizer = AutoTokenizer.from_pretrained(f\"{pretrained_path}tokenizer/\")\n","\n","        num_added_tokens = self.tokenizer.add_tokens(new_style_tokens)\n","        print(f\"Added {num_added_tokens} new tokens to the tokenizer.\")\n","\n","        # Resizing embeddings to include the new tokens\n","        self.model.resize_token_embeddings(len(self.tokenizer))\n","        print(f\"New embedding size: {len(self.tokenizer)} tokens.\")\n","\n","    def train(self):\n","        # Setting the model in training mode\n","        self.model.train()\n","\n","    def eval(self):\n","        # Setting the model in evaluation mode\n","        self.model.eval()\n","\n","    def forward(\n","        self,\n","        sentences: List[str],\n","        target_sentences: List[str] = None,\n","        device=None,\n","    ):\n","\n","        inputs = self.tokenizer(\n","            sentences,\n","            truncation=self.truncation,\n","            padding=self.padding,\n","            max_length=self.max_seq_length,\n","            return_tensors=\"pt\"\n","        )\n","\n","        if target_sentences is not None:\n","            target = self.tokenizer(\n","                target_sentences,\n","                truncation=self.truncation,\n","                padding=self.padding,\n","                max_length=self.max_seq_length,\n","                return_tensors=\"pt\"\n","            )\n","            labels = target[\"input_ids\"]\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            output_supervised = self.model(**inputs, labels=labels)\n","\n","        inputs = inputs.to(device)\n","        output = self.model.generate(**inputs, max_length=self.max_seq_length)\n","        transferred_sentences = self.tokenizer.batch_decode(output, skip_special_tokens=True)\n","\n","        if target_sentences is not None:\n","            return output, transferred_sentences, output_supervised.loss\n","        else:\n","            return output, transferred_sentences\n","\n","    def transfer(\n","        self,\n","        sentences: List[str],\n","        device=None\n","    ):\n","        inputs = self.tokenizer(\n","            sentences,\n","            truncation=self.truncation,\n","            padding=self.padding,\n","            max_length=self.max_seq_length,\n","            return_tensors=\"pt\"\n","        )\n","\n","        inputs = inputs.to(device)\n","        output = self.model.generate(**inputs, max_length=self.max_seq_length)\n","        transferred_sentences = self.tokenizer.batch_decode(output, skip_special_tokens=True)\n","        return transferred_sentences\n","\n","    def save_model(\n","        self,\n","        path: Union[str]\n","    ):\n","        self.model.save_pretrained(path)\n","        self.tokenizer.save_pretrained(f\"{path}/tokenizer/\")\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"_xQjnOzndmSu","executionInfo":{"status":"ok","timestamp":1738531012603,"user_tz":-60,"elapsed":5,"user":{"displayName":"Michele Sgrillo","userId":"00105951137606608722"}}},"outputs":[],"source":["# @title DiscriminatorModel Class\n","class DiscriminatorModel(nn.Module):\n","    def __init__(\n","        self,\n","        model_name_or_path: str,\n","        pretrained_path: str = None,\n","        max_seq_length: int = 64,\n","        truncation: str = \"longest_first\",\n","        padding: str = \"max_length\",\n","        tokenizer = None\n","    ):\n","        super(DiscriminatorModel, self).__init__()\n","\n","        self.model_name_or_path = model_name_or_path\n","        self.max_seq_length = max_seq_length\n","        self.truncation = truncation\n","        self.padding = padding\n","\n","        if pretrained_path is None:\n","            self.model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path)\n","            if tokenizer:\n","                self.tokenizer = tokenizer\n","            else:\n","                self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n","        else:\n","            self.model = AutoModelForSequenceClassification.from_pretrained(pretrained_path)\n","            if tokenizer:\n","                self.tokenizer = tokenizer\n","            else:\n","                self.tokenizer = AutoTokenizer.from_pretrained(f\"{pretrained_path}tokenizer/\")\n","\n","    def train(self):\n","        # Set the model in training mode\n","        self.model.train()\n","\n","    def eval(self):\n","        # Set the model in evaluation mode\n","        self.model.eval()\n","\n","    def forward(\n","        self,\n","        sentences: List[str],\n","        target_labels: Tensor,\n","        return_hidden: bool = False,\n","        device=None,\n","    ):\n","        inputs = self.tokenizer(\n","            sentences,\n","            truncation=self.truncation,\n","            padding=self.padding,\n","            max_length=self.max_seq_length,\n","            return_tensors=\"pt\"\n","        )\n","        inputs[\"labels\"] = target_labels\n","        inputs = inputs.to(device)\n","        output = self.model(**inputs, output_hidden_states=return_hidden)\n","        return output, output.loss\n","\n","    def save_model(\n","        self,\n","        path: Union[str]\n","    ):\n","        self.model.save_pretrained(path)\n","        self.tokenizer.save_pretrained(f\"{path}/tokenizer\")\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"TrFuCkRPd2Im","executionInfo":{"status":"ok","timestamp":1738531012603,"user_tz":-60,"elapsed":5,"user":{"displayName":"Michele Sgrillo","userId":"00105951137606608722"}}},"outputs":[],"source":["# @title ClassifierModel Class\n","class ClassifierModel(nn.Module):\n","    def __init__(\n","        self,\n","        pretrained_path: str = None,\n","        max_seq_length: int = 64,\n","        truncation: str = \"longest_first\",\n","        padding: str = \"max_length\",\n","    ):\n","        super(ClassifierModel, self).__init__()\n","\n","        self.max_seq_length = max_seq_length\n","        self.truncation = truncation\n","        self.padding = padding\n","\n","        self.model = AutoModelForSequenceClassification.from_pretrained(pretrained_path)\n","        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_path)\n","        self.model.eval()\n","\n","    def eval(self):\n","        # Set the model in evaluation mode\n","        self.model.eval()\n","\n","    def forward(\n","        self,\n","        sentences: List[str],\n","        target_labels: Tensor,\n","        return_hidden: bool = False,\n","        device=None,\n","    ):\n","        inputs = self.tokenizer(\n","            sentences,\n","            truncation=self.truncation,\n","            padding=self.padding,\n","            max_length=self.max_seq_length,\n","            return_tensors=\"pt\"\n","        )\n","        inputs[\"labels\"] = target_labels\n","        inputs = inputs.to(device)\n","        output = self.model(**inputs, output_hidden_states=return_hidden)\n","        return output, output.loss\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"vrbv203zeGGI","executionInfo":{"status":"ok","timestamp":1738531012603,"user_tz":-60,"elapsed":4,"user":{"displayName":"Michele Sgrillo","userId":"00105951137606608722"}}},"outputs":[],"source":["# @title CycleGANModel Class\n","class CycleGANModel(nn.Module):\n","    def __init__(\n","        self,\n","        G_ab: Union['GeneratorModel', None],\n","        G_ba: Union['GeneratorModel', None],\n","        D_ab: Union['DiscriminatorModel', None],\n","        D_ba: Union['DiscriminatorModel', None],\n","        Cls: Union['ClassifierModel', None],\n","        device=None,\n","        label2id: Dict[str, int] = None\n","    ):\n","        \"\"\"\n","        Initialization method for the CycleGANModel\n","\n","        Args:\n","            G_ab (GeneratorModel): Generator model for mapping A->B\n","            G_ba (GeneratorModel): Generator model for mapping B->A\n","            D_ab (DiscriminatorModel): Discriminator model for B\n","            D_ba (DiscriminatorModel): Discriminator model for A\n","            Cls (ClassifierModel): Style classifier\n","            label2id (Dict[str,int]): Style-to-integer mapping (e.g., {\"neu\": 0, \"pos\": 1, \"neg\": 2})\n","        \"\"\"\n","        super(CycleGANModel, self).__init__()\n","\n","        if G_ab is None or G_ba is None or D_ab is None or D_ba is None:\n","            logging.warning(\n","                \"CycleGANModel: Some models are missing. Please call 'load_models' to load from a previous checkpoint.\"\n","            )\n","\n","        self.G_ab = G_ab\n","        self.G_ba = G_ba\n","        self.D_ab = D_ab\n","        self.D_ba = D_ba\n","        self.Cls = Cls\n","\n","        self.device = device\n","        logging.info(f\"Device: {device}\")\n","\n","        # Use default label2id if none is provided\n","        if label2id is None:\n","            label2id = {\"neu\": 0, \"pos\": 1, \"neg\": 2}\n","        self.label2id = label2id\n","\n","        # Move all models to device\n","        self.G_ab.model.to(self.device)\n","        self.G_ba.model.to(self.device)\n","        self.D_ab.model.to(self.device)\n","        self.D_ba.model.to(self.device)\n","        if self.Cls is not None:\n","            self.Cls.model.to(self.device)\n","\n","    def train(self):\n","        self.G_ab.train()\n","        self.G_ba.train()\n","        self.D_ab.train()\n","        self.D_ba.train()\n","\n","    def eval(self):\n","        self.G_ab.eval()\n","        self.G_ba.eval()\n","        self.D_ab.eval()\n","        self.D_ba.eval()\n","\n","    def get_optimizer_parameters(self):\n","        params = list(self.G_ab.model.parameters())\n","        params += list(self.G_ba.model.parameters())\n","        params += list(self.D_ab.model.parameters())\n","        params += list(self.D_ba.model.parameters())\n","        return params\n","\n","    def training_cycle(\n","        self,\n","        sentences_a: List[str],\n","        sentences_b: List[str],\n","        target_sentences_ab: List[str] = None,\n","        target_sentences_ba: List[str] = None,\n","        style_source=str,\n","        style_target=str,\n","        lambdas: List[float] = None,\n","        loss_logging=None,\n","        training_step: int = None\n","    ):\n","        # ----- Cycle A -> B -----\n","        token_a_b = f\"[{style_source}->{style_target}]\"\n","        token_b_a = f\"[{style_target}->{style_source}]\"\n","\n","        label2id = self.label2id\n","        #print(f\"Label2id: {label2id}\")\n","\n","\n","        # First half\n","        mono_a_with_style = [f\"{token_a_b} {s}\" for s in sentences_a]\n","        _, transferred_ab = self.G_ab(mono_a_with_style, device=self.device)\n","\n","        # D_ab fake\n","        self.D_ab.eval()\n","        zeros = torch.zeros(len(transferred_ab))\n","        ones = torch.ones(len(transferred_ab))\n","        labels_fake_sentences = torch.column_stack((ones, zeros))  # generator side\n","        _, loss_g_ab = self.D_ab(transferred_ab, labels_fake_sentences, device=self.device)\n","\n","        if lambdas[4] != 0:\n","            labels_style_b_sentences = torch.full(\n","                (len(transferred_ab),),\n","                label2id[style_target],\n","                dtype=int\n","            )\n","            _, loss_g_ab_cls = self.Cls(transferred_ab, labels_style_b_sentences, device=self.device)\n","\n","        # Second half\n","        mono_transferred_ab_with_style = [f\"{token_b_a} {s}\" for s in transferred_ab]\n","        _, _, cycle_loss_aba = self.G_ba(mono_transferred_ab_with_style, sentences_a, device=self.device)\n","\n","        complete_loss_g_ab = lambdas[0] * cycle_loss_aba + lambdas[1] * loss_g_ab\n","\n","        loss_logging['Cycle Loss A-B-A'].append((lambdas[0] * cycle_loss_aba).item())\n","        loss_logging['Loss generator  A-B'].append((lambdas[1] * loss_g_ab).item())\n","\n","        if lambdas[4] != 0:\n","            complete_loss_g_ab += lambdas[4] * loss_g_ab_cls\n","\n","            loss_logging['Classifier-guided A-B'].append((lambdas[4] * loss_g_ab_cls).item())\n","\n","        complete_loss_g_ab.backward()\n","\n","        # D_ab training\n","        self.D_ab.train()\n","        zeros = torch.zeros(len(transferred_ab))\n","        ones = torch.ones(len(transferred_ab))\n","        labels_fake_sentences = torch.column_stack((zeros, ones))  # discriminator side\n","        _, loss_d_ab_fake = self.D_ab(transferred_ab, labels_fake_sentences, device=self.device)\n","\n","        zeros = torch.zeros(len(transferred_ab))\n","        ones = torch.ones(len(transferred_ab))\n","        labels_real_sentences = torch.column_stack((ones, zeros))\n","        _, loss_d_ab_real = self.D_ab(sentences_b, labels_real_sentences, device=self.device)\n","        complete_loss_d_ab = lambdas[2] * loss_d_ab_fake + lambdas[3] * loss_d_ab_real\n","\n","\n","        loss_logging['Loss D(A->B)'].append(complete_loss_d_ab.item())\n","        complete_loss_d_ab.backward()\n","\n","        # ----- Cycle B -> A -----\n","        mono_b_with_style = [f\"{token_b_a} {s}\" for s in sentences_b]\n","\n","        # First half\n","        _, transferred_ba = self.G_ba(mono_b_with_style, device=self.device)\n","\n","        # D_ba\n","        self.D_ba.eval()\n","        zeros = torch.zeros(len(transferred_ba))\n","        ones = torch.ones(len(transferred_ba))\n","        labels_fake_sentences = torch.column_stack((ones, zeros))\n","        _, loss_g_ba = self.D_ba(transferred_ba, labels_fake_sentences, device=self.device)\n","\n","        if lambdas[4] != 0:\n","            labels_style_a_sentences = torch.full(\n","                (len(transferred_ba),),\n","                label2id[style_source],\n","                dtype=int\n","            )\n","            _, loss_g_ba_cls = self.Cls(transferred_ba, labels_style_a_sentences, device=self.device)\n","\n","        # Second half\n","        mono_transferred_ba_with_style = [f\"{token_a_b} {s}\" for s in transferred_ba]\n","        _, _, cycle_loss_bab = self.G_ab(mono_transferred_ba_with_style, sentences_b, device=self.device)\n","\n","        complete_loss_g_ba = lambdas[0] * cycle_loss_bab + lambdas[1] * loss_g_ba\n","\n","        loss_logging['Cycle Loss B-A-B'].append((lambdas[0] * cycle_loss_bab).item())\n","        loss_logging['Loss generator  B-A'].append((lambdas[1] * loss_g_ba).item())\n","\n","        if lambdas[4] != 0:\n","            complete_loss_g_ba += lambdas[4] * loss_g_ba_cls\n","            loss_logging['Classifier-guided B-A'].append((lambdas[4] * loss_g_ba_cls).item())\n","\n","        complete_loss_g_ba.backward()\n","\n","        # D_ba training\n","        self.D_ba.train()\n","        zeros = torch.zeros(len(transferred_ba))\n","        ones = torch.ones(len(transferred_ba))\n","        labels_fake_sentences = torch.column_stack((zeros, ones))\n","        _, loss_d_ba_fake = self.D_ba(transferred_ba, labels_fake_sentences, device=self.device)\n","\n","        zeros = torch.zeros(len(transferred_ba))\n","        ones = torch.ones(len(transferred_ba))\n","        labels_real_sentences = torch.column_stack((ones, zeros))\n","        _, loss_d_ba_real = self.D_ba(sentences_a, labels_real_sentences, device=self.device)\n","        complete_loss_d_ba = lambdas[2] * loss_d_ba_fake + lambdas[3] * loss_d_ba_real\n","\n","        loss_logging['Loss D(B->A)'].append(complete_loss_d_ba.item())\n","        complete_loss_d_ba.backward()\n","\n","    def save_models(self, base_path: Union[str]):\n","        self.G_ab.save_model(base_path + \"/G_ab/\")\n","        self.G_ba.save_model(base_path + \"/G_ba/\")\n","        self.D_ab.save_model(base_path + \"/D_ab/\")\n","        self.D_ba.save_model(base_path + \"/D_ba/\")\n","\n","    def transfer(self, sentences: List[str], direction: str):\n","        if direction == \"AB\":\n","            transferred_sentences = self.G_ab.transfer(sentences, device=self.device)\n","        else:\n","            transferred_sentences = self.G_ba.transfer(sentences, device=self.device)\n","        return transferred_sentences\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"AJjbz7tCeaCh","executionInfo":{"status":"ok","timestamp":1738531012603,"user_tz":-60,"elapsed":4,"user":{"displayName":"Michele Sgrillo","userId":"00105951137606608722"}}},"outputs":[],"source":["# @title Evaluator Class\n","class Evaluator():\n","    def __init__(self, cycleGAN, args, experiment=None, label2id=None):\n","        \"\"\"\n","        Class for evaluation\n","        \"\"\"\n","        super(Evaluator, self).__init__()\n","\n","        self.cycleGAN = cycleGAN\n","        self.args = args\n","        self.experiment = experiment\n","\n","        # If label2id is not provided, use a default mapping\n","        if label2id is None:\n","            label2id = {\"neu\": 0, \"pos\": 1, \"neg\": 2}\n","        self.label2id = label2id\n","\n","        self.bleu = evaluate.load('sacrebleu')\n","        self.rouge = evaluate.load('rouge')\n","        # if args.bertscore: self.bertscore = evaluate.load('bertscore')\n","\n","\n","    def __compute_metric__(self, predictions, references, metric_name, direction=None):\n","        # predictions = list | references = list of lists\n","        scores = []\n","        if metric_name in ['bleu', 'rouge']:\n","            for pred, ref in zip(predictions, references):\n","                if metric_name == 'bleu':\n","                    res = self.bleu.compute(predictions=[pred], references=[ref])\n","                    scores.append(res['score'])\n","                elif metric_name == 'rouge':\n","                    tmp_rouge1, tmp_rouge2, tmp_rougeL = [], [], []\n","                    for r in ref:\n","                        res = self.rouge.compute(predictions=[pred], references=[r], use_aggregator=False)\n","                        tmp_rouge1.append(res['rouge1'][0])\n","                        tmp_rouge2.append(res['rouge2'][0])\n","                        tmp_rougeL.append(res['rougeL'][0])\n","                    scores.append([max(tmp_rouge1), max(tmp_rouge2), max(tmp_rougeL)])\n","        else:\n","            raise Exception(f\"Metric {metric_name} is not supported.\")\n","        return scores\n","\n","    def __compute_classif_metrics__(self, pred_A, pred_B, style_A, style_B):\n","        # Using self.label2id\n","        label2id = self.label2id\n","\n","        device = self.cycleGAN.device\n","        truncation, padding = 'longest_first', 'max_length'\n","\n","        # If certain conditions are met, load an external classifier instead of using self.cycleGAN.Cls\n","        if ('lambdas' not in vars(self.args)\n","            or self.args.lambdas[4] == 0\n","            or self.args.pretrained_classifier_eval != self.args.pretrained_classifier_model):\n","            classifier = AutoModelForSequenceClassification.from_pretrained(self.args.pretrained_classifier_eval)\n","            classifier_tokenizer = AutoTokenizer.from_pretrained(f'{self.args.pretrained_classifier_eval}tokenizer/')\n","            classifier.to(device)\n","        else:\n","            classifier = self.cycleGAN.Cls.model\n","            classifier_tokenizer = self.cycleGAN.Cls.tokenizer\n","        classifier.eval()\n","\n","        y_pred, y_true = [], np.concatenate([\n","            np.full(len(pred_A), label2id[style_A]),\n","            np.full(len(pred_B), label2id[style_B])\n","        ])\n","\n","        for i in range(0, len(pred_A), self.args.batch_size):\n","            batch_a = pred_A[i:i+self.args.batch_size]\n","            inputs = classifier_tokenizer(\n","                batch_a,\n","                truncation=truncation,\n","                padding=padding,\n","                max_length=self.args.max_sequence_length,\n","                return_tensors=\"pt\"\n","            )\n","            inputs = inputs.to(device)\n","            with torch.no_grad():\n","                output = classifier(**inputs)\n","            y_pred.extend(np.argmax(output.logits.cpu().numpy(), axis=1))\n","\n","        for i in range(0, len(pred_B), self.args.batch_size):\n","            batch_b = pred_B[i:i+self.args.batch_size]\n","            inputs = classifier_tokenizer(\n","                batch_b,\n","                truncation=truncation,\n","                padding=padding,\n","                max_length=self.args.max_sequence_length,\n","                return_tensors=\"pt\"\n","            )\n","            inputs = inputs.to(device)\n","            with torch.no_grad():\n","                output = classifier(**inputs)\n","            y_pred.extend(np.argmax(output.logits.cpu().numpy(), axis=1))\n","\n","        acc = accuracy_score(y_true, y_pred)\n","        prec = precision_score(y_true, y_pred, average='macro', zero_division=0)\n","        rec = recall_score(y_true, y_pred, average='macro', zero_division=0)\n","        f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n","        return acc, prec, rec, f1\n","\n","    def run_eval_mono(self, epoch, current_training_step, phase, dl_source, dl_target, style_source, style_target):\n","        print(f'Start {phase}...')\n","        self.cycleGAN.eval()\n","\n","        real_A, real_B = [], []\n","        pred_A, pred_B = [], []\n","        scores_AB_bleu_self, scores_BA_bleu_self = [], []\n","        scores_AB_r1_self, scores_BA_r1_self = [], []\n","        scores_AB_r2_self, scores_BA_r2_self = [], []\n","        scores_AB_rL_self, scores_BA_rL_self = [], []\n","\n","        # Define the style token for style B\n","        style_token_A = f\"[{dl_target.dataset.style}->{dl_source.dataset.style}]\"\n","        style_token_B = f\"[{dl_source.dataset.style}->{dl_target.dataset.style}]\"\n","\n","        for batch in dl_source:\n","            mono_a = list(batch)\n","            mono_a_with_style = [f\"{style_token_B} {sentence}\" for sentence in mono_a]\n","            with torch.no_grad():\n","                transferred = self.cycleGAN.transfer(sentences=mono_a_with_style, direction='AB')\n","            real_A.extend(mono_a)\n","            pred_B.extend(transferred)\n","            mono_a = [[s] for s in mono_a]\n","            scores_AB_bleu_self.extend(self.__compute_metric__(transferred, mono_a, 'bleu'))\n","            scores_rouge_self = np.array(self.__compute_metric__(transferred, mono_a, 'rouge'))\n","            scores_AB_r1_self.extend(scores_rouge_self[:, 0].tolist())\n","            scores_AB_r2_self.extend(scores_rouge_self[:, 1].tolist())\n","            scores_AB_rL_self.extend(scores_rouge_self[:, 2].tolist())\n","\n","        avg_AB_bleu_self = np.mean(scores_AB_bleu_self)\n","        avg_AB_r1_self = np.mean(scores_AB_r1_self)\n","        avg_AB_r2_self = np.mean(scores_AB_r2_self)\n","        avg_AB_rL_self = np.mean(scores_AB_rL_self)\n","\n","        for batch in dl_target:\n","            mono_b = list(batch)\n","            mono_b_with_style = [f\"{style_token_A} {sentence}\" for sentence in mono_b]\n","            with torch.no_grad():\n","                transferred = self.cycleGAN.transfer(sentences=mono_b_with_style, direction='BA')\n","            real_B.extend(mono_b)\n","            pred_A.extend(transferred)\n","            mono_b = [[s] for s in mono_b]\n","            scores_BA_bleu_self.extend(self.__compute_metric__(transferred, mono_b, 'bleu'))\n","            scores_rouge_self = np.array(self.__compute_metric__(transferred, mono_b, 'rouge'))\n","            scores_BA_r1_self.extend(scores_rouge_self[:, 0].tolist())\n","            scores_BA_r2_self.extend(scores_rouge_self[:, 1].tolist())\n","            scores_BA_rL_self.extend(scores_rouge_self[:, 2].tolist())\n","\n","        avg_BA_bleu_self = np.mean(scores_BA_bleu_self)\n","        avg_BA_r1_self = np.mean(scores_BA_r1_self)\n","        avg_BA_r2_self = np.mean(scores_BA_r2_self)\n","        avg_BA_rL_self = np.mean(scores_BA_rL_self)\n","        avg_2dir_bleu_self = (avg_AB_bleu_self + avg_BA_bleu_self) / 2\n","\n","        acc, _, _, _ = self.__compute_classif_metrics__(pred_A, pred_B, style_source, style_target)\n","        acc_scaled = acc * 100\n","        avg_acc_bleu_self = (avg_2dir_bleu_self + acc_scaled) / 2\n","        avg_acc_bleu_self_geom = (avg_2dir_bleu_self * acc_scaled) ** 0.5\n","        avg_acc_bleu_self_h = (2 * avg_2dir_bleu_self * acc_scaled) / (avg_2dir_bleu_self + acc_scaled + 1e-6)\n","\n","        metrics = {\n","            'epoch': epoch,\n","            'step': current_training_step,\n","            'self-BLEU A->B': avg_AB_bleu_self,\n","            'self-BLEU B->A': avg_BA_bleu_self,\n","            'self-BLEU avg': avg_2dir_bleu_self,\n","            'self-ROUGE-1 A->B': avg_AB_r1_self,\n","            'self-ROUGE-1 B->A': avg_BA_r1_self,\n","            'self-ROUGE-2 A->B': avg_AB_r2_self,\n","            'self-ROUGE-2 B->A': avg_BA_r2_self,\n","            'self-ROUGE-L A->B': avg_AB_rL_self,\n","            'self-ROUGE-L B->A': avg_BA_rL_self,\n","            'style accuracy': acc,\n","            'acc-BLEU': avg_acc_bleu_self,\n","            'g-acc-BLEU': avg_acc_bleu_self_geom,\n","            'h-acc-BLEU': avg_acc_bleu_self_h\n","        }\n","\n","        if phase[:10] == 'validation':\n","            base_path = f\"{self.args.save_base_folder}epoch_{epoch}/\"\n","            suffix = f'epoch{epoch}'\n","        else:\n","            if self.args.from_pretrained is not None:\n","                if self.args.save_base_folder is not None:\n","                    base_path = f\"{self.args.save_base_folder}\"\n","                else:\n","                    base_path = f\"{self.args.from_pretrained}epoch_{epoch}/\"\n","            else:\n","                base_path = f\"{self.args.save_base_folder}test/epoch_{epoch}/\"\n","            suffix = f'epoch{epoch}_test'\n","\n","        os.makedirs(os.path.dirname(base_path), exist_ok=True)\n","        pickle.dump(metrics, open(f\"{base_path}metrics_{suffix}.pickle\", 'wb'))\n","\n","        for m, v in metrics.items():\n","            if m not in ['epoch', 'step']:\n","                print(f'{m}: {v}')\n","\n","        df_AB = pd.DataFrame()\n","        df_AB['A (source)'] = real_A\n","        df_AB['B (generated)'] = pred_B\n","        df_AB.to_csv(f\"{base_path}{style_source}_{style_target}_{suffix}.csv\", sep=',', header=True)\n","\n","        df_BA = pd.DataFrame()\n","        df_BA['B (source)'] = real_B\n","        df_BA['A (generated)'] = pred_A\n","        df_BA.to_csv(f\"{base_path}{style_target}_{style_source}_{suffix}.csv\", sep=',', header=True)\n","\n","        del df_AB, df_BA\n","        print(f'End {phase}...')\n","\n","    def dummy_classif(self):\n","        pred_A = [\n","            'wake up or you are going to lose your business .',\n","            'this place has none of them .',\n","            'it is april and there are no grass tees yet .',\n","            'there is no grass on the range .',\n","            'bottom line , this place sucks .',\n","            'someone should buy this place .',\n","            'very disappointed in the customer service .',\n","            'we will not be back .'\n","        ]\n","        pred_B = [\n","            'huge sandwich !',\n","            'i added mushrooms , it was very flavorful .',\n","            'he enjoyed it as well .',\n","            'fast and friendly service .',\n","            'will definitely be back .',\n","            \"my dad 's favorite .\",\n","            'huge burgers , fish sandwiches , salads .',\n","            'decent service .'\n","        ]\n","        acc, _, _, _ = self.__compute_classif_metrics__(pred_A, pred_B, 'neg', 'pos')\n","        print('Dummy classification metrics computation end')\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"HretbKWCf-tU","executionInfo":{"status":"ok","timestamp":1738531013608,"user_tz":-60,"elapsed":1008,"user":{"displayName":"Michele Sgrillo","userId":"00105951137606608722"}}},"outputs":[],"source":["# @title Main function (training pipeline)\n","def main(args):\n","    # List of required attributes\n","    required_attrs = [\n","        \"epochs\", \"style_a\", \"style_b\", \"style_c\",\n","        \"path_mono_A\", \"path_mono_B\", \"path_mono_C\",\n","        \"path_mono_A_eval\", \"path_mono_B_eval\", \"path_mono_C_eval\",\n","        \"batch_size\", \"max_samples_train\", \"max_samples_eval\",\n","        \"nonparal_same_size\", \"generator_model_tag\", \"discriminator_model_tag\",\n","        \"pretrained_classifier_model\", \"pretrained_classifier_eval\",\n","        \"from_pretrained\", \"save_base_folder\", \"save_steps\",\n","        \"lambdas\", \"learning_rate\", \"max_sequence_length\",\n","        \"lr_scheduler_type\", \"warmup_ratio\", \"use_cuda_if_available\"\n","    ]\n","\n","    # Check for missing attributes\n","    missing_attrs = [attr for attr in required_attrs if not hasattr(args, attr)]\n","    if missing_attrs:\n","        raise AttributeError(f\"Args object is missing: {', '.join(missing_attrs)}\")\n","\n","    # Seeding\n","    SEED = 42\n","    torch.manual_seed(SEED)\n","    torch.cuda.manual_seed(SEED)\n","    np.random.seed(SEED)\n","    random.seed(SEED)\n","\n","    # Print paths for debugging\n","    print(f\"Loading dataset A from: {args.path_mono_A}\")\n","    print(f\"Loading dataset B from: {args.path_mono_B}\")\n","    print(f\"Loading dataset C from: {args.path_mono_C}\")\n","\n","    # ----- Load datasets -----\n","    mono_ds_a = MonostyleDataset(\n","        dataset_format=\"line_file\",\n","        dataset_path=args.path_mono_A,\n","        style=args.style_a,\n","        separator='\\n',\n","        max_dataset_samples=args.max_samples_train,\n","    )\n","    mono_ds_b = MonostyleDataset(\n","        dataset_format=\"line_file\",\n","        style=args.style_b,\n","        dataset_path=args.path_mono_B,\n","        separator='\\n',\n","        max_dataset_samples=args.max_samples_train,\n","    )\n","    mono_ds_c = MonostyleDataset(\n","        dataset_format=\"line_file\",\n","        style=args.style_c,\n","        dataset_path=args.path_mono_C,\n","        separator='\\n',\n","        max_dataset_samples=args.max_samples_train,\n","    )\n","\n","    # Parse lambdas\n","    lambdas = [float(l) for l in args.lambdas.split('|')]\n","\n","    # Print all args for clarity\n","    hyper_params = {}\n","    print(\"\\nArguments summary:\")\n","    for key, value in vars(args).items():\n","        hyper_params[key] = value\n","        print(f\"  {key}:\\t{value}\")\n","\n","    # If specified, reduce all datasets to the same size\n","    if args.nonparal_same_size:\n","        min_len = min(len(mono_ds_a), len(mono_ds_b), len(mono_ds_c))\n","        mono_ds_a.reduce_data(min_len)\n","        mono_ds_b.reduce_data(min_len)\n","        mono_ds_c.reduce_data(min_len)\n","\n","    # Create eval datasets\n","    mono_ds_a_eval = MonostyleDataset(\n","        dataset_format=\"line_file\",\n","        style=args.style_a,\n","        dataset_path=args.path_mono_A_eval,\n","        separator='\\n',\n","        max_dataset_samples=args.max_samples_eval\n","    )\n","    mono_ds_b_eval = MonostyleDataset(\n","        dataset_format=\"line_file\",\n","        style=args.style_b,\n","        dataset_path=args.path_mono_B_eval,\n","        separator='\\n',\n","        max_dataset_samples=args.max_samples_eval\n","    )\n","    mono_ds_c_eval = MonostyleDataset(\n","        dataset_format=\"line_file\",\n","        style=args.style_c,\n","        dataset_path=args.path_mono_C_eval,\n","        separator='\\n',\n","        max_dataset_samples=args.max_samples_eval\n","    )\n","\n","    # Dataloaders\n","    mono_dl_a = DataLoader(mono_ds_a, batch_size=args.batch_size, shuffle=True)\n","    mono_dl_b = DataLoader(mono_ds_b, batch_size=args.batch_size, shuffle=True)\n","    mono_dl_c = DataLoader(mono_ds_c, batch_size=args.batch_size, shuffle=True)\n","\n","    mono_dl_a_eval = DataLoader(mono_ds_a_eval, batch_size=args.batch_size, shuffle=False)\n","    mono_dl_b_eval = DataLoader(mono_ds_b_eval, batch_size=args.batch_size, shuffle=False)\n","    mono_dl_c_eval = DataLoader(mono_ds_c_eval, batch_size=args.batch_size, shuffle=False)\n","\n","    # Optional: free memory\n","    del mono_ds_a, mono_ds_b, mono_ds_c\n","    del mono_ds_a_eval, mono_ds_b_eval, mono_ds_c_eval\n","\n","    '''\n","    ----- ----- ----- ----- ----- ----- ----- -----\n","              Load Tokenizers to add New Tokens\n","    ----- ----- ----- ----- ----- ----- ----- -----\n","    '''\n","\n","    gen_tokenizer_AB = AutoTokenizer.from_pretrained(\"facebook/bart-base\")  # For Trump → ALL\n","    gen_tokenizer_BA = AutoTokenizer.from_pretrained(\"facebook/bart-base\")  # For ALL → Trump\n","\n","    disc_tokenizer_AB = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")  # Discriminator for Trump → ALL\n","    disc_tokenizer_BA = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")  # Discriminator for ALL → Trump\n","\n","    def extract_new_tokens(dataset_paths, tokenizer, max_word_length=20):\n","        existing_vocab = set(tokenizer.get_vocab().keys())\n","        new_tokens = set()\n","\n","        # Regex pattern to match only valid words (no punctuation, no digits)\n","        valid_word_pattern = re.compile(r\"^[a-z]+$\")\n","\n","        for path in dataset_paths:\n","            with open(path, 'r', encoding='utf-8') as file:\n","                for line in file:\n","                    words = line.strip().split()\n","                    for word in words:\n","                        word = word.lower().strip()  # Convert to lowercase and trim spaces\n","                        if (\n","                            word not in existing_vocab and  # Not already in vocab\n","                            valid_word_pattern.match(word) and  # No symbols or digits\n","                            len(word) > 1 and len(word) <= max_word_length and  # Length constraint\n","                            \"_\" not in word # Remove placeholders\n","                        ):\n","                            new_tokens.add(word)\n","        return list(new_tokens)\n","\n","    # Extract words separately for each style\n","    trump_words = extract_new_tokens([args.path_mono_A], gen_tokenizer_AB)  # Words in Trump dataset, etc\n","    shakespeare_words = extract_new_tokens([args.path_mono_B], gen_tokenizer_BA)\n","    lyrics_words = extract_new_tokens([args.path_mono_C], gen_tokenizer_BA)\n","\n","    print(f\"Trump-specific tokens: {len(trump_words)}, Shakespeare-specific tokens: {len(shakespeare_words)}, Lyrics-specific tokens: {len(lyrics_words)}\")\n","\n","     # Update tokenizers accordingly\n","    gen_tokenizer_AB.add_tokens(shakespeare_words)  # G_AB must generate Shakespeare and Trump\n","    gen_tokenizer_AB.add_tokens(trump_words)\n","    gen_tokenizer_BA.add_tokens(lyrics_words)       # G_BA must generate lyrics\n","\n","    disc_tokenizer_AB.add_tokens(shakespeare_words)  # D_AB must classify Trump and Lyrics\n","    disc_tokenizer_AB.add_tokens(trump_words)\n","    disc_tokenizer_BA.add_tokens(lyrics_words)       # D_BA must classify lyrics style\n","\n","    print(f\"{len(shakespeare_words+trump_words)} new tokens added to G_AB & D_AB (Trump → Shakespeare and Lyrics)\")\n","    print(f\"{len(lyrics_words)} new tokens added to G_BA & D_BA (Shakespeare/Lyrics → Trump)\")\n","\n","    # ----- Instantiate G, D, Cls -----\n","    if args.from_pretrained:\n","        G_ab = GeneratorModel(\n","            model_name_or_path=args.generator_model_tag,\n","            new_style_tokens=args.style_token_list,\n","            pretrained_path=f\"{args.from_pretrained}G_ab/\",\n","            max_seq_length=args.max_sequence_length, tokenizer=gen_tokenizer_AB\n","        )\n","        G_ba = GeneratorModel(\n","            model_name_or_path=args.generator_model_tag,\n","            new_style_tokens=args.style_token_list,\n","            pretrained_path=f\"{args.from_pretrained}G_ba/\",\n","            max_seq_length=args.max_sequence_length, tokenizer=gen_tokenizer_BA\n","        )\n","        D_ab = DiscriminatorModel(\n","            args.discriminator_model_tag,\n","            f\"{args.from_pretrained}D_ab/\",\n","            max_seq_length=args.max_sequence_length, tokenizer=disc_tokenizer_AB\n","        )\n","        D_ba = DiscriminatorModel(\n","            args.discriminator_model_tag,\n","            f\"{args.from_pretrained}D_ba/\",\n","            max_seq_length=args.max_sequence_length, tokenizer=disc_tokenizer_BA\n","        )\n","        print(\"[INFO] Loaded pretrained G_ab, G_ba, D_ab, D_ba\")\n","    else:\n","        G_ab = GeneratorModel(\n","            model_name_or_path=args.generator_model_tag,\n","            new_style_tokens=args.style_token_list,\n","            max_seq_length=args.max_sequence_length, tokenizer=gen_tokenizer_AB\n","        )\n","        G_ba = GeneratorModel(\n","            model_name_or_path=args.generator_model_tag,\n","            new_style_tokens=args.style_token_list,\n","            max_seq_length=args.max_sequence_length, tokenizer=gen_tokenizer_BA\n","        )\n","        D_ab = DiscriminatorModel(\n","            args.discriminator_model_tag,\n","            max_seq_length=args.max_sequence_length, tokenizer=disc_tokenizer_AB\n","        )\n","        #tokenizer=disc_tokenizer_AB\n","        D_ba = DiscriminatorModel(\n","            args.discriminator_model_tag,\n","            max_seq_length=args.max_sequence_length,tokenizer=disc_tokenizer_BA\n","        )\n","        #tokenizer=disc_tokenizer_BA\n","        print(\"[INFO] Using fresh G_ab, G_ba, D_ab, D_ba\")\n","\n","    '''\n","    ----- ----- ----- ----- ----- ----- ----- -----\n","             Resize Token Embeddings\n","    ----- ----- ----- ----- ----- ----- ----- -----\n","    '''\n","    G_ab.model.resize_token_embeddings(len(gen_tokenizer_AB))\n","    G_ba.model.resize_token_embeddings(len(gen_tokenizer_BA))\n","\n","    D_ab.model.resize_token_embeddings(len(disc_tokenizer_AB))\n","    D_ba.model.resize_token_embeddings(len(disc_tokenizer_BA))\n","\n","\n","    # If we need the classifier\n","    if lambdas[4] != 0 and args.pretrained_classifier_model:\n","        Cls = ClassifierModel(args.pretrained_classifier_model, max_seq_length=args.max_sequence_length)\n","        print(\"[INFO] Loaded pretrained classifier\")\n","    else:\n","        Cls = None\n","\n","    # Device\n","    if args.use_cuda_if_available and torch.cuda.is_available():\n","        device = torch.device(\"cuda\")\n","    else:\n","        device = torch.device(\"cpu\")\n","    print(f\"Device: {device}\")\n","\n","    # Create CycleGAN\n","    cycleGAN = CycleGANModel(\n","        G_ab=G_ab,\n","        G_ba=G_ba,\n","        D_ab=D_ab,\n","        D_ba=D_ba,\n","        Cls=Cls,\n","        device=device,\n","        label2id=args.label2id\n","    )\n","\n","    # Calculate total training steps\n","    n_batch_ab = min(len(mono_dl_a), len(mono_dl_b))\n","    n_batch_ac = min(len(mono_dl_a), len(mono_dl_c))\n","    steps_per_epoch = n_batch_ab + n_batch_ac\n","    total_training_steps = args.epochs * steps_per_epoch\n","\n","    # Optimizer\n","    optimizer = AdamW(cycleGAN.get_optimizer_parameters(), lr=args.learning_rate)\n","\n","    # Scheduler\n","    scheduler = CosineAnnealingWarmRestarts(\n","        optimizer,\n","        T_0=int(steps_per_epoch / 2),\n","        T_mult=1,\n","        eta_min=0\n","    )\n","\n","    current_training_step = 0\n","    start_epoch = 0\n","\n","    # Resume checkpoint if available\n","    if args.from_pretrained and os.path.exists(f\"{args.from_pretrained}checkpoint.pth\"):\n","        ckpt = torch.load(f\"{args.from_pretrained}checkpoint.pth\", map_location=\"cpu\")\n","        optimizer.load_state_dict(ckpt[\"optimizer\"])\n","        scheduler.load_state_dict(ckpt[\"lr_scheduler\"])\n","        current_training_step = ckpt[\"training_step\"]\n","        del ckpt\n","\n","    # Evaluator\n","    evaluator = Evaluator(cycleGAN, args, label2id=args.label2id)\n","\n","    # Training subphase function\n","    def train_subphase(dataloader_a, dataloader_x, style_src, style_tgt, loss_log):\n","        nonlocal current_training_step\n","        n_batch = min(len(dataloader_a), len(dataloader_x))\n","        progress_bar = tqdm(range(n_batch), desc=f\"{style_src}->{style_tgt}\")\n","\n","        cycleGAN.train()\n","        for batch_a, batch_x in zip(dataloader_a, dataloader_x):\n","            # Ensure batch_a and batch_x have the same size\n","            len_a, len_x = len(batch_a), len(batch_x)\n","            if len_a > len_x:\n","                batch_a = batch_a[:len_x]\n","            elif len_x > len_a:\n","                batch_x = batch_x[:len_a]\n","\n","            cycleGAN.training_cycle(\n","                sentences_a=batch_a,\n","                sentences_b=batch_x,\n","                style_source=style_src,\n","                style_target=style_tgt,\n","                lambdas=lambdas,\n","                loss_logging=loss_log,\n","                training_step=current_training_step\n","            )\n","\n","            optimizer.step()\n","            scheduler.step()\n","            optimizer.zero_grad()\n","\n","            current_training_step += 1\n","            progress_bar.update(1)\n","\n","        progress_bar.close()\n","\n","    # Loss logging\n","    loss_logging = {\n","        'Cycle Loss A-B-A': [],\n","        'Loss generator  A-B': [],\n","        'Classifier-guided A-B': [],\n","        'Loss D(A->B)': [],\n","        'Cycle Loss B-A-B': [],\n","        'Loss generator  B-A': [],\n","        'Classifier-guided B-A': [],\n","        'Loss D(B->A)': []\n","    }\n","    loss_logging['hyper_params'] = hyper_params\n","\n","    # ----- Training loop -----\n","    for epoch_idx in range(start_epoch, args.epochs):\n","        print(f\"\\n=== EPOCH {epoch_idx} ===\")\n","\n","        # (1) A->B\n","        train_subphase(mono_dl_a, mono_dl_b, style_src=args.style_a, style_tgt=args.style_b, loss_log=loss_logging)\n","        # (2) A->C\n","        train_subphase(mono_dl_a, mono_dl_c, style_src=args.style_a, style_tgt=args.style_c, loss_log=loss_logging)\n","\n","        # (3) End-of-epoch evaluation\n","        evaluator.run_eval_mono(\n","            epoch_idx,\n","            current_training_step,\n","            phase=\"validation_AB_epoch\",\n","            dl_source=mono_dl_a_eval,\n","            dl_target=mono_dl_b_eval,\n","            style_source=args.style_a,\n","            style_target=args.style_b\n","        )\n","        evaluator.run_eval_mono(\n","            epoch_idx,\n","            current_training_step,\n","            phase=\"validation_AC_epoch\",\n","            dl_source=mono_dl_a_eval,\n","            dl_target=mono_dl_c_eval,\n","            style_source=args.style_a,\n","            style_target=args.style_c\n","        )\n","\n","        # (4) Checkpoint saving\n","        if epoch_idx % args.save_steps == 0:\n","            cycleGAN.save_models(f\"{args.save_base_folder}epoch_{epoch_idx}/\")\n","\n","            checkpoint = {\n","                'epoch': epoch_idx + 1,\n","                'training_step': current_training_step,\n","                'optimizer': optimizer.state_dict(),\n","                'lr_scheduler': scheduler.state_dict()\n","            }\n","            torch.save(checkpoint, f\"{args.save_base_folder}/checkpoint.pth\")\n","\n","            # Remove old loss file if needed\n","            if epoch_idx > 0:\n","                prev_loss_file = f\"{args.save_base_folder}loss.pickle\"\n","                if os.path.exists(prev_loss_file):\n","                    os.remove(prev_loss_file)\n","\n","            # Save training loss\n","            pickle.dump(loss_logging, open(f\"{args.save_base_folder}loss.pickle\", \"wb\"))\n","\n","        cycleGAN.train()\n","\n","    print(\"\\n=== Training completed ===\")\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"yaMsOsqVIxZO","executionInfo":{"status":"ok","timestamp":1738531013608,"user_tz":-60,"elapsed":2,"user":{"displayName":"Michele Sgrillo","userId":"00105951137606608722"}}},"outputs":[],"source":["# @title Args Class to configure the GAN\n","class Args:\n","    def __init__(\n","        self,\n","        epochs,\n","        style_a,\n","        style_b,\n","        style_c,\n","        path_mono_A,\n","        path_mono_B,\n","        path_mono_C,\n","        path_mono_A_eval,\n","        path_mono_B_eval,\n","        path_mono_C_eval,\n","        generator_model_tag,\n","        discriminator_model_tag,\n","        label2id,\n","        style_token_list,\n","        batch_size=8,\n","        max_samples_train=None,\n","        max_samples_eval=None,\n","        nonparal_same_size=False,\n","        pretrained_classifier_model=None,\n","        pretrained_classifier_eval=None,\n","        from_pretrained=None,\n","        save_base_folder=\"./checkpoints/\",\n","        save_steps=1,\n","        lambdas=\"10|1|1|1|1|1\",\n","        learning_rate=5e-5,\n","        max_sequence_length=32,\n","        lr_scheduler_type=\"cosine_with_restarts\",\n","        warmup_ratio=0.0,\n","        use_cuda_if_available=False\n","    ):\n","        \"\"\"\n","        Class to store all training/testing arguments.\n","        \"\"\"\n","        self.epochs = epochs\n","        self.style_a = style_a\n","        self.style_b = style_b\n","        self.style_c = style_c\n","        self.path_mono_A = path_mono_A\n","        self.path_mono_B = path_mono_B\n","        self.path_mono_C = path_mono_C\n","        self.path_mono_A_eval = path_mono_A_eval\n","        self.path_mono_B_eval = path_mono_B_eval\n","        self.path_mono_C_eval = path_mono_C_eval\n","        self.generator_model_tag = generator_model_tag\n","        self.discriminator_model_tag = discriminator_model_tag\n","        self.batch_size = batch_size\n","        self.max_samples_train = max_samples_train\n","        self.max_samples_eval = max_samples_eval\n","        self.nonparal_same_size = nonparal_same_size\n","        self.pretrained_classifier_model = pretrained_classifier_model\n","        self.pretrained_classifier_eval = pretrained_classifier_eval\n","        self.from_pretrained = from_pretrained\n","        self.save_base_folder = save_base_folder\n","        self.save_steps = save_steps\n","        self.lambdas = lambdas\n","        self.learning_rate = learning_rate\n","        self.max_sequence_length = max_sequence_length\n","        self.lr_scheduler_type = lr_scheduler_type\n","        self.warmup_ratio = warmup_ratio\n","        self.use_cuda_if_available = use_cuda_if_available\n","        self.label2id = label2id\n","        self.style_token_list = style_token_list\n"]},{"cell_type":"markdown","source":["### Training and Validation"],"metadata":{"id":"Rd5g4729aV9k"}},{"cell_type":"code","source":["# Create the label2id map based on the defined styles\n","label2id = {\n","    \"tru\": 0,\n","    \"lyr\": 1,\n","    \"sha\": 2\n","}\n","\n","# list of style tokens\n","style_token_list = [\n","    \"[tru->lyr]\", \"[tru->sha]\",\n","    \"[lyr->tru]\", \"[lyr->sha]\",\n","    \"[sha->tru]\", \"[sha->lyr]\"\n","]\n","\n","# Define args directly in the notebook\n","args = Args(\n","      epochs=30,\n","    style_a=\"lyr\",\n","    style_b=\"tru\",\n","    style_c=\"sha\",\n","    path_mono_A=\"/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/Data_spellchecked/train_lyrics_spellchecked.txt\",\n","    path_mono_B=\"/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/Data_spellchecked/train_trump_spellchecked.txt\",\n","    path_mono_C=\"/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/Data_spellchecked/train_shakespeare_spellchecked.txt\",\n","    path_mono_A_eval=\"/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/Data_spellchecked/eval_lyrics_spellchecked.txt\",\n","    path_mono_B_eval=\"/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/Data_spellchecked/eval_trump_spellchecked.txt\",\n","    path_mono_C_eval=\"/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/Data_spellchecked/eval_shakespeare_spellchecked.txt\",\n","    generator_model_tag=\"facebook/bart-base\",\n","    discriminator_model_tag=\"distilbert/distilbert-base-cased\",\n","    pretrained_classifier_model=\"/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/TernaryClassifier/DistilBertCheckpoint/checkpoint-11635\",\n","    pretrained_classifier_eval=\"/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/TernaryClassifier/DistilBertCheckpoint/checkpoint-11635\",\n","    lambdas=\"10|1|1|1|1\",\n","    learning_rate=5e-5,\n","    max_sequence_length=64,\n","    save_base_folder=\"/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/Checkpoint_GAN_Ternaria_Tokenizer_Shakespeare/\",\n","    save_steps=1,\n","    use_cuda_if_available=1,\n","    label2id=label2id,           # Explicitly pass the label mapping\n","    style_token_list=style_token_list  # Explicitly pass the style tokens\n",")\n","\n","# Call the main function with the updated arguments\n","main(args)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["7e48a20504824d71bb401ae60a484308","a0449c608cf94799b4bccbb1d28361a8","8748ac01a949445ba211ca100f0aec48","0c334f7ee755488ba236f8847d83029f","bb37e27e395744c992389cce38d6ec21","08369c20a3cc441099d7b2303bf8fc88","41cd36f51f3f4a1a89e5c3f192022d44","e11656e3e29a4045b18cb5b022e7d2b9","0cd24dadd9f14aa8a48fd4165d43f0c9","8fc7a537008d48ff96a335f742e8187b","4473dc09c5584d0f988d8aeb1d745617","8af0fc12eda34f28bb37f76d29310e81","3cdde870fed04a9eb718cf743e18c3eb","5b21ba1d6c18420f8487dcf4313b4f60","2af7069fb5564411a63048cbb13f3dad","d6057f563cc34a8886b32db2efbac85a","d92d8a7b6e2e496cb90f4897355af61a","1abb6ab5267343b0b191e50f36d7bf9f","0c0edfb52d664cbe81bf3e984416b234","fd589c0bf9334f09a9c2eedf3dbfdf3f","49357b44a0b342e6a74d2748064c9a65","fc24533183c9472bb3cc77d3f4e25e29","b3b28275e403439bb9ece2d79d0e59a2","e314de7a913d4603ab52cc8ef5cf5278","d10196c8fa1b41bca72ed575a2d3d95e","592cfe053194438b807eb75690b8fe01","2038937726ab433396d55f426f30872d","7949ebec40b4470f8c0423aedde490f3","90b7774df5b94e57849e838d06466ce9","9a603c3bbf15477799a9afbc3d938a39","a989625f2485453195afca459f0320ee","d6b32bb39375429b9fda32f3febf18be","730a9bea417c48bea5aceb0d6f440d6f","5d7a5a494cee46d595ce66fcfc41f201","4d1d705c8b0441559955e8de64ea6ac1","93235b299df34936b7e7503403f39889","e8762840f5da438c8bb74fda123bb4c9","be9525441ead4200bec634035d45d2b9","6e569dbb77574220af3a5192a0a30f86","534bb33a969647b2ad1bdb806084b16f","4c3a2e42b2bc4ff7b21ce5a43af836c3","e4b8933fb777490cabb0e849c099172e","6e0f3f7423b144bc8b7c265fba071065","4287932e0f85461abc965d3e59e54c17","71b187c8ecf04c7a9141dd6881c7ddbd","9677f076f7854bfcb704f90bcb07b76b","367a9e57f5634d7c8fe0596925fdf556","3bc0359bfc6b4a738dc2345a1630dff7","309adf280bce499e84cfc2a794f7850f","d0a50859f3534eb48b892d889226c54c","0b8b5a941aca4ee483a3d1b8b470dc19","de994dd648ce47a49036f8517816fe75","c5bfe956d5964cd29a30ae5ad4ad9cec","4685715a48324bf99e6846c8c352bca4","ff44514ead0b499fbee7f01e0936627f","e9a1bcc735664825baeadcd9dba4b5f5","cafe46d9897f4936b6e138cf2223e82a","fa75f3dd97e8446abf54aeda1c54bf93","660eb586790b4dc18055c6444aae1777","86286308a9284515a369c0242a6841b7","593cfa6e711f40d094bab94f93d21c92","07c5d7446f244ba7b47cf726fc7643ae","bed43938106742d7b131ad8322275bc9","43ee672da0ef4f8f9346851d881452c7","fe57145df42d417881ff4fce81f9fc61","fc8c90002f9a4691a27480719a32624b","163acf8169a3486b8f3043d8dd2e9ccc","ea181a22fbf848db8b3471b372c65de1","d849c714d9534224b6f502896dfe7ada","7b4b8033079b43b8b19314951418632e","00709b59c86f495689d2d5648ea86150","cc63a64bbf164f81a91e34f1d75a280e","3aac4a241bd8436a9383010d50ecf2d4","e87a87827e354c70bdf016717989de5c","58f4be19f90c46fdbc643381e50e00a2","5c2af64e08f540139b93307b31207882","9520182cea464e9ba22b9cc5e28b93af","dd65d71ce7704e17bfa3691bc14ec512","c71e42b1cec24b8fb7a93c2a833e0220","a71612db976e4dd6843690763bac1176","b340284c37594bcb808f3e431d613005","9e47c0f20a9647ae86719960b25f7e5b","fd8ac3421ac346b49b5b4eeccb13f09a","9bba265b05d04a9c912e42fda69dfec8","fede6f1be06240908caa666b321ffa4c","6e4d55188dbe40e2970c25df38dad6df","7f1572aed4434217a1db3a7f44d3ac09","0d88075a34bb4cb2aa5ddf4bc526bd23","c2cd03bdfa264134b2ce8246eb920ab0","cd915d040f9344e885191d1e576d2265","4228ffe47b054d4c8d7298e3e6d3d079","d2ed7580d2c4482ea9a5af236a388d59","1ee6b0ee9b2e402eb53ece117c26a36f","09af7b181b9f4ff79da396d9e55e3337","65e8acd91de4407eb0270658e09efa50","92bc2438238542ffb8d950379c7f371a","0d4078db79564f0cac001ca57443ac86","63bf5ccce0aa4046b959d6b18136d2b5","131a27222e7a401dbc6308d1859ab24a","0a04c4bb30704ef9b685e912ce4f8226","9bd9d38c07e044cc87a39b0a285bcbcf","46049c93ac644a9eb23f91545900fca1","d3423c90fc034c9fa9e406e2ab116943","7e8c49cb90ad44028e43183a357a5be1","d3879680cc934a29942f16f4b4f8b920","7e6bd96c516e424f98f9c5c58ed0f111","27fad13a8f444916b5cbbfd51ee44953","0f83dda03c384c7f941dd58dc248c371","7829771857124c3f96b901c83b08ac90","0d22d60e098b49328a4d195bf4a1c3a2","6b61a64f327a4ca1b1f782265b460244","89244d430be8477c8c4874eb5e39e20a","ca20bc8956494066b15fa10c90d16b5f","d219438106f44fd9a527ea84aeecc9c4","f1315ffe031f49f08a7b7708d9ae1e9b","33dedd9568b44f97aacb26246caaddd1","c6509c3c2bee4d1aaef51ac17b663bcf","dc4174680b4541dcbd68959ff5e55cf9","45dd2f2203814577b20b011d604c01c0","1b6a2f782f2f44f6a4c7cae47a5edd99","744d15d727e74989b0e71db12bc820fb","583745c5e64c4a08bc6ed33ff673d61c","5fa16c749d53450d82fa37bb7c8f707b","356dfac33ab54915b76352cc43321a7c","4c883d2059074641b76a25f865e0874d","84743ac22379473abd1a20098f9326eb","1f5c945a7dd74a37ae80852b89aa4c64","8b987d9448584807be7ac11a308eb685","4b3e4dc3e6d14d14906f64fbf77576b2","469c174e2c5949319f8bb91ceac0ce25","fbeb052839624138bbd5ec9558829586","67afdc47ce3341e4a9aca864071f6e04","20f536b4421745cfb99e4d91db052a1b","c3776d21fe1e4d548136798a2b9a99db","ddb12c314d5d4ba08fa505ffc1139319","3d8c51ca316d4616bd73b10ee4594dc8","03606a585aca417789f15cd8eb22a83d","c858af8b16d341a3a02cb4453c44441e","97c0333536ae4bdfae5ba03970da66fc","8721561ba3a94b308f6f37b2d5c0a9bd","78e17b2285304afd9ef8f5610c16db57","ff01969a80cd4026ac86359ac64f2203","c1d4f6f5066141edbe65664598e69d38"]},"id":"iGzks1rHdQ1e","outputId":"d33d8fa8-1392-4236-fe79-b0ad6ca4a2eb","collapsed":true},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Loading dataset A from: /content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/Data_spellchecked/train_lyrics_spellchecked.txt\n","Loading dataset B from: /content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/Data_spellchecked/train_trump_spellchecked.txt\n","Loading dataset C from: /content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/Data_spellchecked/train_shakespeare_spellchecked.txt\n","\n","Arguments summary:\n","  epochs:\t30\n","  style_a:\tlyr\n","  style_b:\ttru\n","  style_c:\tsha\n","  path_mono_A:\t/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/Data_spellchecked/train_lyrics_spellchecked.txt\n","  path_mono_B:\t/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/Data_spellchecked/train_trump_spellchecked.txt\n","  path_mono_C:\t/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/Data_spellchecked/train_shakespeare_spellchecked.txt\n","  path_mono_A_eval:\t/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/Data_spellchecked/eval_lyrics_spellchecked.txt\n","  path_mono_B_eval:\t/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/Data_spellchecked/eval_trump_spellchecked.txt\n","  path_mono_C_eval:\t/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/Data_spellchecked/eval_shakespeare_spellchecked.txt\n","  generator_model_tag:\tfacebook/bart-base\n","  discriminator_model_tag:\tdistilbert/distilbert-base-cased\n","  batch_size:\t8\n","  max_samples_train:\tNone\n","  max_samples_eval:\tNone\n","  nonparal_same_size:\tFalse\n","  pretrained_classifier_model:\t/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/TernaryClassifier/DistilBertCheckpoint/checkpoint-11635\n","  pretrained_classifier_eval:\t/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/TernaryClassifier/DistilBertCheckpoint/checkpoint-11635\n","  from_pretrained:\tNone\n","  save_base_folder:\t/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/Checkpoint_GAN_Ternaria_Tokenizer_Shakespeare/\n","  save_steps:\t1\n","  lambdas:\t10|1|1|1|1\n","  learning_rate:\t5e-05\n","  max_sequence_length:\t64\n","  lr_scheduler_type:\tcosine_with_restarts\n","  warmup_ratio:\t0.0\n","  use_cuda_if_available:\t1\n","  label2id:\t{'tru': 0, 'lyr': 1, 'sha': 2}\n","  style_token_list:\t['[tru->lyr]', '[tru->sha]', '[lyr->tru]', '[lyr->sha]', '[sha->tru]', '[sha->lyr]']\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7e48a20504824d71bb401ae60a484308","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8af0fc12eda34f28bb37f76d29310e81","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b3b28275e403439bb9ece2d79d0e59a2","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5d7a5a494cee46d595ce66fcfc41f201","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"71b187c8ecf04c7a9141dd6881c7ddbd","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e9a1bcc735664825baeadcd9dba4b5f5","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/465 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"163acf8169a3486b8f3043d8dd2e9ccc","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dd65d71ce7704e17bfa3691bc14ec512","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Trump-specific tokens: 2902, Shakespeare-specific tokens: 3819, Lyrics-specific tokens: 3467\n","6721 new tokens added to G_AB & D_AB (Trump → Shakespeare and Lyrics)\n","3467 new tokens added to G_BA & D_BA (Shakespeare/Lyrics → Trump)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c2cd03bdfa264134b2ce8246eb920ab0","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Added 6 new tokens to the tokenizer.\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["New embedding size: 55760 tokens.\n","Added 6 new tokens to the tokenizer.\n","New embedding size: 53738 tokens.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0a04c4bb30704ef9b685e912ce4f8226","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/465 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6b61a64f327a4ca1b1f782265b460244","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/263M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["[INFO] Using fresh G_ab, G_ba, D_ab, D_ba\n","[INFO] Loaded pretrained classifier\n","Device: cuda\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"583745c5e64c4a08bc6ed33ff673d61c","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"20f536b4421745cfb99e4d91db052a1b","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","=== EPOCH 0 ===\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["lyr->tru: 100%|██████████| 717/717 [34:07<00:00,  2.86s/it]\n","lyr->sha: 100%|██████████| 717/717 [30:28<00:00,  2.55s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Start validation_AB_epoch...\n","self-BLEU A->B: 64.93104133142941\n","self-BLEU B->A: 73.06517641189879\n","self-BLEU avg: 68.9981088716641\n","self-ROUGE-1 A->B: 0.8808362740669065\n","self-ROUGE-1 B->A: 0.913437634426684\n","self-ROUGE-2 A->B: 0.7838126647245209\n","self-ROUGE-2 B->A: 0.8416429121626757\n","self-ROUGE-L A->B: 0.8808362740669065\n","self-ROUGE-L B->A: 0.9131877125272775\n","style accuracy: 0.2584745762711864\n","acc-BLEU: 47.42278324939137\n","g-acc-BLEU: 42.23062508904713\n","h-acc-BLEU: 37.606938151001\n","End validation_AB_epoch...\n","Start validation_AC_epoch...\n","self-BLEU A->B: 64.91705811545476\n","self-BLEU B->A: 62.312928876387694\n","self-BLEU avg: 63.614993495921226\n","self-ROUGE-1 A->B: 0.8805390398715041\n","self-ROUGE-1 B->A: 0.8527408730109001\n","self-ROUGE-2 A->B: 0.7834489081107644\n","self-ROUGE-2 B->A: 0.7301715995049621\n","self-ROUGE-L A->B: 0.8805390398715041\n","self-ROUGE-L B->A: 0.8525999286839092\n","style accuracy: 0.5784615384615385\n","acc-BLEU: 60.73057367103753\n","g-acc-BLEU: 60.66203673375248\n","h-acc-BLEU: 60.593576644333375\n","End validation_AC_epoch...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","=== EPOCH 1 ===\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["lyr->tru: 100%|██████████| 717/717 [34:46<00:00,  2.91s/it]\n","lyr->sha: 100%|██████████| 717/717 [30:49<00:00,  2.58s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Start validation_AB_epoch...\n","self-BLEU A->B: 64.20291823862179\n","self-BLEU B->A: 72.22680600071361\n","self-BLEU avg: 68.2148621196677\n","self-ROUGE-1 A->B: 0.8768353316483118\n","self-ROUGE-1 B->A: 0.8994493157790309\n","self-ROUGE-2 A->B: 0.7790843840747816\n","self-ROUGE-2 B->A: 0.8286015313717856\n","self-ROUGE-L A->B: 0.8768353316483118\n","self-ROUGE-L B->A: 0.8989919343128985\n","style accuracy: 0.23163841807909605\n","acc-BLEU: 45.68935196378865\n","g-acc-BLEU: 39.750701567247184\n","h-acc-BLEU: 34.58394986754691\n","End validation_AB_epoch...\n","Start validation_AC_epoch...\n","self-BLEU A->B: 64.20449003740025\n","self-BLEU B->A: 60.023843559392105\n","self-BLEU avg: 62.11416679839618\n","self-ROUGE-1 A->B: 0.8766799953910016\n","self-ROUGE-1 B->A: 0.8249698809340412\n","self-ROUGE-2 A->B: 0.7791711896303373\n","self-ROUGE-2 B->A: 0.7020592483057313\n","self-ROUGE-L A->B: 0.8766799953910016\n","self-ROUGE-L B->A: 0.82449197627009\n","style accuracy: 0.58\n","acc-BLEU: 60.057083399198085\n","g-acc-BLEU: 60.02184330980662\n","h-acc-BLEU: 59.98662339906054\n","End validation_AC_epoch...\n","\n","=== EPOCH 2 ===\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["lyr->tru: 100%|██████████| 717/717 [36:10<00:00,  3.03s/it]\n","lyr->sha: 100%|██████████| 717/717 [30:44<00:00,  2.57s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Start validation_AB_epoch...\n","self-BLEU A->B: 63.388996594143144\n","self-BLEU B->A: 73.14201819277297\n","self-BLEU avg: 68.26550739345805\n","self-ROUGE-1 A->B: 0.8667658984834571\n","self-ROUGE-1 B->A: 0.9018313009235644\n","self-ROUGE-2 A->B: 0.7683229728662295\n","self-ROUGE-2 B->A: 0.8324096139373304\n","self-ROUGE-L A->B: 0.8667658984834571\n","self-ROUGE-L B->A: 0.9014813077735833\n","style accuracy: 0.2175141242937853\n","acc-BLEU: 45.00845991141829\n","g-acc-BLEU: 38.53402660008288\n","h-acc-BLEU: 32.99093531400143\n","End validation_AB_epoch...\n","Start validation_AC_epoch...\n","self-BLEU A->B: 63.388996594143144\n","self-BLEU B->A: 59.99626785630341\n","self-BLEU avg: 61.69263222522328\n","self-ROUGE-1 A->B: 0.8667658984834571\n","self-ROUGE-1 B->A: 0.8263920903845088\n","self-ROUGE-2 A->B: 0.7683229728662295\n","self-ROUGE-2 B->A: 0.7046342340948994\n","self-ROUGE-L A->B: 0.8667658984834571\n","self-ROUGE-L B->A: 0.8255356017097146\n","style accuracy: 0.5092307692307693\n","acc-BLEU: 56.3078545741501\n","g-acc-BLEU: 56.04978730015075\n","h-acc-BLEU: 55.79290229143146\n","End validation_AC_epoch...\n","\n","=== EPOCH 3 ===\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["lyr->tru: 100%|██████████| 717/717 [35:33<00:00,  2.98s/it]\n","lyr->sha: 100%|██████████| 717/717 [30:22<00:00,  2.54s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Start validation_AB_epoch...\n","self-BLEU A->B: 63.7595669258787\n","self-BLEU B->A: 73.56509248995015\n","self-BLEU avg: 68.66232970791442\n","self-ROUGE-1 A->B: 0.8657451114059797\n","self-ROUGE-1 B->A: 0.90303311688328\n","self-ROUGE-2 A->B: 0.772438966921165\n","self-ROUGE-2 B->A: 0.8293485816536531\n","self-ROUGE-L A->B: 0.8657451114059797\n","self-ROUGE-L B->A: 0.9029875006073927\n","style accuracy: 0.22598870056497175\n","acc-BLEU: 45.6305998822058\n","g-acc-BLEU: 39.39151008587414\n","h-acc-BLEU: 34.00549310875656\n","End validation_AB_epoch...\n","Start validation_AC_epoch...\n","self-BLEU A->B: 63.7595669258787\n","self-BLEU B->A: 59.83287541247771\n","self-BLEU avg: 61.79622116917821\n","self-ROUGE-1 A->B: 0.8657451114059797\n","self-ROUGE-1 B->A: 0.8245652156372916\n","self-ROUGE-2 A->B: 0.772438966921165\n","self-ROUGE-2 B->A: 0.6983590657479993\n","self-ROUGE-L A->B: 0.8657451114059797\n","self-ROUGE-L B->A: 0.8240490348028052\n","style accuracy: 0.4938461538461538\n","acc-BLEU: 55.5904182768968\n","g-acc-BLEU: 55.242941763292194\n","h-acc-BLEU: 54.897636711648666\n","End validation_AC_epoch...\n","\n","=== EPOCH 4 ===\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["lyr->tru: 100%|██████████| 717/717 [34:52<00:00,  2.92s/it]\n","lyr->sha: 100%|██████████| 717/717 [30:06<00:00,  2.52s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Start validation_AB_epoch...\n","self-BLEU A->B: 63.77654079753442\n","self-BLEU B->A: 71.48947289585544\n","self-BLEU avg: 67.63300684669494\n","self-ROUGE-1 A->B: 0.8658476685533568\n","self-ROUGE-1 B->A: 0.8783331906842424\n","self-ROUGE-2 A->B: 0.773595100215155\n","self-ROUGE-2 B->A: 0.8027034998261824\n","self-ROUGE-L A->B: 0.8658476685533568\n","self-ROUGE-L B->A: 0.8783331906842424\n","style accuracy: 0.2401129943502825\n","acc-BLEU: 45.822153140861595\n","g-acc-BLEU: 40.298342138198535\n","h-acc-BLEU: 35.44042019096972\n","End validation_AB_epoch...\n","Start validation_AC_epoch...\n","self-BLEU A->B: 63.81123720857833\n","self-BLEU B->A: 58.76962782465006\n","self-BLEU avg: 61.290432516614196\n","self-ROUGE-1 A->B: 0.8659518352200234\n","self-ROUGE-1 B->A: 0.8077374736368409\n","self-ROUGE-2 A->B: 0.7736815626657478\n","self-ROUGE-2 B->A: 0.6784626849088652\n","self-ROUGE-L A->B: 0.8659518352200234\n","self-ROUGE-L B->A: 0.807308305104224\n","style accuracy: 0.46153846153846156\n","acc-BLEU: 53.722139335230175\n","g-acc-BLEU: 53.186362848708704\n","h-acc-BLEU: 52.65592922676963\n","End validation_AC_epoch...\n","\n","=== EPOCH 5 ===\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["lyr->tru: 100%|██████████| 717/717 [33:41<00:00,  2.82s/it]\n","lyr->sha: 100%|██████████| 717/717 [29:17<00:00,  2.45s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Start validation_AB_epoch...\n","self-BLEU A->B: 62.273003286321526\n","self-BLEU B->A: 69.70345004401403\n","self-BLEU avg: 65.98822666516779\n","self-ROUGE-1 A->B: 0.8538153218937865\n","self-ROUGE-1 B->A: 0.8665562277692369\n","self-ROUGE-2 A->B: 0.7562107951544557\n","self-ROUGE-2 B->A: 0.7878461246679523\n","self-ROUGE-L A->B: 0.8538153218937865\n","self-ROUGE-L B->A: 0.8663805014337166\n","style accuracy: 0.2627118644067797\n","acc-BLEU: 46.129706552922876\n","g-acc-BLEU: 41.636390400830145\n","h-acc-BLEU: 37.58075037462213\n","End validation_AB_epoch...\n","Start validation_AC_epoch...\n","self-BLEU A->B: 62.23582307142483\n","self-BLEU B->A: 55.46112965988315\n","self-BLEU avg: 58.848476365653994\n","self-ROUGE-1 A->B: 0.8534547449707095\n","self-ROUGE-1 B->A: 0.7860875307948123\n","self-ROUGE-2 A->B: 0.7555259865420154\n","self-ROUGE-2 B->A: 0.6515139084599213\n","self-ROUGE-L A->B: 0.8534547449707095\n","self-ROUGE-L B->A: 0.785589918135223\n","style accuracy: 0.5184615384615384\n","acc-BLEU: 55.34731510590392\n","g-acc-BLEU: 55.23646584698776\n","h-acc-BLEU: 55.12583809827964\n","End validation_AC_epoch...\n","\n","=== EPOCH 6 ===\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["lyr->tru: 100%|██████████| 717/717 [33:50<00:00,  2.83s/it]\n","lyr->sha: 100%|██████████| 717/717 [29:14<00:00,  2.45s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Start validation_AB_epoch...\n","self-BLEU A->B: 60.89116375625237\n","self-BLEU B->A: 69.40237417908423\n","self-BLEU avg: 65.1467689676683\n","self-ROUGE-1 A->B: 0.8465292653152178\n","self-ROUGE-1 B->A: 0.8616507351768528\n","self-ROUGE-2 A->B: 0.7446381007428208\n","self-ROUGE-2 B->A: 0.7773724578241972\n","self-ROUGE-L A->B: 0.8462167653152178\n","self-ROUGE-L B->A: 0.8616507351768528\n","style accuracy: 0.268361581920904\n","acc-BLEU: 45.991463579879344\n","g-acc-BLEU: 41.8125459368347\n","h-acc-BLEU: 38.01333644615948\n","End validation_AB_epoch...\n","Start validation_AC_epoch...\n","self-BLEU A->B: 60.76102885355554\n","self-BLEU B->A: 54.82513604355268\n","self-BLEU avg: 57.79308244855411\n","self-ROUGE-1 A->B: 0.8452113557283625\n","self-ROUGE-1 B->A: 0.7766593871498073\n","self-ROUGE-2 A->B: 0.7441910899590642\n","self-ROUGE-2 B->A: 0.6443132346213316\n","self-ROUGE-L A->B: 0.8448988557283623\n","self-ROUGE-L B->A: 0.7761590919904213\n","style accuracy: 0.5261538461538462\n","acc-BLEU: 55.204233531969365\n","g-acc-BLEU: 55.1434969977359\n","h-acc-BLEU: 55.08282678785736\n","End validation_AC_epoch...\n","\n","=== EPOCH 7 ===\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["lyr->tru: 100%|██████████| 717/717 [33:20<00:00,  2.79s/it]\n","lyr->sha: 100%|██████████| 717/717 [28:55<00:00,  2.42s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Start validation_AB_epoch...\n","self-BLEU A->B: 59.487004527276596\n","self-BLEU B->A: 63.308363355817484\n","self-BLEU avg: 61.397683941547044\n","self-ROUGE-1 A->B: 0.8340192116749222\n","self-ROUGE-1 B->A: 0.827277565018828\n","self-ROUGE-2 A->B: 0.7332054505212273\n","self-ROUGE-2 B->A: 0.7328247756154577\n","self-ROUGE-L A->B: 0.833769211674922\n","self-ROUGE-L B->A: 0.8271871327525954\n","style accuracy: 0.3107344632768362\n","acc-BLEU: 46.23556513461533\n","g-acc-BLEU: 43.67880076881398\n","h-acc-BLEU: 41.2634215763416\n","End validation_AB_epoch...\n","Start validation_AC_epoch...\n","self-BLEU A->B: 59.349586940475774\n","self-BLEU B->A: 52.767940962871414\n","self-BLEU avg: 56.0587639516736\n","self-ROUGE-1 A->B: 0.8332419073819928\n","self-ROUGE-1 B->A: 0.7563327975845695\n","self-ROUGE-2 A->B: 0.7323836695061898\n","self-ROUGE-2 B->A: 0.6204633333099094\n","self-ROUGE-L A->B: 0.8329919073819928\n","self-ROUGE-L B->A: 0.7561644474162194\n","style accuracy: 0.5307692307692308\n","acc-BLEU: 54.56784351429834\n","g-acc-BLEU: 54.5474720042127\n","h-acc-BLEU: 54.527107599683085\n","End validation_AC_epoch...\n","\n","=== EPOCH 8 ===\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["lyr->tru: 100%|██████████| 717/717 [33:10<00:00,  2.78s/it]\n","lyr->sha: 100%|██████████| 717/717 [29:07<00:00,  2.44s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Start validation_AB_epoch...\n","self-BLEU A->B: 56.64657617435231\n","self-BLEU B->A: 59.18641058342876\n","self-BLEU avg: 57.916493378890536\n","self-ROUGE-1 A->B: 0.8101477574015915\n","self-ROUGE-1 B->A: 0.804005676293448\n","self-ROUGE-2 A->B: 0.7038169291522702\n","self-ROUGE-2 B->A: 0.6983264388027259\n","self-ROUGE-L A->B: 0.8098636664925006\n","self-ROUGE-L B->A: 0.8033576966864909\n","style accuracy: 0.3418079096045198\n","acc-BLEU: 46.048642169671254\n","g-acc-BLEU: 44.493050618565796\n","h-acc-BLEU: 42.99000879455002\n","End validation_AB_epoch...\n","Start validation_AC_epoch...\n","self-BLEU A->B: 56.57416025523334\n","self-BLEU B->A: 49.90256188816078\n","self-BLEU avg: 53.23836107169706\n","self-ROUGE-1 A->B: 0.8106881891920233\n","self-ROUGE-1 B->A: 0.732040055880001\n","self-ROUGE-2 A->B: 0.7049707579623491\n","self-ROUGE-2 B->A: 0.5876448404460429\n","self-ROUGE-L A->B: 0.8104040982829325\n","self-ROUGE-L B->A: 0.7306785084845809\n","style accuracy: 0.5646153846153846\n","acc-BLEU: 54.849949766617755\n","g-acc-BLEU: 54.82626898922537\n","h-acc-BLEU: 54.80259793614305\n","End validation_AC_epoch...\n","\n","=== EPOCH 9 ===\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["lyr->tru: 100%|██████████| 717/717 [32:48<00:00,  2.75s/it]\n","lyr->sha: 100%|██████████| 717/717 [28:51<00:00,  2.42s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Start validation_AB_epoch...\n","self-BLEU A->B: 55.801143266818634\n","self-BLEU B->A: 55.94997450869129\n","self-BLEU avg: 55.87555888775496\n","self-ROUGE-1 A->B: 0.7980166065874006\n","self-ROUGE-1 B->A: 0.7869697671367597\n","self-ROUGE-2 A->B: 0.6945296146909861\n","self-ROUGE-2 B->A: 0.668982883580861\n","self-ROUGE-L A->B: 0.7977666065874007\n","self-ROUGE-L B->A: 0.7866665530676269\n","style accuracy: 0.3347457627118644\n","acc-BLEU: 44.675067579470706\n","g-acc-BLEU: 43.24824456187006\n","h-acc-BLEU: 41.866990652507205\n","End validation_AB_epoch...\n","Start validation_AC_epoch...\n","self-BLEU A->B: 55.691468794618366\n","self-BLEU B->A: 47.06792494568633\n","self-BLEU avg: 51.379696870152344\n","self-ROUGE-1 A->B: 0.7989148036418477\n","self-ROUGE-1 B->A: 0.7074038594162801\n","self-ROUGE-2 A->B: 0.6952006495739775\n","self-ROUGE-2 B->A: 0.5569347418475531\n","self-ROUGE-L A->B: 0.7986648036418477\n","self-ROUGE-L B->A: 0.7058393135026331\n","style accuracy: 0.563076923076923\n","acc-BLEU: 53.843694588922325\n","g-acc-BLEU: 53.787286250814326\n","h-acc-BLEU: 53.73093650889227\n","End validation_AC_epoch...\n","\n","=== EPOCH 10 ===\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["lyr->tru: 100%|██████████| 717/717 [33:11<00:00,  2.78s/it]\n","lyr->sha: 100%|██████████| 717/717 [28:39<00:00,  2.40s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Start validation_AB_epoch...\n","self-BLEU A->B: 54.202459840719165\n","self-BLEU B->A: 53.98718333589026\n","self-BLEU avg: 54.09482158830471\n","self-ROUGE-1 A->B: 0.7882462935586242\n","self-ROUGE-1 B->A: 0.7678225427495026\n","self-ROUGE-2 A->B: 0.6798726580378742\n","self-ROUGE-2 B->A: 0.6509900170626335\n","self-ROUGE-L A->B: 0.7882462935586242\n","self-ROUGE-L B->A: 0.7673988366888124\n","style accuracy: 0.3531073446327684\n","acc-BLEU: 44.70277802579078\n","g-acc-BLEU: 43.70500979227625\n","h-acc-BLEU: 42.72951131753333\n","End validation_AB_epoch...\n","Start validation_AC_epoch...\n","self-BLEU A->B: 54.37245549462007\n","self-BLEU B->A: 45.04364977234448\n","self-BLEU avg: 49.708052633482275\n","self-ROUGE-1 A->B: 0.7900392738626338\n","self-ROUGE-1 B->A: 0.6845739266676745\n","self-ROUGE-2 A->B: 0.6807542417319578\n","self-ROUGE-2 B->A: 0.5416260238085896\n","self-ROUGE-L A->B: 0.7900392738626338\n","self-ROUGE-L B->A: 0.6834405540331998\n","style accuracy: 0.5815384615384616\n","acc-BLEU: 53.930949393664214\n","g-acc-BLEU: 53.76536473841515\n","h-acc-BLEU: 53.60028798221511\n","End validation_AC_epoch...\n","\n","=== EPOCH 11 ===\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["lyr->tru: 100%|██████████| 717/717 [32:14<00:00,  2.70s/it]\n","lyr->sha: 100%|██████████| 717/717 [28:46<00:00,  2.41s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Start validation_AB_epoch...\n","self-BLEU A->B: 53.257764772383645\n","self-BLEU B->A: 49.13168144524062\n","self-BLEU avg: 51.194723108812134\n","self-ROUGE-1 A->B: 0.7783368118462143\n","self-ROUGE-1 B->A: 0.7375963508200611\n","self-ROUGE-2 A->B: 0.6682014234603938\n","self-ROUGE-2 B->A: 0.6146282761950931\n","self-ROUGE-L A->B: 0.7783368118462143\n","self-ROUGE-L B->A: 0.7373470296944512\n","style accuracy: 0.3912429378531073\n","acc-BLEU: 45.159508447061434\n","g-acc-BLEU: 44.7544119296277\n","h-acc-BLEU: 44.35294877795912\n","End validation_AB_epoch...\n","Start validation_AC_epoch...\n","self-BLEU A->B: 53.492653007729565\n","self-BLEU B->A: 41.60053132223724\n","self-BLEU avg: 47.5465921649834\n","self-ROUGE-1 A->B: 0.7809788031255137\n","self-ROUGE-1 B->A: 0.6551016422187295\n","self-ROUGE-2 A->B: 0.6699398048706502\n","self-ROUGE-2 B->A: 0.5068524082059308\n","self-ROUGE-L A->B: 0.7809788031255137\n","self-ROUGE-L B->A: 0.6532184437122971\n","style accuracy: 0.6215384615384615\n","acc-BLEU: 54.85021915941478\n","g-acc-BLEU: 54.36178413703919\n","h-acc-BLEU: 53.87769808237766\n","End validation_AC_epoch...\n","\n","=== EPOCH 12 ===\n"]},{"output_type":"stream","name":"stderr","text":["lyr->tru: 100%|██████████| 717/717 [32:12<00:00,  2.69s/it]\n","lyr->sha: 100%|██████████| 717/717 [28:41<00:00,  2.40s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Start validation_AB_epoch...\n","self-BLEU A->B: 51.53721373621668\n","self-BLEU B->A: 45.07136807296184\n","self-BLEU avg: 48.30429090458926\n","self-ROUGE-1 A->B: 0.7609685246713267\n","self-ROUGE-1 B->A: 0.7087103259630676\n","self-ROUGE-2 A->B: 0.6486815757561526\n","self-ROUGE-2 B->A: 0.5816883524634232\n","self-ROUGE-L A->B: 0.7609685246713267\n","self-ROUGE-L B->A: 0.7074422349163387\n","style accuracy: 0.4138418079096045\n","acc-BLEU: 44.844235847774854\n","g-acc-BLEU: 44.71055253264791\n","h-acc-BLEU: 44.57726723835402\n","End validation_AB_epoch...\n","Start validation_AC_epoch...\n","self-BLEU A->B: 51.66763140505816\n","self-BLEU B->A: 39.68807605507806\n","self-BLEU avg: 45.67785373006811\n","self-ROUGE-1 A->B: 0.7622191951180112\n","self-ROUGE-1 B->A: 0.6416506802060032\n","self-ROUGE-2 A->B: 0.6511115278506268\n","self-ROUGE-2 B->A: 0.4975191253649399\n","self-ROUGE-L A->B: 0.7622191951180112\n","self-ROUGE-L B->A: 0.6406678556353298\n","style accuracy: 0.6461538461538462\n","acc-BLEU: 55.14661917272636\n","g-acc-BLEU: 54.32763649537528\n","h-acc-BLEU: 53.520816048011355\n","End validation_AC_epoch...\n","\n","=== EPOCH 13 ===\n"]},{"output_type":"stream","name":"stderr","text":["lyr->tru: 100%|██████████| 717/717 [32:07<00:00,  2.69s/it]\n","lyr->sha: 100%|██████████| 717/717 [28:12<00:00,  2.36s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Start validation_AB_epoch...\n","self-BLEU A->B: 51.895760732727275\n","self-BLEU B->A: 42.486452217751875\n","self-BLEU avg: 47.19110647523958\n","self-ROUGE-1 A->B: 0.7660605819076018\n","self-ROUGE-1 B->A: 0.6841673507114346\n","self-ROUGE-2 A->B: 0.6564933912525324\n","self-ROUGE-2 B->A: 0.5567116988062926\n","self-ROUGE-L A->B: 0.7660605819076018\n","self-ROUGE-L B->A: 0.6826296698911819\n","style accuracy: 0.4519774011299435\n","acc-BLEU: 46.194423294116966\n","g-acc-BLEU: 46.183669907365775\n","h-acc-BLEU: 46.17291852407866\n","End validation_AB_epoch...\n","Start validation_AC_epoch...\n","self-BLEU A->B: 51.515452400859104\n","self-BLEU B->A: 38.01890979012856\n","self-BLEU avg: 44.76718109549383\n","self-ROUGE-1 A->B: 0.7621768352756935\n","self-ROUGE-1 B->A: 0.6206328750891428\n","self-ROUGE-2 A->B: 0.6517093835935247\n","self-ROUGE-2 B->A: 0.47200868566230453\n","self-ROUGE-L A->B: 0.7621768352756935\n","self-ROUGE-L B->A: 0.6184743918857792\n","style accuracy: 0.6538461538461539\n","acc-BLEU: 55.07589824005461\n","g-acc-BLEU: 54.10254077011808\n","h-acc-BLEU: 53.14638498406439\n","End validation_AC_epoch...\n","\n","=== EPOCH 14 ===\n"]},{"output_type":"stream","name":"stderr","text":["lyr->tru: 100%|██████████| 717/717 [31:46<00:00,  2.66s/it]\n","lyr->sha: 100%|██████████| 717/717 [28:38<00:00,  2.40s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Start validation_AB_epoch...\n","self-BLEU A->B: 51.62334217387438\n","self-BLEU B->A: 40.20314277747333\n","self-BLEU avg: 45.913242475673854\n","self-ROUGE-1 A->B: 0.7585827370364803\n","self-ROUGE-1 B->A: 0.6559488661755737\n","self-ROUGE-2 A->B: 0.6484432806103023\n","self-ROUGE-2 B->A: 0.5251767060280509\n","self-ROUGE-L A->B: 0.7585827370364803\n","self-ROUGE-L B->A: 0.6544677115280979\n","style accuracy: 0.4533898305084746\n","acc-BLEU: 45.62611276326066\n","g-acc-BLEU: 45.625209286249046\n","h-acc-BLEU: 45.624305327147674\n","End validation_AB_epoch...\n","Start validation_AC_epoch...\n","self-BLEU A->B: 51.4675031007114\n","self-BLEU B->A: 36.2543149998738\n","self-BLEU avg: 43.8609090502926\n","self-ROUGE-1 A->B: 0.7578462757853132\n","self-ROUGE-1 B->A: 0.599020347449596\n","self-ROUGE-2 A->B: 0.6479745406514308\n","self-ROUGE-2 B->A: 0.45112826941849554\n","self-ROUGE-L A->B: 0.7578462757853132\n","self-ROUGE-L B->A: 0.5974876274505244\n","style accuracy: 0.6323076923076923\n","acc-BLEU: 53.545839140530916\n","g-acc-BLEU: 52.6626909529964\n","h-acc-BLEU: 51.79410831969747\n","End validation_AC_epoch...\n","\n","=== EPOCH 15 ===\n"]},{"output_type":"stream","name":"stderr","text":["lyr->tru: 100%|██████████| 717/717 [32:00<00:00,  2.68s/it]\n","lyr->sha: 100%|██████████| 717/717 [28:02<00:00,  2.35s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Start validation_AB_epoch...\n","self-BLEU A->B: 49.94699255884448\n","self-BLEU B->A: 39.27710878826423\n","self-BLEU avg: 44.61205067355435\n","self-ROUGE-1 A->B: 0.7395953795929735\n","self-ROUGE-1 B->A: 0.6404428680487769\n","self-ROUGE-2 A->B: 0.6301152641003647\n","self-ROUGE-2 B->A: 0.518489490115291\n","self-ROUGE-L A->B: 0.7393112886838826\n","self-ROUGE-L B->A: 0.6393259309087492\n","style accuracy: 0.4788135593220339\n","acc-BLEU: 46.24670330287887\n","g-acc-BLEU: 46.21780476359678\n","h-acc-BLEU: 46.1889237829957\n","End validation_AB_epoch...\n","Start validation_AC_epoch...\n","self-BLEU A->B: 49.93415026528061\n","self-BLEU B->A: 35.091376388175625\n","self-BLEU avg: 42.51276332672812\n","self-ROUGE-1 A->B: 0.7374681214774221\n","self-ROUGE-1 B->A: 0.5834927452791251\n","self-ROUGE-2 A->B: 0.6270048032785068\n","self-ROUGE-2 B->A: 0.44329559088693615\n","self-ROUGE-L A->B: 0.7371840305683313\n","self-ROUGE-L B->A: 0.5821257634974354\n","style accuracy: 0.6615384615384615\n","acc-BLEU: 54.33330474028713\n","g-acc-BLEU: 53.03190364951313\n","h-acc-BLEU: 51.761673475478446\n","End validation_AC_epoch...\n","\n","=== EPOCH 16 ===\n"]},{"output_type":"stream","name":"stderr","text":["lyr->tru: 100%|██████████| 717/717 [32:02<00:00,  2.68s/it]\n","lyr->sha: 100%|██████████| 717/717 [28:15<00:00,  2.36s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Start validation_AB_epoch...\n","self-BLEU A->B: 49.48722348768943\n","self-BLEU B->A: 39.08454501172929\n","self-BLEU avg: 44.285884249709355\n","self-ROUGE-1 A->B: 0.7317703241341151\n","self-ROUGE-1 B->A: 0.6327459520405515\n","self-ROUGE-2 A->B: 0.6217353364244693\n","self-ROUGE-2 B->A: 0.509988406818698\n","self-ROUGE-L A->B: 0.7314862332250243\n","self-ROUGE-L B->A: 0.6307262875901128\n","style accuracy: 0.481638418079096\n","acc-BLEU: 46.22486302880948\n","g-acc-BLEU: 46.18417827921589\n","h-acc-BLEU: 46.14352883912845\n","End validation_AB_epoch...\n","Start validation_AC_epoch...\n","self-BLEU A->B: 49.41071178263177\n","self-BLEU B->A: 33.367357382817566\n","self-BLEU avg: 41.38903458272467\n","self-ROUGE-1 A->B: 0.7298287193907778\n","self-ROUGE-1 B->A: 0.5720773254795711\n","self-ROUGE-2 A->B: 0.6197731053939166\n","self-ROUGE-2 B->A: 0.4290076538572197\n","self-ROUGE-L A->B: 0.72912796181502\n","self-ROUGE-L B->A: 0.570228517358077\n","style accuracy: 0.6553846153846153\n","acc-BLEU: 53.463748060593105\n","g-acc-BLEU: 52.08237370852019\n","h-acc-BLEU: 50.73669026479992\n","End validation_AC_epoch...\n","\n","=== EPOCH 17 ===\n"]},{"output_type":"stream","name":"stderr","text":["lyr->tru: 100%|██████████| 717/717 [32:12<00:00,  2.69s/it]\n","lyr->sha: 100%|██████████| 717/717 [27:57<00:00,  2.34s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Start validation_AB_epoch...\n","self-BLEU A->B: 45.85371684532421\n","self-BLEU B->A: 36.394200205375675\n","self-BLEU avg: 41.12395852534994\n","self-ROUGE-1 A->B: 0.6959385156141544\n","self-ROUGE-1 B->A: 0.6125547616238232\n","self-ROUGE-2 A->B: 0.5845227830121964\n","self-ROUGE-2 B->A: 0.4892597864064899\n","self-ROUGE-L A->B: 0.6950010156141545\n","self-ROUGE-L B->A: 0.6108876764915218\n","style accuracy: 0.501412429378531\n","acc-BLEU: 45.63260073160152\n","g-acc-BLEU: 45.40932057392806\n","h-acc-BLEU: 45.18713243017548\n","End validation_AB_epoch...\n","Start validation_AC_epoch...\n","self-BLEU A->B: 46.03385376729942\n","self-BLEU B->A: 31.535978040881496\n","self-BLEU avg: 38.784915904090454\n","self-ROUGE-1 A->B: 0.6966615060292405\n","self-ROUGE-1 B->A: 0.5351015128207824\n","self-ROUGE-2 A->B: 0.585296134660548\n","self-ROUGE-2 B->A: 0.3933515499774254\n","self-ROUGE-L A->B: 0.6962448393625738\n","self-ROUGE-L B->A: 0.5335259979349227\n","style accuracy: 0.6923076923076923\n","acc-BLEU: 54.00784256742984\n","g-acc-BLEU: 51.81804282864104\n","h-acc-BLEU: 49.71703015872095\n","End validation_AC_epoch...\n","\n","=== EPOCH 18 ===\n"]},{"output_type":"stream","name":"stderr","text":["lyr->tru: 100%|██████████| 717/717 [31:54<00:00,  2.67s/it]\n","lyr->sha: 100%|██████████| 717/717 [27:50<00:00,  2.33s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Start validation_AB_epoch...\n","self-BLEU A->B: 46.524100852401055\n","self-BLEU B->A: 34.74837965870938\n","self-BLEU avg: 40.63624025555522\n","self-ROUGE-1 A->B: 0.7066790360409534\n","self-ROUGE-1 B->A: 0.5825547994554414\n","self-ROUGE-2 A->B: 0.5919129607622422\n","self-ROUGE-2 B->A: 0.4621827011786293\n","self-ROUGE-L A->B: 0.7066790360409534\n","self-ROUGE-L B->A: 0.5802755065147185\n","style accuracy: 0.5112994350282486\n","acc-BLEU: 45.88309187919004\n","g-acc-BLEU: 45.58210908277233\n","h-acc-BLEU: 45.2831001725615\n","End validation_AB_epoch...\n","Start validation_AC_epoch...\n","self-BLEU A->B: 46.253358574027175\n","self-BLEU B->A: 28.552025076432432\n","self-BLEU avg: 37.4026918252298\n","self-ROUGE-1 A->B: 0.7056750438762123\n","self-ROUGE-1 B->A: 0.508858313749714\n","self-ROUGE-2 A->B: 0.5891593688680252\n","self-ROUGE-2 B->A: 0.36840383643065105\n","self-ROUGE-L A->B: 0.7056750438762123\n","self-ROUGE-L B->A: 0.5070490709948987\n","style accuracy: 0.7246153846153847\n","acc-BLEU: 54.93211514338414\n","g-acc-BLEU: 52.060124781438624\n","h-acc-BLEU: 49.338288913789086\n","End validation_AC_epoch...\n","\n","=== EPOCH 19 ===\n"]},{"output_type":"stream","name":"stderr","text":["lyr->tru: 100%|██████████| 717/717 [33:10<00:00,  2.78s/it]\n","lyr->sha: 100%|██████████| 717/717 [27:49<00:00,  2.33s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Start validation_AB_epoch...\n","self-BLEU A->B: 45.67524025886218\n","self-BLEU B->A: 30.60878462176866\n","self-BLEU avg: 38.14201244031542\n","self-ROUGE-1 A->B: 0.6924370051143107\n","self-ROUGE-1 B->A: 0.5422084177798391\n","self-ROUGE-2 A->B: 0.577489839007458\n","self-ROUGE-2 B->A: 0.4208850338847633\n","self-ROUGE-L A->B: 0.6920269522042577\n","self-ROUGE-L B->A: 0.5395418417693674\n","style accuracy: 0.5268361581920904\n","acc-BLEU: 45.412814129762225\n","g-acc-BLEU: 44.826991087703725\n","h-acc-BLEU: 44.24872464654787\n","End validation_AB_epoch...\n","Start validation_AC_epoch...\n","self-BLEU A->B: 45.469933966806025\n","self-BLEU B->A: 26.750230685897296\n","self-BLEU avg: 36.11008232635166\n","self-ROUGE-1 A->B: 0.6898545360445536\n","self-ROUGE-1 B->A: 0.48198044753177477\n","self-ROUGE-2 A->B: 0.5750995265786761\n","self-ROUGE-2 B->A: 0.3439962163817303\n","self-ROUGE-L A->B: 0.6891603922254098\n","self-ROUGE-L B->A: 0.4796274739999395\n","style accuracy: 0.7353846153846154\n","acc-BLEU: 54.8242719324066\n","g-acc-BLEU: 51.531348714225324\n","h-acc-BLEU: 48.43620868076379\n","End validation_AC_epoch...\n","\n","=== EPOCH 20 ===\n"]},{"output_type":"stream","name":"stderr","text":["lyr->tru: 100%|██████████| 717/717 [32:48<00:00,  2.74s/it]\n","lyr->sha: 100%|██████████| 717/717 [27:55<00:00,  2.34s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Start validation_AB_epoch...\n","self-BLEU A->B: 43.87545225177326\n","self-BLEU B->A: 31.5057795453828\n","self-BLEU avg: 37.69061589857803\n","self-ROUGE-1 A->B: 0.6782597067827657\n","self-ROUGE-1 B->A: 0.5467329543324085\n","self-ROUGE-2 A->B: 0.5620225444766361\n","self-ROUGE-2 B->A: 0.42571359609979714\n","self-ROUGE-L A->B: 0.6774753930572756\n","self-ROUGE-L B->A: 0.5450551021639336\n","style accuracy: 0.5127118644067796\n","acc-BLEU: 44.480901169628\n","g-acc-BLEU: 43.95955635353905\n","h-acc-BLEU: 43.44432154619443\n","End validation_AB_epoch...\n","Start validation_AC_epoch...\n","self-BLEU A->B: 43.78113105225062\n","self-BLEU B->A: 25.069527556540706\n","self-BLEU avg: 34.42532930439566\n","self-ROUGE-1 A->B: 0.6752468491481977\n","self-ROUGE-1 B->A: 0.47090039839504055\n","self-ROUGE-2 A->B: 0.5579348719797144\n","self-ROUGE-2 B->A: 0.3273575021594348\n","self-ROUGE-L A->B: 0.6744625354227074\n","self-ROUGE-L B->A: 0.4680044250158782\n","style accuracy: 0.7353846153846154\n","acc-BLEU: 53.981895421428604\n","g-acc-BLEU: 50.314866143120895\n","h-acc-BLEU: 46.8969403869203\n","End validation_AC_epoch...\n","\n","=== EPOCH 21 ===\n"]},{"output_type":"stream","name":"stderr","text":["lyr->tru: 100%|██████████| 717/717 [32:58<00:00,  2.76s/it]\n","lyr->sha: 100%|██████████| 717/717 [28:05<00:00,  2.35s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Start validation_AB_epoch...\n","self-BLEU A->B: 43.19261411428304\n","self-BLEU B->A: 30.1105946477635\n","self-BLEU avg: 36.65160438102327\n","self-ROUGE-1 A->B: 0.6699032494481625\n","self-ROUGE-1 B->A: 0.5277535405145598\n","self-ROUGE-2 A->B: 0.5567585895311838\n","self-ROUGE-2 B->A: 0.40645347075485033\n","self-ROUGE-L A->B: 0.6699032494481625\n","self-ROUGE-L B->A: 0.5258545582326073\n","style accuracy: 0.5324858757062146\n","acc-BLEU: 44.950095975822364\n","g-acc-BLEU: 44.17743955331376\n","h-acc-BLEU: 43.41806399762539\n","End validation_AB_epoch...\n","Start validation_AC_epoch...\n","self-BLEU A->B: 43.1869082249939\n","self-BLEU B->A: 23.60241537258346\n","self-BLEU avg: 33.39466179878868\n","self-ROUGE-1 A->B: 0.6677739897267718\n","self-ROUGE-1 B->A: 0.4451393421540703\n","self-ROUGE-2 A->B: 0.5525721798680866\n","self-ROUGE-2 B->A: 0.3084987421401981\n","self-ROUGE-L A->B: 0.6675425082452904\n","self-ROUGE-L B->A: 0.4438409211639043\n","style accuracy: 0.7292307692307692\n","acc-BLEU: 53.1588693609328\n","g-acc-BLEU: 49.34816603657329\n","h-acc-BLEU: 45.810633249803125\n","End validation_AC_epoch...\n","\n","=== EPOCH 22 ===\n"]},{"output_type":"stream","name":"stderr","text":["lyr->tru:  53%|█████▎    | 377/717 [17:32<15:03,  2.66s/it]"]}]}]}