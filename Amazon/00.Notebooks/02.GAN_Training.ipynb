{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3TYfXztbwo0",
        "outputId": "d3e171ae-f2d5-457a-867a-828d9be1d645",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is CUDA available: True\n",
            "CUDA version: 12.4\n",
            "PyTorch version: 2.5.1+cu124\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (2024.10.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers) (0.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.12.14)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Collecting bert-score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.5.1+cu124)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.2)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.47.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert-score) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2024.12.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n",
            "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bert-score\n",
            "Successfully installed bert-score-0.3.13\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=e304bd36b1c912f92250265641738c2b6818f7f85f97e872269ecd27542efd6b\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.0)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-3.1.1 sacrebleu-2.5.1\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting datasets>=2.0.0 (from evaluate)\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Collecting xxhash (from evaluate)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.10.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.11)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, evaluate\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (0.9.0)\n"
          ]
        }
      ],
      "source": [
        "# @title Start installing and importing the required libraries\n",
        "\n",
        "# Ensure the Google Colab runtime is using GPU\n",
        "import torch\n",
        "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "\n",
        "#Install libraries\n",
        "!pip install fsspec\n",
        "!pip install torch\n",
        "!pip install scikit-learn\n",
        "!pip install tokenizers\n",
        "!pip install transformers\n",
        "!pip install bert-score\n",
        "!pip install rouge-score\n",
        "!pip install sacrebleu\n",
        "!pip install evaluate\n",
        "!pip install tabulate\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_uDJ2Ilchzf"
      },
      "outputs": [],
      "source": [
        "# @title Reorganized imports\n",
        "import argparse\n",
        "import csv\n",
        "import logging\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "from typing import List, Optional, Tuple, Union, Dict\n",
        "\n",
        "# Third-party libraries\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import rouge_score\n",
        "import sacrebleu\n",
        "import sklearn\n",
        "import tokenizers\n",
        "import transformers\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from torch import nn, Tensor\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "from transformers import (\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    DebertaForSequenceClassification,\n",
        "    DebertaTokenizer,\n",
        "    get_scheduler\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCxKiR5gcrY8",
        "outputId": "6808d363-64a4-4f42-8948-08e2ab4e43cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# @title Mount Google Drive on Colab for persistent storage\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class Definition"
      ],
      "metadata": {
        "id": "Z2NLsgDdZkDL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycJm88zSc0eL"
      },
      "outputs": [],
      "source": [
        "# @title MonostyleDataset Class, it is used to represents datasets\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "class MonostyleDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Mono-style dataset:\n",
        "    Loads textual data from CSV files, line-based files, or a provided list of sentences.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset_format: str,\n",
        "        dataset_path: str = None,\n",
        "        sentences_list: List[str] = None,\n",
        "        text_column_name: str = None,\n",
        "        separator: str = None,\n",
        "        style: str = None,\n",
        "        max_dataset_samples: int = None,\n",
        "        SEED: int = 42\n",
        "    ):\n",
        "        super(MonostyleDataset, self).__init__()\n",
        "\n",
        "        self.allowed_dataset_formats = [\"list\", \"csv\", \"line_file\"]\n",
        "        if dataset_format not in self.allowed_dataset_formats:\n",
        "            raise Exception(\n",
        "                f\"MonostyleDataset: '{dataset_format}' is not supported. \"\n",
        "                f\"Allowed formats: {self.allowed_dataset_formats}.\"\n",
        "            )\n",
        "\n",
        "        self.dataset_format = dataset_format\n",
        "        self.dataset_path = dataset_path\n",
        "        self.sentences_list = sentences_list\n",
        "        self.text_column_name = text_column_name\n",
        "        self.separator = separator\n",
        "        self.style = style\n",
        "        self.max_dataset_samples = max_dataset_samples\n",
        "\n",
        "        # Load data based on the format\n",
        "        self.load_data(SEED)\n",
        "\n",
        "    def _load_data_csv(self):\n",
        "        try:\n",
        "            df = pd.read_csv(self.dataset_path, sep=self.separator, header=None, encoding='utf-8')\n",
        "            df.dropna(inplace=True)\n",
        "            if self.text_column_name is not None:\n",
        "                self.data = df[self.text_column_name].tolist()\n",
        "            else:\n",
        "                self.data = df.iloc[:, 0].tolist()\n",
        "            logging.debug(\n",
        "                f\"MonostyleDataset, _load_data_csv: parsed {len(self.data)} examples from '{self.dataset_path}'.\"\n",
        "            )\n",
        "        except UnicodeDecodeError as e:\n",
        "            logging.error(\n",
        "                f\"MonostyleDataset, _load_data_csv: UnicodeDecodeError while reading '{self.dataset_path}': {e}\"\n",
        "            )\n",
        "            raise\n",
        "        except FileNotFoundError:\n",
        "            logging.error(\n",
        "                f\"MonostyleDataset, _load_data_csv: File not found: '{self.dataset_path}'.\"\n",
        "            )\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            logging.error(\n",
        "                f\"MonostyleDataset, _load_data_csv: Error loading CSV dataset: {e}\"\n",
        "            )\n",
        "            raise\n",
        "\n",
        "    def _load_data_line_file(self):\n",
        "        try:\n",
        "            with open(self.dataset_path, 'r', encoding='utf-8') as f:\n",
        "                self.data = f.read().split(self.separator)\n",
        "            logging.debug(\n",
        "                f\"MonostyleDataset, _load_data_line_file: parsed {len(self.data)} examples from '{self.dataset_path}'.\"\n",
        "            )\n",
        "        except UnicodeDecodeError as e:\n",
        "            logging.error(\n",
        "                f\"MonostyleDataset, _load_data_line_file: UnicodeDecodeError while reading '{self.dataset_path}': {e}\"\n",
        "            )\n",
        "            raise\n",
        "        except FileNotFoundError:\n",
        "            logging.error(\n",
        "                f\"MonostyleDataset, _load_data_line_file: File not found: '{self.dataset_path}'.\"\n",
        "            )\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            logging.error(\n",
        "                f\"MonostyleDataset, _load_data_line_file: Error loading line_file dataset: {e}\"\n",
        "            )\n",
        "            raise\n",
        "\n",
        "    def load_data(self, SEED=42):\n",
        "        if self.dataset_format == \"csv\":\n",
        "            self._load_data_csv()\n",
        "        elif self.dataset_format == \"line_file\":\n",
        "            self._load_data_line_file()\n",
        "        elif self.dataset_format == \"list\":\n",
        "            if self.sentences_list is None:\n",
        "                raise Exception(\n",
        "                    \"MonostyleDataset: 'list' format specified but 'sentences_list' is None.\"\n",
        "                )\n",
        "            self.data = self.sentences_list\n",
        "            logging.debug(\n",
        "                f\"MonostyleDataset, load_data: data already loaded, {len(self.data)} examples.\"\n",
        "            )\n",
        "        else:\n",
        "            raise Exception(\n",
        "                f\"MonostyleDataset, load_data: '{self.dataset_format}' format is not supported.\"\n",
        "            )\n",
        "\n",
        "        # Limit the number of samples if needed\n",
        "        if self.max_dataset_samples is not None and self.max_dataset_samples < len(self.data):\n",
        "            random.seed(SEED)\n",
        "            ix = random.sample(range(len(self.data)), self.max_dataset_samples)\n",
        "            self.data = [self.data[i] for i in ix]\n",
        "            logging.debug(f\"MonostyleDataset, load_data: reduced data to {len(self.data)} samples.\")\n",
        "\n",
        "        # Shuffle the data\n",
        "        random.shuffle(self.data)\n",
        "        logging.debug(\"MonostyleDataset, load_data: data has been shuffled.\")\n",
        "\n",
        "    def reduce_data(self, n_samples):\n",
        "        if n_samples < len(self.data):\n",
        "            self.data = self.data[:n_samples]\n",
        "            logging.debug(f\"MonostyleDataset, reduce_data: reduced data to {n_samples} samples.\")\n",
        "        else:\n",
        "            logging.debug(\n",
        "                f\"MonostyleDataset, reduce_data: requested {n_samples}, but dataset has {len(self.data)}.\"\n",
        "            )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmQaZLhAdKWg"
      },
      "outputs": [],
      "source": [
        "# @title GeneratorModel Class, it represents the Generator of the GAN\n",
        "class GeneratorModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name_or_path: str,\n",
        "        new_style_tokens: List[str] = None,\n",
        "        pretrained_path: str = None,\n",
        "        max_seq_length: int = 64,\n",
        "        truncation: str = \"longest_first\",\n",
        "        padding: str = \"max_length\",\n",
        "    ):\n",
        "        super(GeneratorModel, self).__init__()\n",
        "\n",
        "        self.model_name_or_path = model_name_or_path\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.truncation = truncation\n",
        "        self.padding = padding\n",
        "\n",
        "        # If no style tokens are provided, use default ones\n",
        "        if new_style_tokens is None:\n",
        "            new_style_tokens = [\n",
        "                '[pos->neu]', '[pos->neg]',\n",
        "                '[neu->pos]', '[neg->pos]',\n",
        "                '[neu->neg]', '[neg->neu]'\n",
        "            ]\n",
        "\n",
        "        if pretrained_path is None:\n",
        "            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "        else:\n",
        "            self.model = AutoModelForSeq2SeqLM.from_pretrained(pretrained_path)\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(f\"{pretrained_path}tokenizer/\")\n",
        "\n",
        "        num_added_tokens = self.tokenizer.add_tokens(new_style_tokens)\n",
        "        print(f\"Added {num_added_tokens} new tokens to the tokenizer.\")\n",
        "\n",
        "        # Resizing embeddings to include the new tokens\n",
        "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "        print(f\"New embedding size: {len(self.tokenizer)} tokens.\")\n",
        "\n",
        "    def train(self):\n",
        "        # Setting the model in training mode\n",
        "        self.model.train()\n",
        "\n",
        "    def eval(self):\n",
        "        # Setting the model in evaluation mode\n",
        "        self.model.eval()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        sentences: List[str],\n",
        "        target_sentences: List[str] = None,\n",
        "        device=None,\n",
        "    ):\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            sentences,\n",
        "            truncation=self.truncation,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_seq_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        if target_sentences is not None:\n",
        "            target = self.tokenizer(\n",
        "                target_sentences,\n",
        "                truncation=self.truncation,\n",
        "                padding=self.padding,\n",
        "                max_length=self.max_seq_length,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            labels = target[\"input_ids\"]\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            output_supervised = self.model(**inputs, labels=labels)\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        output = self.model.generate(**inputs, max_length=self.max_seq_length)\n",
        "        transferred_sentences = self.tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "\n",
        "        if target_sentences is not None:\n",
        "            return output, transferred_sentences, output_supervised.loss\n",
        "        else:\n",
        "            return output, transferred_sentences\n",
        "\n",
        "    def transfer(\n",
        "        self,\n",
        "        sentences: List[str],\n",
        "        device=None\n",
        "    ):\n",
        "        inputs = self.tokenizer(\n",
        "            sentences,\n",
        "            truncation=self.truncation,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_seq_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        output = self.model.generate(**inputs, max_length=self.max_seq_length)\n",
        "        transferred_sentences = self.tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "        return transferred_sentences\n",
        "\n",
        "    def save_model(\n",
        "        self,\n",
        "        path: Union[str]\n",
        "    ):\n",
        "        self.model.save_pretrained(path)\n",
        "        self.tokenizer.save_pretrained(f\"{path}/tokenizer/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xQjnOzndmSu"
      },
      "outputs": [],
      "source": [
        "# @title DiscriminatorModel Class, it represents the discriminator\n",
        "class DiscriminatorModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name_or_path: str,\n",
        "        pretrained_path: str = None,\n",
        "        max_seq_length: int = 64,\n",
        "        truncation: str = \"longest_first\",\n",
        "        padding: str = \"max_length\",\n",
        "    ):\n",
        "        super(DiscriminatorModel, self).__init__()\n",
        "\n",
        "        self.model_name_or_path = model_name_or_path\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.truncation = truncation\n",
        "        self.padding = padding\n",
        "\n",
        "        if pretrained_path is None:\n",
        "            self.model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path)\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "        else:\n",
        "            self.model = AutoModelForSequenceClassification.from_pretrained(pretrained_path)\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(f\"{pretrained_path}tokenizer/\")\n",
        "\n",
        "    def train(self):\n",
        "        # Set the model in training mode\n",
        "        self.model.train()\n",
        "\n",
        "    def eval(self):\n",
        "        # Set the model in evaluation mode\n",
        "        self.model.eval()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        sentences: List[str],\n",
        "        target_labels: Tensor,\n",
        "        return_hidden: bool = False,\n",
        "        device=None,\n",
        "    ):\n",
        "        inputs = self.tokenizer(\n",
        "            sentences,\n",
        "            truncation=self.truncation,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_seq_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        inputs[\"labels\"] = target_labels\n",
        "        inputs = inputs.to(device)\n",
        "        output = self.model(**inputs, output_hidden_states=return_hidden)\n",
        "        return output, output.loss\n",
        "\n",
        "    def save_model(\n",
        "        self,\n",
        "        path: Union[str]\n",
        "    ):\n",
        "        self.model.save_pretrained(path)\n",
        "        self.tokenizer.save_pretrained(f\"{path}/tokenizer\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrFuCkRPd2Im"
      },
      "outputs": [],
      "source": [
        "# @title ClassifierModel Class, it represents the classifier\n",
        "class ClassifierModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        pretrained_path: str = None,\n",
        "        max_seq_length: int = 64,\n",
        "        truncation: str = \"longest_first\",\n",
        "        padding: str = \"max_length\",\n",
        "    ):\n",
        "        super(ClassifierModel, self).__init__()\n",
        "\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.truncation = truncation\n",
        "        self.padding = padding\n",
        "\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(pretrained_path)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_path)\n",
        "        self.model.eval()\n",
        "\n",
        "    def eval(self):\n",
        "        # Set the model in evaluation mode\n",
        "        self.model.eval()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        sentences: List[str],\n",
        "        target_labels: Tensor,\n",
        "        return_hidden: bool = False,\n",
        "        device=None,\n",
        "    ):\n",
        "        inputs = self.tokenizer(\n",
        "            sentences,\n",
        "            truncation=self.truncation,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_seq_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        inputs[\"labels\"] = target_labels\n",
        "        inputs = inputs.to(device)\n",
        "        output = self.model(**inputs, output_hidden_states=return_hidden)\n",
        "        return output, output.loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrbv203zeGGI"
      },
      "outputs": [],
      "source": [
        "# @title CycleGANModel Class, it represents the gan\n",
        "class CycleGANModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        G_ab: Union['GeneratorModel', None],\n",
        "        G_ba: Union['GeneratorModel', None],\n",
        "        D_ab: Union['DiscriminatorModel', None],\n",
        "        D_ba: Union['DiscriminatorModel', None],\n",
        "        Cls: Union['ClassifierModel', None],\n",
        "        device=None,\n",
        "        label2id: Dict[str, int] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialization method for the CycleGANModel\n",
        "\n",
        "        Args:\n",
        "            G_ab (GeneratorModel): Generator model for mapping A->B\n",
        "            G_ba (GeneratorModel): Generator model for mapping B->A\n",
        "            D_ab (DiscriminatorModel): Discriminator model for B\n",
        "            D_ba (DiscriminatorModel): Discriminator model for A\n",
        "            Cls (ClassifierModel): Style classifier\n",
        "            label2id (Dict[str,int]): Style-to-integer mapping (e.g., {\"neu\": 0, \"pos\": 1, \"neg\": 2})\n",
        "        \"\"\"\n",
        "        super(CycleGANModel, self).__init__()\n",
        "\n",
        "        if G_ab is None or G_ba is None or D_ab is None or D_ba is None:\n",
        "            logging.warning(\n",
        "                \"CycleGANModel: Some models are missing. Please call 'load_models' to load from a previous checkpoint.\"\n",
        "            )\n",
        "\n",
        "        self.G_ab = G_ab\n",
        "        self.G_ba = G_ba\n",
        "        self.D_ab = D_ab\n",
        "        self.D_ba = D_ba\n",
        "        self.Cls = Cls\n",
        "\n",
        "        self.device = device\n",
        "        logging.info(f\"Device: {device}\")\n",
        "\n",
        "        # Use default label2id if none is provided\n",
        "        if label2id is None:\n",
        "            label2id = {\"neu\": 0, \"pos\": 1, \"neg\": 2}\n",
        "        self.label2id = label2id\n",
        "\n",
        "        # Move all models to device\n",
        "        self.G_ab.model.to(self.device)\n",
        "        self.G_ba.model.to(self.device)\n",
        "        self.D_ab.model.to(self.device)\n",
        "        self.D_ba.model.to(self.device)\n",
        "        if self.Cls is not None:\n",
        "            self.Cls.model.to(self.device)\n",
        "\n",
        "    def train(self):\n",
        "        self.G_ab.train()\n",
        "        self.G_ba.train()\n",
        "        self.D_ab.train()\n",
        "        self.D_ba.train()\n",
        "\n",
        "    def eval(self):\n",
        "        self.G_ab.eval()\n",
        "        self.G_ba.eval()\n",
        "        self.D_ab.eval()\n",
        "        self.D_ba.eval()\n",
        "\n",
        "    def get_optimizer_parameters(self):\n",
        "        params = list(self.G_ab.model.parameters())\n",
        "        params += list(self.G_ba.model.parameters())\n",
        "        params += list(self.D_ab.model.parameters())\n",
        "        params += list(self.D_ba.model.parameters())\n",
        "        return params\n",
        "\n",
        "    def training_cycle(\n",
        "        self,\n",
        "        sentences_a: List[str],\n",
        "        sentences_b: List[str],\n",
        "        target_sentences_ab: List[str] = None,\n",
        "        target_sentences_ba: List[str] = None,\n",
        "        style_source=str,\n",
        "        style_target=str,\n",
        "        lambdas: List[float] = None,\n",
        "        loss_logging=None,\n",
        "        training_step: int = None\n",
        "    ):\n",
        "        # ----- Cycle A -> B -----\n",
        "        token_a_b = f\"[{style_source}->{style_target}]\"\n",
        "        token_b_a = f\"[{style_target}->{style_source}]\"\n",
        "\n",
        "        label2id = self.label2id\n",
        "\n",
        "        # First half\n",
        "        mono_a_with_style = [f\"{token_a_b} {s}\" for s in sentences_a]\n",
        "        _, transferred_ab = self.G_ab(mono_a_with_style, device=self.device)\n",
        "\n",
        "        # D_ab fake\n",
        "        self.D_ab.eval()\n",
        "        zeros = torch.zeros(len(transferred_ab))\n",
        "        ones = torch.ones(len(transferred_ab))\n",
        "        labels_fake_sentences = torch.column_stack((ones, zeros))  # generator side\n",
        "        _, loss_g_ab = self.D_ab(transferred_ab, labels_fake_sentences, device=self.device)\n",
        "\n",
        "        if lambdas[4] != 0:\n",
        "            labels_style_b_sentences = torch.full(\n",
        "                (len(transferred_ab),),\n",
        "                label2id[style_target],\n",
        "                dtype=int\n",
        "            )\n",
        "            _, loss_g_ab_cls = self.Cls(transferred_ab, labels_style_b_sentences, device=self.device)\n",
        "\n",
        "        # Second half\n",
        "        mono_transferred_ab_with_style = [f\"{token_b_a} {s}\" for s in transferred_ab]\n",
        "        _, _, cycle_loss_aba = self.G_ba(mono_transferred_ab_with_style, sentences_a, device=self.device)\n",
        "\n",
        "        complete_loss_g_ab = lambdas[0] * cycle_loss_aba + lambdas[1] * loss_g_ab\n",
        "\n",
        "        loss_logging['Cycle Loss A-B-A'].append((lambdas[0] * cycle_loss_aba).item())\n",
        "        loss_logging['Loss generator  A-B'].append((lambdas[1] * loss_g_ab).item())\n",
        "\n",
        "        if lambdas[4] != 0:\n",
        "            complete_loss_g_ab += lambdas[4] * loss_g_ab_cls\n",
        "\n",
        "            loss_logging['Classifier-guided A-B'].append((lambdas[4] * loss_g_ab_cls).item())\n",
        "\n",
        "        complete_loss_g_ab.backward()\n",
        "\n",
        "        # D_ab training\n",
        "        self.D_ab.train()\n",
        "        zeros = torch.zeros(len(transferred_ab))\n",
        "        ones = torch.ones(len(transferred_ab))\n",
        "        labels_fake_sentences = torch.column_stack((zeros, ones))  # discriminator side\n",
        "        _, loss_d_ab_fake = self.D_ab(transferred_ab, labels_fake_sentences, device=self.device)\n",
        "\n",
        "        zeros = torch.zeros(len(transferred_ab))\n",
        "        ones = torch.ones(len(transferred_ab))\n",
        "        labels_real_sentences = torch.column_stack((ones, zeros))\n",
        "        _, loss_d_ab_real = self.D_ab(sentences_b, labels_real_sentences, device=self.device)\n",
        "        complete_loss_d_ab = lambdas[2] * loss_d_ab_fake + lambdas[3] * loss_d_ab_real\n",
        "\n",
        "\n",
        "        loss_logging['Loss D(A->B)'].append(complete_loss_d_ab.item())\n",
        "        complete_loss_d_ab.backward()\n",
        "\n",
        "        # ----- Cycle B -> A -----\n",
        "        mono_b_with_style = [f\"{token_b_a} {s}\" for s in sentences_b]\n",
        "\n",
        "        # First half\n",
        "        _, transferred_ba = self.G_ba(mono_b_with_style, device=self.device)\n",
        "\n",
        "        # D_ba\n",
        "        self.D_ba.eval()\n",
        "        zeros = torch.zeros(len(transferred_ba))\n",
        "        ones = torch.ones(len(transferred_ba))\n",
        "        labels_fake_sentences = torch.column_stack((ones, zeros))\n",
        "        _, loss_g_ba = self.D_ba(transferred_ba, labels_fake_sentences, device=self.device)\n",
        "\n",
        "        if lambdas[4] != 0:\n",
        "            labels_style_a_sentences = torch.full(\n",
        "                (len(transferred_ba),),\n",
        "                label2id[style_source],\n",
        "                dtype=int\n",
        "            )\n",
        "            _, loss_g_ba_cls = self.Cls(transferred_ba, labels_style_a_sentences, device=self.device)\n",
        "\n",
        "        # Second half\n",
        "        mono_transferred_ba_with_style = [f\"{token_a_b} {s}\" for s in transferred_ba]\n",
        "        _, _, cycle_loss_bab = self.G_ab(mono_transferred_ba_with_style, sentences_b, device=self.device)\n",
        "\n",
        "        complete_loss_g_ba = lambdas[0] * cycle_loss_bab + lambdas[1] * loss_g_ba\n",
        "\n",
        "        loss_logging['Cycle Loss B-A-B'].append((lambdas[0] * cycle_loss_bab).item())\n",
        "        loss_logging['Loss generator  B-A'].append((lambdas[1] * loss_g_ba).item())\n",
        "\n",
        "        if lambdas[4] != 0:\n",
        "            complete_loss_g_ba += lambdas[4] * loss_g_ba_cls\n",
        "            loss_logging['Classifier-guided B-A'].append((lambdas[4] * loss_g_ba_cls).item())\n",
        "\n",
        "        complete_loss_g_ba.backward()\n",
        "\n",
        "        # D_ba training\n",
        "        self.D_ba.train()\n",
        "        zeros = torch.zeros(len(transferred_ba))\n",
        "        ones = torch.ones(len(transferred_ba))\n",
        "        labels_fake_sentences = torch.column_stack((zeros, ones))\n",
        "        _, loss_d_ba_fake = self.D_ba(transferred_ba, labels_fake_sentences, device=self.device)\n",
        "\n",
        "        zeros = torch.zeros(len(transferred_ba))\n",
        "        ones = torch.ones(len(transferred_ba))\n",
        "        labels_real_sentences = torch.column_stack((ones, zeros))\n",
        "        _, loss_d_ba_real = self.D_ba(sentences_a, labels_real_sentences, device=self.device)\n",
        "        complete_loss_d_ba = lambdas[2] * loss_d_ba_fake + lambdas[3] * loss_d_ba_real\n",
        "\n",
        "        loss_logging['Loss D(B->A)'].append(complete_loss_d_ba.item())\n",
        "        complete_loss_d_ba.backward()\n",
        "\n",
        "    def save_models(self, base_path: Union[str]):\n",
        "        self.G_ab.save_model(base_path + \"/G_ab/\")\n",
        "        self.G_ba.save_model(base_path + \"/G_ba/\")\n",
        "        self.D_ab.save_model(base_path + \"/D_ab/\")\n",
        "        self.D_ba.save_model(base_path + \"/D_ba/\")\n",
        "\n",
        "    def transfer(self, sentences: List[str], direction: str):\n",
        "        if direction == \"AB\":\n",
        "            transferred_sentences = self.G_ab.transfer(sentences, device=self.device)\n",
        "        else:\n",
        "            transferred_sentences = self.G_ba.transfer(sentences, device=self.device)\n",
        "        return transferred_sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJjbz7tCeaCh"
      },
      "outputs": [],
      "source": [
        "# @title Evaluator Class, it is used to evaluate model performance using a ternary classifier\n",
        "class Evaluator():\n",
        "    def __init__(self, cycleGAN, args, experiment=None, label2id=None):\n",
        "        \"\"\"\n",
        "        Class for evaluation\n",
        "        \"\"\"\n",
        "        super(Evaluator, self).__init__()\n",
        "\n",
        "        self.cycleGAN = cycleGAN\n",
        "        self.args = args\n",
        "        self.experiment = experiment\n",
        "\n",
        "        # If label2id is not provided, use a default mapping\n",
        "        if label2id is None:\n",
        "            label2id = {\"neu\": 0, \"pos\": 1, \"neg\": 2}\n",
        "        self.label2id = label2id\n",
        "\n",
        "        self.bleu = evaluate.load('sacrebleu')\n",
        "        self.rouge = evaluate.load('rouge')\n",
        "        # if args.bertscore: self.bertscore = evaluate.load('bertscore')\n",
        "\n",
        "\n",
        "    def __compute_metric__(self, predictions, references, metric_name, direction=None):\n",
        "        # predictions = list | references = list of lists\n",
        "        scores = []\n",
        "        if metric_name in ['bleu', 'rouge']:\n",
        "            for pred, ref in zip(predictions, references):\n",
        "                if metric_name == 'bleu':\n",
        "                    res = self.bleu.compute(predictions=[pred], references=[ref])\n",
        "                    scores.append(res['score'])\n",
        "                elif metric_name == 'rouge':\n",
        "                    tmp_rouge1, tmp_rouge2, tmp_rougeL = [], [], []\n",
        "                    for r in ref:\n",
        "                        res = self.rouge.compute(predictions=[pred], references=[r], use_aggregator=False)\n",
        "                        tmp_rouge1.append(res['rouge1'][0])\n",
        "                        tmp_rouge2.append(res['rouge2'][0])\n",
        "                        tmp_rougeL.append(res['rougeL'][0])\n",
        "                    scores.append([max(tmp_rouge1), max(tmp_rouge2), max(tmp_rougeL)])\n",
        "        else:\n",
        "            raise Exception(f\"Metric {metric_name} is not supported.\")\n",
        "        return scores\n",
        "\n",
        "    def __compute_classif_metrics__(self, pred_A, pred_B, style_A, style_B):\n",
        "        # Using self.label2id\n",
        "        label2id = self.label2id\n",
        "\n",
        "        device = self.cycleGAN.device\n",
        "        truncation, padding = 'longest_first', 'max_length'\n",
        "\n",
        "        # If certain conditions are met, load an external classifier instead of using self.cycleGAN.Cls\n",
        "        if ('lambdas' not in vars(self.args)\n",
        "            or self.args.lambdas[4] == 0\n",
        "            or self.args.pretrained_classifier_eval != self.args.pretrained_classifier_model):\n",
        "            classifier = AutoModelForSequenceClassification.from_pretrained(self.args.pretrained_classifier_eval)\n",
        "            classifier_tokenizer = AutoTokenizer.from_pretrained(f'{self.args.pretrained_classifier_eval}tokenizer/')\n",
        "            classifier.to(device)\n",
        "        else:\n",
        "            classifier = self.cycleGAN.Cls.model\n",
        "            classifier_tokenizer = self.cycleGAN.Cls.tokenizer\n",
        "        classifier.eval()\n",
        "\n",
        "        y_pred, y_true = [], np.concatenate([\n",
        "            np.full(len(pred_A), label2id[style_A]),\n",
        "            np.full(len(pred_B), label2id[style_B])\n",
        "        ])\n",
        "\n",
        "        for i in range(0, len(pred_A), self.args.batch_size):\n",
        "            batch_a = pred_A[i:i+self.args.batch_size]\n",
        "            inputs = classifier_tokenizer(\n",
        "                batch_a,\n",
        "                truncation=truncation,\n",
        "                padding=padding,\n",
        "                max_length=self.args.max_sequence_length,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            inputs = inputs.to(device)\n",
        "            with torch.no_grad():\n",
        "                output = classifier(**inputs)\n",
        "            y_pred.extend(np.argmax(output.logits.cpu().numpy(), axis=1))\n",
        "\n",
        "        for i in range(0, len(pred_B), self.args.batch_size):\n",
        "            batch_b = pred_B[i:i+self.args.batch_size]\n",
        "            inputs = classifier_tokenizer(\n",
        "                batch_b,\n",
        "                truncation=truncation,\n",
        "                padding=padding,\n",
        "                max_length=self.args.max_sequence_length,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            inputs = inputs.to(device)\n",
        "            with torch.no_grad():\n",
        "                output = classifier(**inputs)\n",
        "            y_pred.extend(np.argmax(output.logits.cpu().numpy(), axis=1))\n",
        "\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        prec = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "        rec = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "        f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "        return acc, prec, rec, f1\n",
        "\n",
        "    def run_eval_mono(self, epoch, current_training_step, phase, dl_source, dl_target, style_source, style_target):\n",
        "        print(f'Start {phase}...')\n",
        "        self.cycleGAN.eval()\n",
        "\n",
        "        real_A, real_B = [], []\n",
        "        pred_A, pred_B = [], []\n",
        "        scores_AB_bleu_self, scores_BA_bleu_self = [], []\n",
        "        scores_AB_r1_self, scores_BA_r1_self = [], []\n",
        "        scores_AB_r2_self, scores_BA_r2_self = [], []\n",
        "        scores_AB_rL_self, scores_BA_rL_self = [], []\n",
        "\n",
        "        # Define the style token for style B\n",
        "        style_token_A = f\"[{dl_target.dataset.style}->{dl_source.dataset.style}]\"\n",
        "        style_token_B = f\"[{dl_source.dataset.style}->{dl_target.dataset.style}]\"\n",
        "\n",
        "        for batch in dl_source:\n",
        "            mono_a = list(batch)\n",
        "            mono_a_with_style = [f\"{style_token_B} {sentence}\" for sentence in mono_a]\n",
        "            with torch.no_grad():\n",
        "                transferred = self.cycleGAN.transfer(sentences=mono_a_with_style, direction='AB')\n",
        "            real_A.extend(mono_a)\n",
        "            pred_B.extend(transferred)\n",
        "            mono_a = [[s] for s in mono_a]\n",
        "            scores_AB_bleu_self.extend(self.__compute_metric__(transferred, mono_a, 'bleu'))\n",
        "            scores_rouge_self = np.array(self.__compute_metric__(transferred, mono_a, 'rouge'))\n",
        "            scores_AB_r1_self.extend(scores_rouge_self[:, 0].tolist())\n",
        "            scores_AB_r2_self.extend(scores_rouge_self[:, 1].tolist())\n",
        "            scores_AB_rL_self.extend(scores_rouge_self[:, 2].tolist())\n",
        "\n",
        "        avg_AB_bleu_self = np.mean(scores_AB_bleu_self)\n",
        "        avg_AB_r1_self = np.mean(scores_AB_r1_self)\n",
        "        avg_AB_r2_self = np.mean(scores_AB_r2_self)\n",
        "        avg_AB_rL_self = np.mean(scores_AB_rL_self)\n",
        "\n",
        "        for batch in dl_target:\n",
        "            mono_b = list(batch)\n",
        "            mono_b_with_style = [f\"{style_token_A} {sentence}\" for sentence in mono_b]\n",
        "            with torch.no_grad():\n",
        "                transferred = self.cycleGAN.transfer(sentences=mono_b_with_style, direction='BA')\n",
        "            real_B.extend(mono_b)\n",
        "            pred_A.extend(transferred)\n",
        "            mono_b = [[s] for s in mono_b]\n",
        "            scores_BA_bleu_self.extend(self.__compute_metric__(transferred, mono_b, 'bleu'))\n",
        "            scores_rouge_self = np.array(self.__compute_metric__(transferred, mono_b, 'rouge'))\n",
        "            scores_BA_r1_self.extend(scores_rouge_self[:, 0].tolist())\n",
        "            scores_BA_r2_self.extend(scores_rouge_self[:, 1].tolist())\n",
        "            scores_BA_rL_self.extend(scores_rouge_self[:, 2].tolist())\n",
        "\n",
        "        avg_BA_bleu_self = np.mean(scores_BA_bleu_self)\n",
        "        avg_BA_r1_self = np.mean(scores_BA_r1_self)\n",
        "        avg_BA_r2_self = np.mean(scores_BA_r2_self)\n",
        "        avg_BA_rL_self = np.mean(scores_BA_rL_self)\n",
        "        avg_2dir_bleu_self = (avg_AB_bleu_self + avg_BA_bleu_self) / 2\n",
        "\n",
        "        acc, _, _, _ = self.__compute_classif_metrics__(pred_A, pred_B, style_source, style_target)\n",
        "        acc_scaled = acc * 100\n",
        "        avg_acc_bleu_self = (avg_2dir_bleu_self + acc_scaled) / 2\n",
        "        avg_acc_bleu_self_geom = (avg_2dir_bleu_self * acc_scaled) ** 0.5\n",
        "        avg_acc_bleu_self_h = (2 * avg_2dir_bleu_self * acc_scaled) / (avg_2dir_bleu_self + acc_scaled + 1e-6)\n",
        "\n",
        "        metrics = {\n",
        "            'epoch': epoch,\n",
        "            'step': current_training_step,\n",
        "            'self-BLEU A->B': avg_AB_bleu_self,\n",
        "            'self-BLEU B->A': avg_BA_bleu_self,\n",
        "            'self-BLEU avg': avg_2dir_bleu_self,\n",
        "            'self-ROUGE-1 A->B': avg_AB_r1_self,\n",
        "            'self-ROUGE-1 B->A': avg_BA_r1_self,\n",
        "            'self-ROUGE-2 A->B': avg_AB_r2_self,\n",
        "            'self-ROUGE-2 B->A': avg_BA_r2_self,\n",
        "            'self-ROUGE-L A->B': avg_AB_rL_self,\n",
        "            'self-ROUGE-L B->A': avg_BA_rL_self,\n",
        "            'style accuracy': acc,\n",
        "            'acc-BLEU': avg_acc_bleu_self,\n",
        "            'g-acc-BLEU': avg_acc_bleu_self_geom,\n",
        "            'h-acc-BLEU': avg_acc_bleu_self_h\n",
        "        }\n",
        "\n",
        "        if phase[:10] == 'validation':\n",
        "            base_path = f\"{self.args.save_base_folder}epoch_{epoch}/\"\n",
        "            suffix = f'epoch{epoch}'\n",
        "        else:\n",
        "            if self.args.from_pretrained is not None:\n",
        "                if self.args.save_base_folder is not None:\n",
        "                    base_path = f\"{self.args.save_base_folder}\"\n",
        "                else:\n",
        "                    base_path = f\"{self.args.from_pretrained}epoch_{epoch}/\"\n",
        "            else:\n",
        "                base_path = f\"{self.args.save_base_folder}test/epoch_{epoch}/\"\n",
        "            suffix = f'epoch{epoch}_test'\n",
        "\n",
        "        os.makedirs(os.path.dirname(base_path), exist_ok=True)\n",
        "        pickle.dump(metrics, open(f\"{base_path}metrics_{suffix}.pickle\", 'wb'))\n",
        "\n",
        "        for m, v in metrics.items():\n",
        "            if m not in ['epoch', 'step']:\n",
        "                print(f'{m}: {v}')\n",
        "\n",
        "        df_AB = pd.DataFrame()\n",
        "        df_AB['A (source)'] = real_A\n",
        "        df_AB['B (generated)'] = pred_B\n",
        "        df_AB.to_csv(f\"{base_path}{style_source}_{style_target}_{suffix}.csv\", sep=',', header=True)\n",
        "\n",
        "        df_BA = pd.DataFrame()\n",
        "        df_BA['B (source)'] = real_B\n",
        "        df_BA['A (generated)'] = pred_A\n",
        "        df_BA.to_csv(f\"{base_path}{style_target}_{style_source}_{suffix}.csv\", sep=',', header=True)\n",
        "\n",
        "        del df_AB, df_BA\n",
        "        print(f'End {phase}...')\n",
        "\n",
        "    def dummy_classif(self):\n",
        "        pred_A = [\n",
        "            'wake up or you are going to lose your business .',\n",
        "            'this place has none of them .',\n",
        "            'it is april and there are no grass tees yet .',\n",
        "            'there is no grass on the range .',\n",
        "            'bottom line , this place sucks .',\n",
        "            'someone should buy this place .',\n",
        "            'very disappointed in the customer service .',\n",
        "            'we will not be back .'\n",
        "        ]\n",
        "        pred_B = [\n",
        "            'huge sandwich !',\n",
        "            'i added mushrooms , it was very flavorful .',\n",
        "            'he enjoyed it as well .',\n",
        "            'fast and friendly service .',\n",
        "            'will definitely be back .',\n",
        "            \"my dad 's favorite .\",\n",
        "            'huge burgers , fish sandwiches , salads .',\n",
        "            'decent service .'\n",
        "        ]\n",
        "        acc, _, _, _ = self.__compute_classif_metrics__(pred_A, pred_B, 'neg', 'pos')\n",
        "        print('Dummy classification metrics computation end')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HretbKWCf-tU"
      },
      "outputs": [],
      "source": [
        "# @title Main function: it instantiates Generators, Discriminators and classifiers and control the training flow\n",
        "def main(args):\n",
        "    # List of required attributes\n",
        "    required_attrs = [\n",
        "        \"epochs\", \"style_a\", \"style_b\", \"style_c\",\n",
        "        \"path_mono_A\", \"path_mono_B\", \"path_mono_C\",\n",
        "        \"path_mono_A_eval\", \"path_mono_B_eval\", \"path_mono_C_eval\",\n",
        "        \"batch_size\", \"max_samples_train\", \"max_samples_eval\",\n",
        "        \"nonparal_same_size\", \"generator_model_tag\", \"discriminator_model_tag\",\n",
        "        \"pretrained_classifier_model\", \"pretrained_classifier_eval\",\n",
        "        \"from_pretrained\", \"save_base_folder\", \"save_steps\",\n",
        "        \"lambdas\", \"learning_rate\", \"max_sequence_length\",\n",
        "        \"lr_scheduler_type\", \"warmup_ratio\", \"use_cuda_if_available\"\n",
        "    ]\n",
        "\n",
        "    # Check for missing attributes\n",
        "    missing_attrs = [attr for attr in required_attrs if not hasattr(args, attr)]\n",
        "    if missing_attrs:\n",
        "        raise AttributeError(f\"Args object is missing: {', '.join(missing_attrs)}\")\n",
        "\n",
        "    # Seeding\n",
        "    SEED = 42\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    random.seed(SEED)\n",
        "\n",
        "    # Print paths for debugging\n",
        "    print(f\"Loading dataset A from: {args.path_mono_A}\")\n",
        "    print(f\"Loading dataset B from: {args.path_mono_B}\")\n",
        "    print(f\"Loading dataset C from: {args.path_mono_C}\")\n",
        "\n",
        "    # ----- Load datasets -----\n",
        "    mono_ds_a = MonostyleDataset(\n",
        "        dataset_format=\"line_file\",\n",
        "        dataset_path=args.path_mono_A,\n",
        "        style=args.style_a,\n",
        "        separator='\\n',\n",
        "        max_dataset_samples=args.max_samples_train,\n",
        "    )\n",
        "    mono_ds_b = MonostyleDataset(\n",
        "        dataset_format=\"line_file\",\n",
        "        style=args.style_b,\n",
        "        dataset_path=args.path_mono_B,\n",
        "        separator='\\n',\n",
        "        max_dataset_samples=args.max_samples_train,\n",
        "    )\n",
        "    mono_ds_c = MonostyleDataset(\n",
        "        dataset_format=\"line_file\",\n",
        "        style=args.style_c,\n",
        "        dataset_path=args.path_mono_C,\n",
        "        separator='\\n',\n",
        "        max_dataset_samples=args.max_samples_train,\n",
        "    )\n",
        "\n",
        "    # Parse lambdas\n",
        "    lambdas = [float(l) for l in args.lambdas.split('|')]\n",
        "\n",
        "    # Print all args for clarity\n",
        "    hyper_params = {}\n",
        "    print(\"\\nArguments summary:\")\n",
        "    for key, value in vars(args).items():\n",
        "        hyper_params[key] = value\n",
        "        print(f\"  {key}:\\t{value}\")\n",
        "\n",
        "    # If specified, reduce all datasets to the same size\n",
        "    if args.nonparal_same_size:\n",
        "        min_len = min(len(mono_ds_a), len(mono_ds_b), len(mono_ds_c))\n",
        "        mono_ds_a.reduce_data(min_len)\n",
        "        mono_ds_b.reduce_data(min_len)\n",
        "        mono_ds_c.reduce_data(min_len)\n",
        "\n",
        "    # Create eval datasets\n",
        "    mono_ds_a_eval = MonostyleDataset(\n",
        "        dataset_format=\"line_file\",\n",
        "        style=args.style_a,\n",
        "        dataset_path=args.path_mono_A_eval,\n",
        "        separator='\\n',\n",
        "        max_dataset_samples=args.max_samples_eval\n",
        "    )\n",
        "    mono_ds_b_eval = MonostyleDataset(\n",
        "        dataset_format=\"line_file\",\n",
        "        style=args.style_b,\n",
        "        dataset_path=args.path_mono_B_eval,\n",
        "        separator='\\n',\n",
        "        max_dataset_samples=args.max_samples_eval\n",
        "    )\n",
        "    mono_ds_c_eval = MonostyleDataset(\n",
        "        dataset_format=\"line_file\",\n",
        "        style=args.style_c,\n",
        "        dataset_path=args.path_mono_C_eval,\n",
        "        separator='\\n',\n",
        "        max_dataset_samples=args.max_samples_eval\n",
        "    )\n",
        "\n",
        "    # Dataloaders\n",
        "    mono_dl_a = DataLoader(mono_ds_a, batch_size=args.batch_size, shuffle=True)\n",
        "    mono_dl_b = DataLoader(mono_ds_b, batch_size=args.batch_size, shuffle=True)\n",
        "    mono_dl_c = DataLoader(mono_ds_c, batch_size=args.batch_size, shuffle=True)\n",
        "\n",
        "    mono_dl_a_eval = DataLoader(mono_ds_a_eval, batch_size=args.batch_size, shuffle=False)\n",
        "    mono_dl_b_eval = DataLoader(mono_ds_b_eval, batch_size=args.batch_size, shuffle=False)\n",
        "    mono_dl_c_eval = DataLoader(mono_ds_c_eval, batch_size=args.batch_size, shuffle=False)\n",
        "\n",
        "    # Optional: free memory\n",
        "    del mono_ds_a, mono_ds_b, mono_ds_c\n",
        "    del mono_ds_a_eval, mono_ds_b_eval, mono_ds_c_eval\n",
        "\n",
        "    # ----- Instantiate G, D, Cls -----\n",
        "    if args.from_pretrained:\n",
        "        G_ab = GeneratorModel(\n",
        "            model_name_or_path=args.generator_model_tag,\n",
        "            new_style_tokens=args.style_token_list,\n",
        "            pretrained_path=f\"{args.from_pretrained}G_ab/\",\n",
        "            max_seq_length=args.max_sequence_length\n",
        "        )\n",
        "        G_ba = GeneratorModel(\n",
        "            model_name_or_path=args.generator_model_tag,\n",
        "            new_style_tokens=args.style_token_list,\n",
        "            pretrained_path=f\"{args.from_pretrained}G_ba/\",\n",
        "            max_seq_length=args.max_sequence_length\n",
        "        )\n",
        "        D_ab = DiscriminatorModel(\n",
        "            args.discriminator_model_tag,\n",
        "            f\"{args.from_pretrained}D_ab/\",\n",
        "            max_seq_length=args.max_sequence_length\n",
        "        )\n",
        "        D_ba = DiscriminatorModel(\n",
        "            args.discriminator_model_tag,\n",
        "            f\"{args.from_pretrained}D_ba/\",\n",
        "            max_seq_length=args.max_sequence_length\n",
        "        )\n",
        "        print(\"[INFO] Loaded pretrained G_ab, G_ba, D_ab, D_ba\")\n",
        "    else:\n",
        "        G_ab = GeneratorModel(\n",
        "            model_name_or_path=args.generator_model_tag,\n",
        "            new_style_tokens=args.style_token_list,\n",
        "            max_seq_length=args.max_sequence_length\n",
        "        )\n",
        "        G_ba = GeneratorModel(\n",
        "            model_name_or_path=args.generator_model_tag,\n",
        "            new_style_tokens=args.style_token_list,\n",
        "            max_seq_length=args.max_sequence_length\n",
        "        )\n",
        "        D_ab = DiscriminatorModel(\n",
        "            args.discriminator_model_tag,\n",
        "            max_seq_length=args.max_sequence_length\n",
        "        )\n",
        "        D_ba = DiscriminatorModel(\n",
        "            args.discriminator_model_tag,\n",
        "            max_seq_length=args.max_sequence_length\n",
        "        )\n",
        "        print(\"[INFO] Using fresh G_ab, G_ba, D_ab, D_ba\")\n",
        "\n",
        "    # If we need the classifier\n",
        "    if lambdas[4] != 0 and args.pretrained_classifier_model:\n",
        "        Cls = ClassifierModel(args.pretrained_classifier_model, max_seq_length=args.max_sequence_length)\n",
        "        print(\"[INFO] Loaded pretrained classifier\")\n",
        "    else:\n",
        "        Cls = None\n",
        "\n",
        "    # Device\n",
        "    if args.use_cuda_if_available and torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    # Create CycleGAN\n",
        "    cycleGAN = CycleGANModel(\n",
        "        G_ab=G_ab,\n",
        "        G_ba=G_ba,\n",
        "        D_ab=D_ab,\n",
        "        D_ba=D_ba,\n",
        "        Cls=Cls,\n",
        "        device=device,\n",
        "        label2id=args.label2id\n",
        "    )\n",
        "\n",
        "    # Calculate total training steps\n",
        "    n_batch_ab = min(len(mono_dl_a), len(mono_dl_b))\n",
        "    n_batch_ac = min(len(mono_dl_a), len(mono_dl_c))\n",
        "    steps_per_epoch = n_batch_ab + n_batch_ac\n",
        "    total_training_steps = args.epochs * steps_per_epoch\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = AdamW(cycleGAN.get_optimizer_parameters(), lr=args.learning_rate)\n",
        "\n",
        "    # Scheduler\n",
        "    scheduler = CosineAnnealingWarmRestarts(\n",
        "        optimizer,\n",
        "        T_0=int(steps_per_epoch / 2),\n",
        "        T_mult=1,\n",
        "        eta_min=0\n",
        "    )\n",
        "\n",
        "    current_training_step = 0\n",
        "    start_epoch = 0\n",
        "\n",
        "    # Resume checkpoint if available\n",
        "    if args.from_pretrained and os.path.exists(f\"{args.from_pretrained}checkpoint.pth\"):\n",
        "        ckpt = torch.load(f\"{args.from_pretrained}checkpoint.pth\", map_location=\"cpu\")\n",
        "        optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
        "        scheduler.load_state_dict(ckpt[\"lr_scheduler\"])\n",
        "        current_training_step = ckpt[\"training_step\"]\n",
        "        del ckpt\n",
        "\n",
        "    # Evaluator\n",
        "    evaluator = Evaluator(cycleGAN, args, label2id=args.label2id)\n",
        "\n",
        "    # Training subphase function\n",
        "    def train_subphase(dataloader_a, dataloader_x, style_src, style_tgt, loss_log):\n",
        "        nonlocal current_training_step\n",
        "        n_batch = min(len(dataloader_a), len(dataloader_x))\n",
        "        progress_bar = tqdm(range(n_batch), desc=f\"{style_src}->{style_tgt}\")\n",
        "\n",
        "        cycleGAN.train()\n",
        "        for batch_a, batch_x in zip(dataloader_a, dataloader_x):\n",
        "            # Ensure batch_a and batch_x have the same size\n",
        "            len_a, len_x = len(batch_a), len(batch_x)\n",
        "            if len_a > len_x:\n",
        "                batch_a = batch_a[:len_x]\n",
        "            elif len_x > len_a:\n",
        "                batch_x = batch_x[:len_a]\n",
        "\n",
        "            cycleGAN.training_cycle(\n",
        "                sentences_a=batch_a,\n",
        "                sentences_b=batch_x,\n",
        "                style_source=style_src,\n",
        "                style_target=style_tgt,\n",
        "                lambdas=lambdas,\n",
        "                loss_logging=loss_log,\n",
        "                training_step=current_training_step\n",
        "            )\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            current_training_step += 1\n",
        "            progress_bar.update(1)\n",
        "\n",
        "        progress_bar.close()\n",
        "\n",
        "    # Loss logging\n",
        "    loss_logging = {\n",
        "        'Cycle Loss A-B-A': [],\n",
        "        'Loss generator  A-B': [],\n",
        "        'Classifier-guided A-B': [],\n",
        "        'Loss D(A->B)': [],\n",
        "        'Cycle Loss B-A-B': [],\n",
        "        'Loss generator  B-A': [],\n",
        "        'Classifier-guided B-A': [],\n",
        "        'Loss D(B->A)': []\n",
        "    }\n",
        "    loss_logging['hyper_params'] = hyper_params\n",
        "\n",
        "    # ----- Training loop -----\n",
        "    for epoch_idx in range(start_epoch, args.epochs):\n",
        "        print(f\"\\n=== EPOCH {epoch_idx} ===\")\n",
        "\n",
        "        # (1) A->B\n",
        "        train_subphase(mono_dl_a, mono_dl_b, style_src=args.style_a, style_tgt=args.style_b, loss_log=loss_logging)\n",
        "        # (2) A->C\n",
        "        train_subphase(mono_dl_a, mono_dl_c, style_src=args.style_a, style_tgt=args.style_c, loss_log=loss_logging)\n",
        "\n",
        "        # (3) End-of-epoch evaluation\n",
        "        evaluator.run_eval_mono(\n",
        "            epoch_idx,\n",
        "            current_training_step,\n",
        "            phase=\"validation_AB_epoch\",\n",
        "            dl_source=mono_dl_a_eval,\n",
        "            dl_target=mono_dl_b_eval,\n",
        "            style_source=args.style_a,\n",
        "            style_target=args.style_b\n",
        "        )\n",
        "        evaluator.run_eval_mono(\n",
        "            epoch_idx,\n",
        "            current_training_step,\n",
        "            phase=\"validation_AC_epoch\",\n",
        "            dl_source=mono_dl_a_eval,\n",
        "            dl_target=mono_dl_c_eval,\n",
        "            style_source=args.style_a,\n",
        "            style_target=args.style_c\n",
        "        )\n",
        "\n",
        "        # (4) Checkpoint saving\n",
        "        if epoch_idx % args.save_steps == 0:\n",
        "            cycleGAN.save_models(f\"{args.save_base_folder}epoch_{epoch_idx}/\")\n",
        "\n",
        "            checkpoint = {\n",
        "                'epoch': epoch_idx + 1,\n",
        "                'training_step': current_training_step,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'lr_scheduler': scheduler.state_dict()\n",
        "            }\n",
        "            torch.save(checkpoint, f\"{args.save_base_folder}/checkpoint.pth\")\n",
        "\n",
        "            # Remove old loss file if needed\n",
        "            if epoch_idx > 0:\n",
        "                prev_loss_file = f\"{args.save_base_folder}loss.pickle\"\n",
        "                if os.path.exists(prev_loss_file):\n",
        "                    os.remove(prev_loss_file)\n",
        "\n",
        "            # Save training loss\n",
        "            pickle.dump(loss_logging, open(f\"{args.save_base_folder}loss.pickle\", \"wb\"))\n",
        "\n",
        "        cycleGAN.train()\n",
        "\n",
        "    print(\"\\n=== Training completed ===\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaMsOsqVIxZO",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Args Class, it is used to configure the GAN\n",
        "class Args:\n",
        "    def __init__(\n",
        "        self,\n",
        "        epochs,\n",
        "        style_a,\n",
        "        style_b,\n",
        "        style_c,\n",
        "        path_mono_A,\n",
        "        path_mono_B,\n",
        "        path_mono_C,\n",
        "        path_mono_A_eval,\n",
        "        path_mono_B_eval,\n",
        "        path_mono_C_eval,\n",
        "        generator_model_tag,\n",
        "        discriminator_model_tag,\n",
        "        label2id,\n",
        "        style_token_list,\n",
        "        batch_size=8,\n",
        "        max_samples_train=None,\n",
        "        max_samples_eval=None,\n",
        "        nonparal_same_size=False,\n",
        "        pretrained_classifier_model=None,\n",
        "        pretrained_classifier_eval=None,\n",
        "        from_pretrained=None,\n",
        "        save_base_folder=\"./checkpoints/\",\n",
        "        save_steps=1,\n",
        "        lambdas=\"10|1|1|1|1|1\",\n",
        "        learning_rate=5e-5,\n",
        "        max_sequence_length=32,\n",
        "        lr_scheduler_type=\"cosine_with_restarts\",\n",
        "        warmup_ratio=0.0,\n",
        "        use_cuda_if_available=False\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Class to store all training/testing arguments.\n",
        "        \"\"\"\n",
        "        self.epochs = epochs\n",
        "        self.style_a = style_a\n",
        "        self.style_b = style_b\n",
        "        self.style_c = style_c\n",
        "        self.path_mono_A = path_mono_A\n",
        "        self.path_mono_B = path_mono_B\n",
        "        self.path_mono_C = path_mono_C\n",
        "        self.path_mono_A_eval = path_mono_A_eval\n",
        "        self.path_mono_B_eval = path_mono_B_eval\n",
        "        self.path_mono_C_eval = path_mono_C_eval\n",
        "        self.generator_model_tag = generator_model_tag\n",
        "        self.discriminator_model_tag = discriminator_model_tag\n",
        "        self.batch_size = batch_size\n",
        "        self.max_samples_train = max_samples_train\n",
        "        self.max_samples_eval = max_samples_eval\n",
        "        self.nonparal_same_size = nonparal_same_size\n",
        "        self.pretrained_classifier_model = pretrained_classifier_model\n",
        "        self.pretrained_classifier_eval = pretrained_classifier_eval\n",
        "        self.from_pretrained = from_pretrained\n",
        "        self.save_base_folder = save_base_folder\n",
        "        self.save_steps = save_steps\n",
        "        self.lambdas = lambdas\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.lr_scheduler_type = lr_scheduler_type\n",
        "        self.warmup_ratio = warmup_ratio\n",
        "        self.use_cuda_if_available = use_cuda_if_available\n",
        "        self.label2id = label2id\n",
        "        self.style_token_list = style_token_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments"
      ],
      "metadata": {
        "id": "51vP8UbUZuCb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "qargewlKJ0Cj",
        "outputId": "feb0d06e-7d9e-45df-b278-87ef67a1f1d7",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Create the label2id map based on the defined styles\\nlabel2id = {\\n    \"pos\": 0,\\n    \"neu\": 1,\\n    \"neg\": 2\\n}\\n\\n# Dynamically create the list of style tokens\\nstyle_token_list = [\\n    \"[pos->neu]\", \"[pos->neg]\",\\n    \"[neu->pos]\", \"[neu->neg]\",\\n    \"[neg->pos]\", \"[neg->neu]\"\\n]\\n\\n# Define args directly in the notebook\\nargs = Args(\\n    epochs=7,\\n    style_a=\"pos\",\\n    style_b=\"neu\",\\n    style_c=\"neg\",\\n    path_mono_A=\"/content/drive/MyDrive/ProjectNLP/00.Amazon_Project/data/AMAZON/Train_split/positive_train.txt\",\\n    path_mono_B=\"/content/drive/MyDrive/ProjectNLP/00.Amazon_Project/data/AMAZON/Train_split/neutral_train.txt\",\\n    path_mono_C=\"/content/drive/MyDrive/ProjectNLP/00.Amazon_Project/data/AMAZON/Train_split/negative_train.txt\",\\n    path_mono_A_eval=\"/content/drive/MyDrive/ProjectNLP/00.Amazon_Project/data/AMAZON/Eval_split/positive_eval.txt\",\\n    path_mono_B_eval=\"/content/drive/MyDrive/ProjectNLP/00.Amazon_Project/data/AMAZON/Eval_split/neutral_eval.txt\",\\n    path_mono_C_eval=\"/content/drive/MyDrive/ProjectNLP/00.Amazon_Project/data/AMAZON/Eval_split/negative_eval.txt\",\\n    generator_model_tag=\"facebook/bart-base\",\\n    discriminator_model_tag=\"distilbert/distilbert-base-cased\",\\n    pretrained_classifier_model=\"/content/drive/MyDrive/ProjectNLP/00.Amazon_Project/CLASSIFIER_CHECKPOINT\",\\n    pretrained_classifier_eval=\"/content/drive/MyDrive/ProjectNLP/00.Amazon_Project/CLASSIFIER_CHECKPOINT\",\\n    lambdas=\"10|1|1|1|1\",\\n    learning_rate=1e-4,\\n    max_sequence_length=64,\\n    save_base_folder=\"/content/drive/MyDrive/ProjectNLP/00.Amazon_Project/GAN_CHECKPOINT_POS_PROVA/\",\\n    save_steps=1,\\n    use_cuda_if_available=True,\\n    label2id=label2id,           # Explicitly pass the label mapping\\n    style_token_list=style_token_list  # Explicitly pass the style tokens\\n)\\n\\n# Call the main function with the updated arguments\\nmain(args)\\n\\n# from_pretrained=\"/content/drive/MyDrive/ProjectNLP/00.Amazon_Project/GAN_CHECKPOINT_POS/\"\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# @title Amazon runs: uncomment to test it\n",
        "\n",
        "'''\n",
        "# Create the label2id map based on the defined styles\n",
        "label2id = {\n",
        "    \"pos\": 0,\n",
        "    \"neu\": 1,\n",
        "    \"neg\": 2\n",
        "}\n",
        "\n",
        "# Dynamically create the list of style tokens\n",
        "style_token_list = [\n",
        "    \"[pos->neu]\", \"[pos->neg]\",\n",
        "    \"[neu->pos]\", \"[neu->neg]\",\n",
        "    \"[neg->pos]\", \"[neg->neu]\"\n",
        "]\n",
        "\n",
        "# Define args directly in the notebook\n",
        "args = Args(\n",
        "    epochs=7,\n",
        "    style_a=\"pos\",\n",
        "    style_b=\"neu\",\n",
        "    style_c=\"neg\",\n",
        "    path_mono_A=\"/content/drive/MyDrive/ProjectNLP/00.Amazon_Project/data/AMAZON/Train_split/positive_train.txt\",\n",
        "    path_mono_B=\"/content/drive/MyDrive/ProjectNLP/00.Amazon_Project/data/AMAZON/Train_split/neutral_train.txt\",\n",
        "    path_mono_C=\"/content/drive/MyDrive/ProjectNLP/00.Amazon_Project/data/AMAZON/Train_split/negative_train.txt\",\n",
        "    path_mono_A_eval=\"/content/drive/MyDrive/ProjectNLP/00.Amazon_Project/data/AMAZON/Eval_split/positive_eval.txt\",\n",
        "    path_mono_B_eval=\"/content/drive/MyDrive/ProjectNLP/00.Amazon_Project/data/AMAZON/Eval_split/neutral_eval.txt\",\n",
        "    path_mono_C_eval=\"/content/drive/MyDrive/ProjectNLP/00.Amazon_Project/data/AMAZON/Eval_split/negative_eval.txt\",\n",
        "    generator_model_tag=\"facebook/bart-base\",\n",
        "    discriminator_model_tag=\"distilbert/distilbert-base-cased\",\n",
        "    pretrained_classifier_model=\"/content/drive/MyDrive/ProjectNLP/00.Amazon_Project/CLASSIFIER_CHECKPOINT\",\n",
        "    pretrained_classifier_eval=\"/content/drive/MyDrive/ProjectNLP/00.Amazon_Project/CLASSIFIER_CHECKPOINT\",\n",
        "    lambdas=\"10|1|1|1|1\",\n",
        "    learning_rate=1e-4,\n",
        "    max_sequence_length=64,\n",
        "    save_base_folder=\"/content/drive/MyDrive/ProjectNLP/00.Amazon_Project/GAN_CHECKPOINT_POS_PROVA/\",\n",
        "    save_steps=1,\n",
        "    use_cuda_if_available=True,\n",
        "    label2id=label2id,           # Explicitly pass the label mapping\n",
        "    style_token_list=style_token_list  # Explicitly pass the style tokens\n",
        ")\n",
        "\n",
        "# Call the main function with the updated arguments\n",
        "main(args)\n",
        "\n",
        "# from_pretrained=\"/content/drive/MyDrive/ProjectNLP/00.Amazon_Project/GAN_CHECKPOINT_POS/\"\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Author runs\n",
        "# Create the label2id map based on the defined styles\n",
        "label2id = {\n",
        "    \"tru\": 0,\n",
        "    \"lyr\": 1,\n",
        "    \"sha\": 2\n",
        "}\n",
        "\n",
        "# Dynamically create the list of style tokens\n",
        "style_token_list = [\n",
        "    \"[tru->lyr]\", \"[tru->sha]\",\n",
        "    \"[lyr->tru]\", \"[lyr->sha]\",\n",
        "    \"[sha->tru]\", \"[sha->lyr]\"\n",
        "]\n",
        "\n",
        "# Define args directly in the notebook\n",
        "args = Args(\n",
        "    epochs=7,\n",
        "    style_a=\"tru\",\n",
        "    style_b=\"lyr\",\n",
        "    style_c=\"sha\",\n",
        "    path_mono_A=\"/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/Data_spellchecked/train_trump_spellchecked.txt\",\n",
        "    path_mono_B=\"/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/Data_spellchecked/train_lyrics_spellchecked.txt\",\n",
        "    path_mono_C=\"/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/Data_spellchecked/train_shakespeare_spellchecked.txt\",\n",
        "    path_mono_A_eval=\"/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/Data_spellchecked/eval_trump_spellchecked.txt\",\n",
        "    path_mono_B_eval=\"/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/Data_spellchecked/eval_lyrics_spellchecked.txt\",\n",
        "    path_mono_C_eval=\"/content/drive/MyDrive/ProjectNLP/03.Ultima_Estensione_Shakespeare/Data_spellchecked/eval_shakespeare_spellchecked.txt\",\n",
        "    generator_model_tag=\"facebook/bart-base\",\n",
        "    discriminator_model_tag=\"distilbert/distilbert-base-cased\",\n",
        "    pretrained_classifier_model=\"/content/drive/MyDrive/ProjectNLP/20250112_Autori/ClassifierCheckpoint/checkpoint-4654\",\n",
        "    pretrained_classifier_eval=\"/content/drive/MyDrive/ProjectNLP/20250112_Autori/ClassifierCheckpoint/checkpoint-4654\",\n",
        "    lambdas=\"10|1|1|1|1\",\n",
        "    learning_rate=1e-4,\n",
        "    max_sequence_length=64,\n",
        "    save_base_folder=\"/content/drive/MyDrive/ProjectNLP/20250112_Autori/GANPROVA/\",\n",
        "    save_steps=1,\n",
        "    use_cuda_if_available=True,\n",
        "    label2id=label2id,           # Explicitly pass the label mapping\n",
        "    style_token_list=style_token_list  # Explicitly pass the style tokens\n",
        ")\n",
        "\n",
        "# Call the main function with the updated arguments\n",
        "main(args)\n",
        "\n"
      ],
      "metadata": {
        "id": "84wE6fjm6PlG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}